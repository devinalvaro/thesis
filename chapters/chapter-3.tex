\chapter{Analisis Permasalahan dan Rancangan Solusi}

\section{Analisis Permasalahan} \label{problemanalysis}

\en{Deep learning} merupakan metode pembelajaran mesin yang pada umumnya menggunakan \en{deep neural network}, yaitu \en{neural network} dengan \en{hidden layer} berjumlah banyak. Di luar kinerja model, tantangan yang terdapat pada pelatihan model \en{deep learning} meliputi panjangnya durasi pelatihan dan besarnya sumber daya komputasi (seperti kapasitas CPU dan GPU) yang dibutuhkan. Salah satunya penyebabnya adalah banyaknya jumlah \en{hidden layer} yang mengakibatkan banyaknya pula parameter-parameter yang harus di-\en{maintain}, sehingga meningkatkan durasi dan komputasi yang dilakukan saat pelatihan model. Selain itu, pelatihan model \en{deep learning} seringkali menggunakan \en{dataset} berukuran besar, menambah durasi dan sumber daya yang dibutuhkan.

Pada tugas akhir ini, fokus penelitian terletak pada penggunaan memori untuk pelatihan model \en{deep learning}. Pada pelatihan model \en{deep learning}, memori terutama digunakan untuk me-\en{maintain} nilai-nilai seperti \en{weight}, \en{bias}, hasil fungsi aktivasi, serta data yang sedang diproses. Kelebihan penggunaan memori dari kapasitas yang ada dapat mengakibatkan gagalnya pelatihan model karena masalah \en{out of memory}.

Saat ini, terdapat berbagai penanganan terhadap masalah tersebut. Salah satunya adalah mengurangi jumlah \en{hidden layer} serta neuron-neuronnya yang secara langsung mengurangi jumlah parameter yang harus di-\en{maintain} di memori. Akan tetapi, metode ini berpotensi mengurangi kinerja, apabila topologi model menjadi kurang dari optimal.

Metode penanganan lain disebut \en{mini-batch gradient descent} (MBGD), di mana \en{dataset} pelatihan dibagi menjadi beberapa \en{mini-batch} (atau \en{batch}). Seperti yang telah disebutkan sebelumnya, \en{dataset} pelatihan \en{deep learning} umumnya berukuran besar, sehingga mahal secara waktu dan komputasi. Tujuan MBGD adalah mengurangi jumlah data yang diproses pada suatu waktu, sehingga mengurangi komputasi (termasuk penggunaan memori) dan durasi pelatihan. Namun, MBGD memiliki \en{trade-off} tersendiri, yaitu karena data yang diproses sekaligus hanya sebagian, sehingga kurang representatif dari \en{dataset} sebenarnya. Seperti disebutkan sebelumnya, hal ini berpotensi mengurangi keoptimalan kinerja model.

Satu lagi permasalahan memori pada pelatihan model \en{deep learning} adalah kapasitas GPU. Karena sifat paralelismenya yang tinggi, pelatihan menggunakan GPU lebih cepat dari CPU. Akan tetapi, pada umumnya memori GPU umumnya berukuran 8-16GB, sedangkan memori CPU dapat berukuran hingga 1TB. Terutama pada pelatihan model dengan satu GPU, masalah ini menjadi signifikan karena \en{memory load} tidak dapat didistribusi ke banyak GPU sekaligus.

Dari penjabaran tersebut ditunjukkan bahwa optimisasi penggunaan memori GPU pada pelatihan model \en{deep learning} merupakan permasalahan yang layak dipecahkan, agar dengan sumber daya yang sama dapat dilatih model dengan jumlah \en{layer} dan parameter yang lebih banyak serta meningkatkan jumlah data yang dapat diproses pada suatu waktu (ukuran \en{batch}).

Salah satu penanganan permasalahan tersebut adalah \en{memory swapping}. \en{Memory swapping} adalah metode pengurangan beban memori dengan memindahkan isi memori secara temporer ke \en{memory pool} yang lebih besar (dan biasanya lebih lambat). Pada kasus ini, sebagian isi memori GPU dapat disimpan di memori CPU secara sementara.

\en{Memory swapping} telah diterapkan di beberapa \en{framework} pembelajaran mesin seperti TensorFlow. Akan tetapi, pada TensorFlow pengambilan keputusan mengenai data apa yang di-\en{swap} dan kapan melakukan \en{swapping} hanya menggunakan heuristik sederhana yang menurut referensi kurang optimal. Mengenai heuristik tersebut akan dijelaskan secara lebih detail di Bagian~\ref{heuristic}.

Melihat celah optimisasi tersebut, demikian fokus tugas akhir ini adalah mengembangkan optimisasi penggunaan memori dengan \en{memory swapping} di TensorFlow yang lebih optimal dari \en{memory swapping} yang sudah ada di TensorFlow.

\subsection{Penggunaan Memori pada Pelatihan Model}

Berikut akan dibahas mengenai penyebab penggunaan memori pada pelatihan model serta perhitungannya. Selain untuk memuat \en{dataset} yang digunakan, memori juga digunakan untuk me-\en{maintain} variabel yang memuat parameter model (bobot dan bias) dan hasil fungsi aktivasi (\en{output}). Bila variabel direpresentasikan oleh 32-bit \en{floating point}, maka ukuran memori yang digunakan adalah 4 \en{byte} dikali dengan jumlah variabel yang ada. Sebagai contoh, model ResNet50 memiliki sekitar 26 juta parameter dan 16 juta \en{output} pada sekali \en{forward pass}. Maka, ukuran memori yang digunakan pada satu sampel sekitar $\num{26e6} \times \SI{4}{byte} = \SI{104}{\mega\byte}$ untuk parameter dan $\num{16e6} \times \SI{4}{byte} = \SI{64}{\mega\byte}$ untuk \en{output} \citep{hanlon2016why}.

Ukuran tersebut barulah ukuran memori yang digunakan oleh satu sampel pada \en{batch}. Biasanya, pelatihan model berlangsung secara paralel, yang berarti dilakukan pencarian gradien terhadap seluruh sampel pada \en{batch} sekaligus. Berarti, perlu dimuat \en{output} seluruh sampel karena \en{output} dibutuhkan untuk pencarian gradien dengan \en{backpropagation} (seperti dijelaskan pada Bagian~\ref{backpropagation} dan \ref{bptt}).

Misalnya \en{batch} berukuran 16, maka diperlukan $16 * \SI{64}{\mega\byte} = \SI{1024}{\mega\byte}$ untuk memuat seluruh \en{output} pada satu \en{batch}. Bila dijumlah dengan ukuran parameter (bobot dan bias) seperti di atas, maka diperlukan sekitar $\SI{104}{\mega\byte} + \SI{1024}{\mega\byte} = \SI{1.128}{\giga\byte}$ memori pada pelatihan model ResNet50 untuk \en{batch} berukuran 16. Perhitungan ini didukung oleh Gambar~\ref{fig:MemoryVSBatchSize} di mana pada ${batch\ size} = 16$, penggunaan memori ResNet50 (merah muda) sekitar $\SI{1000}{\mega\byte}$. Perhitungan yang sama untuk \en{batch size} lainnya juga memberikan hasil yang sesuai dengan grafik tersebut.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/memory-vs-batch-size.png}
    \caption{Penggunaan memori berbagai model \en{deep learning} terhadap ukuran \en{batch} \citep{canziani2016analysis}.}
    \label{fig:MemoryVSBatchSize}
\end{figure}

Dari hasil perhitungan tersebut, ditunjukkan bagaimana penggunaan memori meningkat seiring peningkatan ukuran \en{batch}. Selain itu, terlihat bahwa penggunaan terbesar memori terletak pada pemuatan hasil fungsi aktivasi (\en{output}) yang dibutuhkan untuk mencari gradien pada suatu \en{batch}. Hal ini bersesuaian dengan yang disebutkan di \citep{meng2017training} di mana \en{output} (pada \en{paper} tersebut disebut \en{feature map}) merupakan penyebab utama penggunaan memori pada pelatihan model.

Lebih lanjut, akan dilakukan eksperimen untuk memverifikasi kebenaran perhitungan penggunaan memori tersebut.

\subsection{Heuristik \en{Memory Swapping} di TensorFlow} \label{heuristic}

Seperti disebutkan sebelumnya, pada TensorFlow dapat digunakan \en{memory swapping} yang telah disediakan. Dari sisi \en{high-level}, TensorFlow menyediakan kelas \code{tf.nn.dynamic\_rnn} (model RNN dinamis) yang menerima parameter \code{swap\_memory} untuk memberi pilihan kepada pengguna apakah akan digunakan metode \en{memory swapping} saat pelatihan model.

Selanjutnya akan dijelaskan mekanisme \en{memory swapping} di Tensorflow dari sisi yang lebih \en{low-level}. Seperti yang telah dijelaskan pada Bagian~\ref{tensorflow}, program TensorFlow dapat dilihat sebagai \en{dataflow graph}, di mana \en{tensor} merupakan sisi graf dan simpul graf adalah operasi-operasi (\en{op}) terhadap \en{tensor}. Pada setiap \en{op}, TensorFlow meletakkan \en{input tensor} ke atas \en{memory stack} agar kemudian dapat diproses.

Berikut adalah cuplikan kode dari \en{codebase} TensorFlow mengenai \code{StackPushOp}, kelas yang bertanggung jawab dalam peletakkan \en{tensor} ke \en{memory stack} tersebut. Kode telah dimodifikasi agar hanya menampilkan bagian-bagian yang menjadi perhatian pembahasan.

\begin{lstlisting}[language=C++, caption=\code{tensorflow/core/kernels/stack\_ops.cc}, label={lst:SwapMemory}]
template <typename Device>
class StackPushOp : public AsyncOpKernel {

  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {
    static constexpr int kCopyThreshold = 2048;
    static constexpr double kOccupancy = 0.7;
    if (swap_memory_ && tensor.TotalBytes() > kCopyThreshold) {
      // ... retrieve device statistics
      if (stats.bytes_in_use > (stats.bytes_limit * kOccupancy)) {
        // ... copy the tensor from GPU to CPU memory (async)

        // push to stack the pointer to CPU memory
        // ...
          ctx->SetStatus(stack->Push({*cpu_tensor, alloc_attrs, true}));
        // ...
      }
      return
    }

    // if no memory swapping, just push tensor value to stack
    OP_REQUIRES_OK_ASYNC(ctx, stack->Push({tensor, alloc_attrs, false}), done);
    // ...
  }
}
\end{lstlisting}

Seperti terlihat pada Kode~\ref{lst:SwapMemory}, kelas \code{StackPushOp} memiliki \en{method} \code{ComputeAsync} yang berfungsi meletakkan \en{tensor} ke \en{memory stack}. \code{ComputeAsync} memiliki cara kerja sebagai berikut. Apabila \code{swap\_memory\_} bernilai \code{true} dan kondisi heuristiknya terpenuhi, maka dimulai pemindahan tensor dari GPU ke memori CPU. Setelahnya, yang diletakkan ke atas \en{memory stack} hanyalah \en{pointer} ke memori CPU tempat \en{tensor} tadi dipindahkan. Untuk memperjelas konteks, \en{pointer} ke memori CPU ini nantinya digunakan untuk menyalin kembali \en{tensor} dari CPU ke GPU ketika dibutuhkan.

Sebaliknya, bila \en{memory swapping} tidak dilakukan baik karena \code{swap\_memory\_} bernilai \code{false} atau heuristik tidak terpenuhi, \code{ComputeAsync} \en{by value} meletakkan \en{tensor} ke \en{memory stack}.

Kembali menelaah Kode \code{ComputeAsync}, heuristik yang digunakan untuk menentukan apakah \en{tensor} layak di-\en{swap} ke CPU cukup sederhana. Pertama, \code{ComputeAsync} memeriksa apakah ukuran \en{tensor} melebihi \code{kCopyThreshold} yang bernilai cukup \en{arbitrary}, yaitu 2048 \en{bytes}. Selanjutnya, \code{ComputeAsync} mengevaluasi apakah persentase penggunaan memori GPU saat ini melebihi \code{kOccupancy}, yang ditetapkan bernilai 0.7 (70\%).

\subsection{Potensi Optimisasi dengan \en{Memory Swapping}}

Pada umumnya, pelatihan model \en{deep learning} dilakukan dengan teknik \en{backpropagation}. Secara singkat, satu iterasi pelatihan terdiri dari 2 tahap, \en{forward pass} dan \en{backpropagation}. Pada \en{forward pass}, \en{input layer} menerima input kemudian mempropagasinya \en{layer} demi \en{layer} hingga didapatkan \en{output} di \en{output layer}. Setelahnya, \en{output} dibandingkan dengan label, kemudian dari selisihnya (\en{loss}) tersebut disesuaikan parameter-parameter model dari \en{output layer} hingga \en{input layer} sedemikian rupa sehingga secara keseluruhan, \en{loss} bernilai minimum. Mengenai \en{backpropagation} dijelaskan secara lebih mendalam pada Bagian~\ref{backpropagation} dan \ref{bptt}.

Melihat proses pelatihan dengan \en{backpropagation} tersebut, terdapat interval antara digunakannya suatu \en{layer} saat \en{forward pass} hingga digunakannya kembali saat \en{backpropagation}. Dilihat dari sisi penggunaan memori, pada interval ini \en{layer} tetap mengonsumsi memori walaupun tidak sedang digunakan. Konsumsi memori yang tidak perlu ini tentunya lebih baik bila dapat digunakan untuk keperluan lain.

Dari penjelasan tersebut, terlihat potensi optimisasi penggunaan memori pada pelatihan model \en{deep learning}. Memori untuk \en{layer}-\en{layer} yang sedang tidak digunakan tersebut lebih baik bila didelegasi ke \en{memory pool} yang lebih besar. Pada kasus pelatihan model dengan satu GPU, lebih baik bila memori yang digunakan \en{layer} di-\en{swap} dari GPU ke CPU apabila tidak sedang digunakan.

Menurut \citep{meng2017training}, \en{layer}-\en{layer} yang paling baik untuk di-\en{swap} adalah \en{layer}-\en{layer} awal, karena interval dari \en{forward pass} hingga digunakannya kembali saat \en{backpropagation} terpanjang. Dengan begitu, diharapkan bahwa \en{overlapping} antara komputasi dengan \en{memory swapping} meningkat, sehingga mengurangi waktu yang dibutuhkan (karena \en{memory copy} antara GPU dan CPU berjalan secara \en{asynchronous}).

Bila melihat heuristik \en{memory swapping} di TensorFlow yang dijelaskan pada bagian sebelumnya, terlihat bahwa yang dilakukan TensorFlow justru kebalikannya. Karena \en{swapping} dilakukan bila memori melebihi \code{kOccupancy}, berarti \en{tensor} yang di-\en{swap} adalah \en{tensor} pada \en{layer-layer} akhir. Di sinilah terletak potensi optimisasi \en{memory swapping} di TensorFlow tersebut.

\subsection{\en{Recurrent Neural Network} sebagai Model Uji}

Arsitektur \en{deep neural network} yang dipilih untuk menjadi fokus optimisasi ini adalah \en{recurrent neural network} (RNN) yang antara \en{input layer} atau \en{output layer}-nya (atau keduanya) berjenis \en{sequence}. Alasan pemilihan arsitektur yang demikian adalah karena topologinya yang dinamis sesuai panjang \en{input} atau \en{output}, penggunaan memori tidak dapat diprediksi sebelum eksekusi, sehingga optimisasi memori pun menjadi lebih krusial.

Selain itu, seperti yang dijelaskan pada Bagian~\ref{bptt} lebih kompleks dan lebih sulit dilatih dari \en{deep neural network} biasa, ditunjukkan dengan perhitungan \en{backpropagation through time} pada bagian tersebut yang lebih banyak membutuhkan komputasi (dan memori) dibandingkan dengan \en{backpropagation} biasa pada Bagian~\ref{backpropagation}.

Alasan lainnya adalah fitur \en{memory swapping} pada TensorFlow hanya terdapat pada \code{tf.nn.dynamic\_rnn} (kelas RNN dinamis), sehingga dengan memilih model berarsitektur RNN, dapat dibandingkan secara langsung \en{memory swapping} hasil optimisasi terhadap \en{memory swapping} bawaan dari TensorFlow.

\section{Rancangan Solusi} \label{rancangansolusi}

Setelah menganalisis permasalahan didapatkan bahwa terdapat potensi optimisasi dengan \en{memory swapping} pada TensorFlow. Pada bagian sebelumnya, disebutkan bahwa \en{memory swapping} akan dioptimisasi dengan memprioritaskan \en{tensor} dengan interval waktu yang lama hingga digunakan kembali saat \en{backpropagation}. Untuk itu, metode optimisasi tersebut perlu diimplementasikan ke TensorFlow.

Pada Bagian~\ref{relatedworks}, disebutkan bahwa \en{paper} \citep{meng2017training} melakukan optimisasi dengan ide yang serupa. Pada penelitian tersebut, optimisasi diimplementasi dengan menambahkan \en{op} baru ke TensorFlow, yaitu \code{swap\_out} dan \code{swap\_in} yang berfungsi memindahkan \en{tensor} dari \en{device} ke \en{host}, dan sebaliknya. Kedua \en{op} tersebut kemudian disisipkan ke graf TensorFlow, seperti ditunjukkan oleh Gambar~\ref{fig:SwapOutIn}. Sayangnya, \en{paper} tersebut kurang menjelaskan bagaimana kedua \en{op} diimplementasikan dan bagaimana disisipkan ke graf TensorFlow.

Karena studi kasus yang digunakan adalah arsitektur RNN, maka solusi terhadap permasalahan berfokus pada kelas-kelas RNN yang disediakan oleh TensorFlow, seperti \code{tf.nn.nn\_cell.RNNCell} dan \code{tf.nn.nn\_cell.LSTMCell}. Akan dianalisis \en{source code} kelas-kelas RNN tersebut, terutama bagaimana \en{StackPushOp} pada Kode~\ref{lst:SwapMemory} dipanggil, kemudian memodifikasinya mengikuti ide yang diajukan pada analisis permasalahan dan pada \en{paper} \citep{meng2017training}.

Kemudian akan dilakukan pengujian program TensorFlow asli dengan program TensorFlow hasil modifikasi tersebut. Pengujian tersebut dilakukan pada sebuah model RNN, dengan variabel-variabel berupa jumlah \en{layer} dan neuron, ukuran \en{batch}, dan ukuran \en{dataset}. Dari pengujian tersebut, ingin diketahui perbandingan penggunaan memori dan durasi \en{training} antara program TensorFlow asli (baik yang menggunakan dan tidak menggunakan \en{memory swapping}) dengan program TensorFlow hasil modifikasi.
