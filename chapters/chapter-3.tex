\chapter{Analisis Permasalahan dan Rancangan Solusi}

\section{Analisis Permasalahan} \label{problemanalysis}

Di samping kinerja model, tantangan dalam melatih model \en{deep learning} meliputi panjangnya durasi pelatihan dan besarnya sumber daya komputasi yang dibutuhkan, seperti kapasitas memori. Salah satunya penyebabnya adalah besarnya jumlah dan ukuran \en{hidden layer}, meningkatkan jumlah nilai-nilai yang harus dikomputasi. Selain itu, pelatihan model \en{deep learning} seringkali menggunakan \en{dataset} berukuran besar, menambah durasi pelatihan dan sumber daya yang dibutuhkan.

Pada tugas akhir ini, fokus penelitian terletak pada optimisasi penggunaan memori untuk pelatihan model, khususnya pada bagian \en{memory swapping} yang akan dijelaskan kemudian. Pada pelatihan model \en{deep learning}, mayoritas memori digunakan untuk me-\en{maintain} nilai-nilai seperti bobot, bias, hasil fungsi aktivasi, serta data yang sedang diproses. Kelebihan penggunaan memori dari kapasitas yang ada dapat mengakibatkan gagalnya pelatihan model karena masalah \en{out of memory} (OOM).

Satu hal yang  memperparah masalah ini adalah kapasitas memori GPU. Saat ini, GPU banyak digunakan untuk melatih model \en{deep learning} karena sifat paralelismenya yang tinggi, sehingga dapat melatih model dengan lebih cepat dari CPU. Akan tetapi, ukuran memori GPU cenderung lebih kecil dari ukuran memori CPU. Sebagai gambaran, saat ini memori GPU umumnya berukuran sekitar 2-16 GB sedangkan memori CPU dapat berukuran hingga 1TB.

Salah satu penanganan masalah tersebut adalah \en{memory swapping}, yaitu metode pengurangan beban memori dengan memindahkan isi memori secara temporer ke \en{memory pool} yang lebih besar (dan biasanya memiliki waktu akses lebih lambat). Pada kasus ini, sebagian isi memori GPU dapat dipindahkan ke memori CPU secara sementara. Dalam berbagai kasus, terutama dengan kapasitas memori yang terbatas, \en{memory swapping} merupakan satu-satunya solusi terhadap masalah OOM.

Namun, \en{memory swapping} memiliki \en{trade-off} tersendiri. Seperti pemindahan data apapun, memindahkan data dari GPU ke CPU (dan sebaliknya) juga membutuhkan waktu tersendiri, berbanding lurus dengan jumlah data yang dipindahkan. Maka dari itu, \en{memory swapping} menambah durasi pelatihan yang bertambahnya semakin signifikan seiring  akumulasi data yang di-\en{swap}.

Penjabaran tersebut menunjukkan bahwa \en{memory swapping} antara GPU dan CPU pada pelatihan model \en{deep learning} merupakan permasalahan yang layak dioptimisasi, agar \en{memory swapping} dapat digunakan tanpa menambah durasi pelatihan secara terlalu besar.

Salah satu \en{framework} pembelajaran mesin, TensorFlow, memiliki fitur \en{memory swapping}. Akan tetapi, terdapat potensi bahwa \en{memory swapping} di TensorFlow masih kurang optimal, seperti yang dijelaskan secara lebih detail di Bagian~\ref{heuristic}.

Melihat potensi optimisasi tersebut, fokus tugas akhir ini adalah meneliti dan mengembangkan \en{memory swapping} di TensorFlow sehingga lebih optimal dari yang sudah ada, terutama dari faktor durasi pelatihan.

\subsection{Penggunaan Memori pada Pelatihan Model}

Berikut akan dibahas mengenai penyebab penggunaan memori pada pelatihan model serta perhitungannya. Selain untuk memuat \en{dataset} yang digunakan, memori juga digunakan untuk me-\en{maintain} variabel yang memuat parameter model (bobot dan bias) dan hasil fungsi aktivasi (\en{output}). Bila variabel direpresentasikan oleh 32-bit \en{floating point}, maka ukuran memori yang digunakan adalah 4 \en{byte} dikali dengan jumlah variabel yang ada. Sebagai contoh, model ResNet50 memiliki sekitar 26 juta parameter dan 16 juta \en{output} pada sekali \en{forward pass}. Maka, ukuran memori yang digunakan pada satu sampel sekitar $\num{26e6} \times \SI{4}{byte} = \SI{104}{\mega\byte}$ untuk parameter dan $\num{16e6} \times \SI{4}{byte} = \SI{64}{\mega\byte}$ untuk \en{output} \citep{hanlon2016why}.

Ukuran tersebut barulah ukuran memori yang digunakan oleh satu sampel pada \en{batch}. Biasanya, pelatihan model berlangsung secara paralel, yang berarti dilakukan pencarian gradien terhadap seluruh sampel pada \en{batch} sekaligus. Berarti, perlu dimuat \en{output} seluruh sampel karena \en{output} dibutuhkan untuk pencarian gradien dengan \en{backpropagation} (seperti dijelaskan pada Bagian~\ref{backpropagation} dan \ref{bptt}).

Misalnya \en{batch} berukuran 16, maka diperlukan $16 * \SI{64}{\mega\byte} = \SI{1024}{\mega\byte}$ untuk memuat seluruh \en{output} pada satu \en{batch}. Bila dijumlah dengan ukuran parameter (bobot dan bias) seperti di atas, maka diperlukan sekitar $\SI{104}{\mega\byte} + \SI{1024}{\mega\byte} = \SI{1.128}{\giga\byte}$ memori pada pelatihan model ResNet50 untuk \en{batch} berukuran 16. Perhitungan ini didukung oleh Gambar~\ref{fig:MemoryVSBatchSize} di mana pada ${batch\ size} = 16$, penggunaan memori ResNet50 (merah muda) sekitar $\SI{1000}{\mega\byte}$. Perhitungan yang sama untuk \en{batch size} lainnya juga memberikan hasil yang sesuai dengan grafik tersebut.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/memory-vs-batch-size.png}
    \caption{Penggunaan memori berbagai model \en{deep learning} terhadap ukuran \en{batch} \citep{canziani2016analysis}.}
    \label{fig:MemoryVSBatchSize}
\end{figure}

Dari hasil perhitungan tersebut, ditunjukkan bagaimana penggunaan memori meningkat seiring peningkatan ukuran \en{batch}. Selain itu, terlihat bahwa penggunaan terbesar memori terletak pada pemuatan hasil fungsi aktivasi (\en{output}) yang dibutuhkan untuk mencari gradien pada suatu \en{batch}. Hal ini bersesuaian dengan yang disebutkan di \citep{meng2017training} di mana \en{output} (pada \en{paper} tersebut disebut \en{feature map}) merupakan penyebab utama penggunaan memori pada pelatihan model.

Lebih lanjut, akan dilakukan eksperimen untuk memverifikasi kebenaran perhitungan penggunaan memori tersebut.

\subsection{Heuristik \en{Memory Swapping} di TensorFlow} \label{heuristic}

Seperti disebutkan sebelumnya, pada TensorFlow dapat digunakan \en{memory swapping} yang telah disediakan. Dari sisi \en{high-level}, TensorFlow menyediakan kelas \code{tf.nn.dynamic\_rnn} (model RNN dinamis) yang menerima parameter \code{swap\_memory} untuk memberi pilihan kepada pengguna apakah akan digunakan metode \en{memory swapping} saat pelatihan model.

Selanjutnya akan dijelaskan mekanisme \en{memory swapping} di Tensorflow dari sisi yang lebih \en{low-level}. Seperti yang telah dijelaskan pada Bagian~\ref{tensorflow}, program TensorFlow dapat dilihat sebagai \en{dataflow graph}, di mana \en{tensor} merupakan sisi graf dan simpul graf adalah operasi-operasi (\en{op}) terhadap \en{tensor}. Pada setiap \en{op}, TensorFlow meletakkan \en{input tensor} ke atas \en{memory stack} agar kemudian dapat diproses.

Berikut adalah cuplikan kode dari \en{codebase} TensorFlow mengenai \code{StackPushOp}, kelas yang bertanggung jawab dalam peletakkan \en{tensor} ke \en{memory stack} tersebut. Kode telah dimodifikasi agar hanya menampilkan bagian-bagian yang menjadi perhatian pembahasan.

\begin{lstlisting}[language=C++, caption=\code{tensorflow/core/kernels/stack\_ops.cc}, label={lst:SwapMemory}]
template <typename Device>
class StackPushOp : public AsyncOpKernel {

  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {
    static constexpr int kCopyThreshold = 2048;
    static constexpr double kOccupancy = 0.7;
    if (swap_memory_ && tensor.TotalBytes() > kCopyThreshold) {
      // ... retrieve device statistics
      if (stats.bytes_in_use > (stats.bytes_limit * kOccupancy)) {
        // ... copy the tensor from GPU to CPU memory (async)

        // push to stack the pointer to CPU memory
        // ...
          ctx->SetStatus(stack->Push({*cpu_tensor, alloc_attrs, true}));
        // ...
      }
      return
    }

    // if no memory swapping, just push tensor value to stack
    OP_REQUIRES_OK_ASYNC(ctx, stack->Push({tensor, alloc_attrs, false}), done);
    // ...
  }
}
\end{lstlisting}

Seperti terlihat pada Kode~\ref{lst:SwapMemory}, kelas \code{StackPushOp} memiliki \en{method} \code{ComputeAsync} yang berfungsi meletakkan \en{tensor} ke \en{memory stack}. \code{ComputeAsync} memiliki cara kerja sebagai berikut. Apabila \code{swap\_memory\_} bernilai \code{true} dan kondisi heuristiknya terpenuhi, maka dimulai pemindahan tensor dari GPU ke memori CPU. Setelahnya, yang diletakkan ke atas \en{memory stack} hanyalah \en{pointer} ke memori CPU tempat \en{tensor} tadi dipindahkan. Untuk memperjelas konteks, \en{pointer} ke memori CPU ini nantinya digunakan untuk menyalin kembali \en{tensor} dari CPU ke GPU ketika dibutuhkan.

Sebaliknya, bila \en{memory swapping} tidak dilakukan baik karena \code{swap\_memory\_} bernilai \code{false} atau heuristik tidak terpenuhi, \code{ComputeAsync} \en{by value} meletakkan \en{tensor} ke \en{memory stack}.

Kembali menelaah Kode \code{ComputeAsync}, heuristik yang digunakan untuk menentukan apakah \en{tensor} layak di-\en{swap} ke CPU cukup sederhana. Pertama, \code{ComputeAsync} memeriksa apakah ukuran \en{tensor} melebihi \code{kCopyThreshold} yang bernilai cukup \en{arbitrary}, yaitu 2048 \en{bytes}. Selanjutnya, \code{ComputeAsync} mengevaluasi apakah persentase penggunaan memori GPU saat ini melebihi \code{kOccupancy}, yang ditetapkan bernilai 0.7 (70\%).

\subsection{Potensi Optimisasi dengan \en{Memory Swapping}}

Pada umumnya, pelatihan model \en{deep learning} dilakukan dengan teknik \en{backpropagation}. Secara singkat, satu iterasi pelatihan terdiri dari 2 tahap, \en{forward pass} dan \en{backpropagation}. Pada \en{forward pass}, \en{input layer} menerima input kemudian mempropagasinya \en{layer} demi \en{layer} hingga didapatkan \en{output} di \en{output layer}. Setelahnya, \en{output} dibandingkan dengan label, kemudian dari selisihnya (\en{loss}) tersebut disesuaikan parameter-parameter model dari \en{output layer} hingga \en{input layer} sedemikian rupa sehingga secara keseluruhan, \en{loss} bernilai minimum. Mengenai \en{backpropagation} dijelaskan secara lebih mendalam pada Bagian~\ref{backpropagation} dan \ref{bptt}.

Melihat proses pelatihan dengan \en{backpropagation} tersebut, terdapat interval antara digunakannya suatu \en{layer} saat \en{forward pass} hingga digunakannya kembali saat \en{backpropagation}. Dilihat dari sisi penggunaan memori, pada interval ini \en{layer} tetap mengonsumsi memori walaupun tidak sedang digunakan. Konsumsi memori yang tidak perlu ini tentunya lebih baik bila dapat digunakan untuk keperluan lain.

Dari penjelasan tersebut, terlihat potensi optimisasi penggunaan memori pada pelatihan model \en{deep learning}. Memori untuk \en{layer}-\en{layer} yang sedang tidak digunakan tersebut lebih baik bila didelegasi ke \en{memory pool} yang lebih besar. Pada kasus pelatihan model dengan satu GPU, lebih baik bila memori yang digunakan \en{layer} di-\en{swap} dari GPU ke CPU apabila tidak sedang digunakan.

Menurut \citep{meng2017training}, \en{layer}-\en{layer} yang paling baik untuk di-\en{swap} adalah \en{layer}-\en{layer} awal, karena interval dari \en{forward pass} hingga digunakannya kembali saat \en{backpropagation} terpanjang. Dengan begitu, diharapkan bahwa \en{overlapping} antara komputasi dengan \en{memory swapping} meningkat, sehingga mengurangi waktu yang dibutuhkan (karena \en{memory copy} antara GPU dan CPU berjalan secara \en{asynchronous}).

Bila melihat heuristik \en{memory swapping} di TensorFlow yang dijelaskan pada bagian sebelumnya, terlihat bahwa yang dilakukan TensorFlow justru kebalikannya. Karena \en{swapping} dilakukan bila memori melebihi \code{kOccupancy}, berarti \en{tensor} yang di-\en{swap} adalah \en{tensor} pada \en{layer-layer} akhir. Di sinilah terletak potensi optimisasi \en{memory swapping} di TensorFlow tersebut.

\subsection{\en{Recurrent Neural Network} sebagai Model Uji}

Arsitektur \en{deep neural network} yang dipilih untuk menjadi fokus optimisasi ini adalah \en{recurrent neural network} (RNN) yang antara \en{input layer} atau \en{output layer}-nya (atau keduanya) berjenis \en{sequence}. Alasan pemilihan arsitektur yang demikian adalah karena topologinya yang dinamis sesuai panjang \en{input} atau \en{output}, penggunaan memori tidak dapat diprediksi sebelum eksekusi, sehingga optimisasi memori pun menjadi lebih krusial.

Selain itu, seperti yang dijelaskan pada Bagian~\ref{bptt} lebih kompleks dan lebih sulit dilatih dari \en{deep neural network} biasa, ditunjukkan dengan perhitungan \en{backpropagation through time} pada bagian tersebut yang lebih banyak membutuhkan komputasi (dan memori) dibandingkan dengan \en{backpropagation} biasa pada Bagian~\ref{backpropagation}.

Alasan lainnya adalah fitur \en{memory swapping} pada TensorFlow hanya terdapat pada \code{tf.nn.dynamic\_rnn} (kelas RNN dinamis), sehingga dengan memilih model berarsitektur RNN, dapat dibandingkan secara langsung \en{memory swapping} hasil optimisasi terhadap \en{memory swapping} bawaan dari TensorFlow.

\section{Rancangan Solusi} \label{rancangansolusi}

Setelah menganalisis permasalahan didapatkan bahwa terdapat potensi optimisasi dengan \en{memory swapping} pada TensorFlow. Pada bagian sebelumnya, disebutkan bahwa \en{memory swapping} akan dioptimisasi dengan memprioritaskan \en{tensor} dengan interval waktu yang lama hingga digunakan kembali saat \en{backpropagation}. Untuk itu, metode optimisasi tersebut perlu diimplementasikan ke TensorFlow.

Pada Bagian~\ref{relatedworks}, disebutkan bahwa \en{paper} \citep{meng2017training} melakukan optimisasi dengan ide yang serupa. Pada penelitian tersebut, optimisasi diimplementasi dengan menambahkan \en{op} baru ke TensorFlow, yaitu \code{swap\_out} dan \code{swap\_in} yang berfungsi memindahkan \en{tensor} dari \en{device} ke \en{host}, dan sebaliknya. Kedua \en{op} tersebut kemudian disisipkan ke graf TensorFlow, seperti ditunjukkan oleh Gambar~\ref{fig:SwapOutIn}. Sayangnya, \en{paper} tersebut kurang menjelaskan bagaimana kedua \en{op} diimplementasikan dan bagaimana disisipkan ke graf TensorFlow.

Karena studi kasus yang digunakan adalah arsitektur RNN, maka solusi terhadap permasalahan berfokus pada kelas-kelas RNN yang disediakan oleh TensorFlow, seperti \code{tf.nn.nn\_cell.RNNCell} dan \code{tf.nn.nn\_cell.LSTMCell}. Akan dianalisis \en{source code} kelas-kelas RNN tersebut, terutama bagaimana \en{StackPushOp} pada Kode~\ref{lst:SwapMemory} dipanggil, kemudian memodifikasinya mengikuti ide yang diajukan pada analisis permasalahan dan pada \en{paper} \citep{meng2017training}.

Kemudian akan dilakukan pengujian program TensorFlow asli dengan program TensorFlow hasil modifikasi tersebut. Pengujian tersebut dilakukan pada sebuah model RNN, dengan variabel-variabel berupa jumlah \en{layer} dan neuron, ukuran \en{batch}, dan ukuran \en{dataset}. Dari pengujian tersebut, ingin diketahui perbandingan penggunaan memori dan durasi \en{training} antara program TensorFlow asli (baik yang menggunakan dan tidak menggunakan \en{memory swapping}) dengan program TensorFlow hasil modifikasi.
