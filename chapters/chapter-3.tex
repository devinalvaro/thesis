\chapter{Analisis Permasalahan dan Rancangan Solusi}

\section{Analisis Permasalahan} \label{analisispermasalahan}

\en{Deep learning} merupakan metode pembelajaran mesin menggunakan \en{neural network} dengan \en{hidden layer} berjumlah banyak. Di luar kinerja model, tantangan yang terdapat pada pelatihan model \en{deep learning} meliputi panjangnya durasi pelatihan dan besarnya sumber daya komputasi (seperti kapasitas CPU dan GPU) yang dibutuhkan. Salah satunya penyebabnya adalah banyaknya jumlah \en{hidden layer} yang mengakibatkan banyaknya pula parameter-parameter yang harus di-\en{maintain}, sehingga meningkatkan durasi dan komputasi yang dilakukan saat pelatihan model. Selain itu, pelatihan model \en{deep learning} seringkali menggunakan data berukuran besar, menambah durasi dan sumber daya yang dibutuhkan.

Pada tugas akhir ini, fokus penelitian terletak pada penggunaan memori untuk pelatihan model \en{deep learning}. Pada pelatihan model \en{deep learning}, memori terutama digunakan untuk me-\en{maintain} nilai parameter-parameter model seperti \en{weight}, \en{bias}, hasil fungsi aktivasi, serta data yang sedang diproses. Kelebihan penggunaan memori dari kapasitas yang ada dapat mengakibatkan gagalnya pelatihan model karena masalah \en{out of memory}.

Saat ini, terdapat berbagai penanganan terhadap masalah tersebut. Salah satunya adalah mengurangi jumlah \en{hidden layer} serta neuron-neuronnya yang secara langsung mengurangi jumlah parameter yang harus di-\en{maintain} di memori. Akan tetapi, metode ini berpotensi mengurangi kinerja, apabila topologi model menjadi kurang dari optimal.

Metode penanganan lain disebut \en{mini-batch gradient descent} (MBGD), di mana data pelatihan dibagi menjadi beberapa \en{mini-batch} (atau \en{batch}). Seperti yang telah disebutkan sebelumnya, data pelatihan \en{deep learning} seringkali berukuran besar, sehingga mahal secara waktu dan komputasi. Tujuan MBGD adalah mengurangi ukuran data yang diproses pada suatu waktu, sehingga mengurangi komputasi (termasuk penggunaan memori) dan durasi pelatihan. Namun, MBGD memiliki \en{trade-off} tersendiri, yaitu karena data yang diproses pada suatu waktu hanya sebagian, maka data yang diproses kurang representatif dari data sebenarnya. Seperti sebelumnya, hal ini berpotensi mengurangi keoptimalan kinerja model.

Dari penjabaran tersebut ditunjukkan bahwa optimisasi penggunaan memori GPU pada pelatihan model \en{deep learning} merupakan permasalahan yang layak dipecahkan, karena dengan sumber daya yang sama dapat dilatih model dengan jumlah \en{layer} dan parameter yang lebih banyak serta meningkatkan ukuran data yang dapat diproses pada suatu waktu (ukuran \en{batch}).

Satu lagi permasalahan memori pada pelatihan model \en{deep learning} adalah kapasitas GPU. Karena sifat paralelismenya yang tinggi, pelatihan menggunakan GPU lebih cepat dari CPU. Akan tetapi, pada umumnya memori GPU berukuran 8-12 kali lebih kecil dari memori CPU. Terutama pada pelatihan model dengan satu GPU, masalah ini menjadi signifikan karena \en{memory load} tidak dapat didistribusi ke banyak GPU sekaligus.

Salah satu penanganan permasalahan tersebut pada saat ini adalah \en{memory swapping}. \en{Memory swapping} adalah metode pengurangan beban memori dengan menyimpan data secara sementara di \en{memory pool} yang lebih besar (dan biasanya lebih lambat). Pada kasus \en{deep learning}, sebagian memori di GPU dapat disimpan di memori CPU secara sementara.

\en{Memory swapping} telah diterapkan di beberapa \en{framework} pembelajaran mesin seperti TensorFlow. Akan tetapi, pada TensorFlow pengambilan keputusan mengenai data apa yang di-\en{swap} dan kapan melakukan \en{swapping} hanya menggunakan heuristik sederhana. Heuristik tersebut akan dijelaskan secara lebih detail di bawah.

Melihat celah optimisasi tersebut, demikian fokus tugas akhir ini adalah mengoptimisasi penggunaan memori pada pelatihan model \en{deep learning} dengan satu GPU di TensorFlow.

\subsection{Penggunaan Memori pada Pelatihan Model}

Berbagai penelitian menunjukkan bahwa pada model \en{deep learning}, menambah jumlah neuron dan \en{hidden layer} dapat meningkatkan kinerja model \citep{dean2012large,nielsen2015neural}. Tetapi, meningkatkan kompleksitas model seperti demikian juga meningkatkan \en{training cost}, seperti waktu pelatihan dan ukuran memori yang dibutuhkan. Gambar~\ref{fig:MemoryVSBatchSize} menunjukkan utilisasi memori beberapa \en{pre-trained model} terhadap ukuran \en{batch}. Berikutnya, akan dibahas beberapa penyebab meningkatnya kebutuhan memori pada model \en{deep learning} seiring meningkatnya kompleksitas model.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/memory-vs-batch-size.png}
    \caption{Penggunaan memori berbagai model \en{deep learning} terhadap ukuran \en{batch} \citep{canziani2016analysis}.}
    \label{fig:MemoryVSBatchSize}
\end{figure}

Selain untuk memuat \en{dataset} yang digunakan, memori juga digunakan untuk variabel yang memuat parameter model (bobot dan bias) dan hasil komputasi seperti fungsi aktivasi. Katakanlah bahwa variabel direpresentasikan oleh 32-bit \en{floating point}, maka ukuran memori yang digunakan adalah 4 \en{byte} dikali dengan jumlah variabel. Sebagai contoh, model ResNet50 memiliki sekitar 26 juta parameter dan 16 juta fungsi aktivasi pada sekali \en{forward pass}. Maka, ukuran memori yang digunakan pada satu sampel sekitar $\num{26e6} \times \SI{4}{byte} = \SI{104}{\mega\byte}$ untuk parameter dan $\num{16e6} \times \SI{4}{byte} = \SI{64}{\mega\byte}$ untuk fungsi aktivasi \citep{hanlon2016why}.

Ukuran tersebut barulah ukuran memori yang digunakan oleh satu sampel pada \en{batch}. Seperti dijelaskan sebelumnya, pelatihan model dengan MBGD meng-\en{update} parameter model (dengan gradien yang didapat dari fungsi aktivasi setiap neuron) sekaligus per \en{batch}. Oleh karena itu, untuk setiap \en{batch} perlu dimuat seluruh fungsi aktivasi yang didapat dari setiap sampel pada \en{batch}. Misalnya \en{batch} berukuran 16, maka diperlukan $16 * \SI{64}{\mega\byte} = \SI{1024}{\mega\byte}$ untuk memuat seluruh hasil fungsi aktivasi pada satu \en{batch}. Bila dijumlah dengan ukuran parameter tadi, maka diperlukan sekitar $\SI{104}{\mega\byte} + \SI{1024}{\mega\byte} = \SI{1.128}{\giga\byte}$ memori pada pelatihan model ResNet50 untuk \en{batch} berukuran 16. Perhitungan ini didukung oleh Gambar~\ref{fig:MemoryVSBatchSize} di mana pada ${batch\ size} = 16$, penggunaan memori ResNet50 (merah muda) sekitar $\SI{1000}{\mega\byte}$.

Dari hasil perhitungan tersebut, ditunjukkan bahwa penggunaan terbesar memori terletak pada pemuatan hasil fungsi aktivasi yang dibutuhkan untuk mencari gradien pada suatu \en{batch}. Selain itu, ditunjukkan juga bagaimana penggunaan memori meningkat seiring peningkatan ukuran \en{batch}.

\subsection{Heuristik \en{Memory Swapping} di TensorFlow}

Seperti disebutkan sebelumnya, pada TensorFlow dapat digunakan \en{memory swapping} untuk mengurangi penggunaan memori. Dari sisi \en{high-level}, TensorFlow menyediakan kelas-kelas seperti \code{tf.nn.dynamic\_rnn} (model RNN dinamis) yang menerima parameter \code{swap\_memory} untuk memberi pilihan kepada pengguna apakah akan digunakan metode \en{memory swapping} saat pelatihan model.

Selanjutnya akan dijelaskan mekanisme \en{memory swapping} di Tensorflow dari sisi yang lebih \en{low-level}. Seperti telah dijelaskan pada Bab Studi Literatur, program TensorFlow dapat dilihat sebagai \en{dataflow graph}, di mana \en{tensor} merupakan sisi graf dan simpul graf adalah operasi-operasi (\en{op}) terhadap \en{tensor}. Pada setiap \en{op}, TensorFlow meletakkan \en{input tensor} ke atas \en{memory stack} agar kemudian dapat diproses.

Berikut adalah cuplikan kode dari \en{codebase} TensorFlow mengenai \code{StackPushOp}, kelas yang bertanggung jawab dalam peletakkan tensor ke \en{memory stack} tersebut. Kode telah dimodifikasi agar hanya menampilkan bagian-bagian yang menjadi perhatian pembahasan.

\begin{lstlisting}[language=C++, caption=\code{tensorflow/core/kernels/stack\_ops.cc}, label={lst:SwapMemory}]
template <typename Device>
class StackPushOp : public AsyncOpKernel {

  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {
    static constexpr int kCopyThreshold = 2048;
    static constexpr double kOccupancy = 0.7;
    if (swap_memory_ && tensor.TotalBytes() > kCopyThreshold) {
      // ... retrieve device statistics
      if (stats.bytes_in_use > (stats.bytes_limit * kOccupancy)) {
        // ... copy the tensor from GPU to CPU memory (async)

        // push to stack the pointer to CPU memory
        // ...
          ctx->SetStatus(stack->Push({*cpu_tensor, alloc_attrs, true}));
        // ...
      }
      return
    }

    // if no memory swapping, just push tensor value to stack
    OP_REQUIRES_OK_ASYNC(ctx, stack->Push({tensor, alloc_attrs, false}), done);
    // ...
  }
}
\end{lstlisting}

Seperti terlihat pada Kode~\ref{lst:SwapMemory}, kelas \code{StackPushOp} memiliki \en{method} \code{ComputeAsync} yang berfungsi meletakkan \en{tensor} ke \en{memory stack}. \code{ComputeAsync} memiliki cara kerja sebagai berikut. Apabila \code{swap\_memory\_} bernilai \code{true} dan kondisi heuristiknya terpenuhi, maka dimulai pemindahan tensor dari GPU ke memori CPU. Setelahnya, yang diletakkan ke atas \en{memory stack} hanyalah \en{pointer} ke memori CPU tempat \en{tensor} tadi dipindahkan. Untuk memperjelas konteks, \en{pointer} ke memori CPU ini nantinya digunakan untuk menyalin kembali \en{tensor} dari CPU ke GPU ketika dibutuhkan.

Sebaliknya, bila \en{memory swapping} tidak dilakukan baik karena \code{swap\_memory\_} bernilai \code{false} atau heuristik tidak terpenuhi, \code{ComputeAsync} \en{by value} meletakkan \en{tensor} ke \en{memory stack}.

Kembali menelaah Kode \code{ComputeAsync}, heuristik yang digunakan untuk menentukan apakah \en{tensor} layak di-\en{swap} ke CPU cukup sederhana. Pertama, \code{ComputeAsync} memeriksa apakah ukuran \en{tensor} melebihi \code{kCopyThreshold} yang bernilai cukup \en{arbitrary}, yaitu 2048 \en{bytes}. Selanjutnya, \code{ComputeAsync} mengevaluasi apakah persentase penggunaan memori GPU saat ini melebihi \code{kOccupancy}, yang ditetapkan bernilai 0.7 (70\%).

\subsection{Potensi Optimisasi dengan \en{Memory Swapping}}

Pada umumnya, pelatihan model \en{deep learning} dilakukan dengan teknik \en{backpropagation}. Secara singkat, satu iterasi pelatihan terdiri dari 2 tahap, \en{forward propagation} dan \en{backward propagation} (atau \en{backpropagation}). Pada \en{forward propagation}, \en{input layer} menerima input kemudian mempropagasinya \en{layer} demi \en{layer} hingga didapatkan \en{output} di \en{output layer}. Setelahnya, \en{output} dibandingkan dengan label, kemudian dari selisihnya (\en{loss}) tersebut disesuaikan parameter-parameter model dari \en{output layer} sampai \en{input layer} sedemikian rupa sehingga secara keseluruhan, \en{loss} bernilai minimum.

Melihat proses pelatihan dengan \en{backpropagation} tersebut, terdapat interval antara digunakannya suatu \en{layer} saat \en{forward propagation} sampai digunakannya kembali saat \en{backpropagation}. Dilihat dari sisi penggunaan memori, pada interval ini \en{layer} tetap mengonsumsi memori walaupun tidak sedang digunakan. Konsumsi memori yang tidak perlu ini tentunya lebih baik bila dapat digunakan untuk keperluan lain.

Dari penjelasan tersebut, terlihat potensi optimisasi penggunaan memori pada pelatihan model \en{deep learning}. Memori untuk \en{layer}-\en{layer} yang sedang tidak digunakan tersebut lebih baik bila didelegasi ke \en{memory pool} yang lebih besar. Pada kasus pelatihan model dengan satu GPU, lebih baik bila memori yang digunakan \en{layer} di-\en{swap} dari GPU ke CPU apabila tidak sedang digunakan.

Bila melihat heuristik \en{memory swapping} di TensorFlow yang dijelaskan pada bagian sebelumnya, terlihat bahwa yang dilakukan TensorFlow justru kebalikannya. Karena \en{swapping} dilakukan bila memori melebihi \code{kOccupancy}, berarti \en{tensor} yang di-\en{swap} adalah \en{tensor} pada \en{layer-layer} akhir. Di sinilah terletak potensi optimisasi \en{memory swapping} di TensorFlow tersebut.

Selain itu, perlu diingat bahwa \en{memory swapping} di TensorFlow hingga saat ini (v1.12) masih terbatas pada satu model saja, yaitu kelas \code{tf.nn.dynamic\_rnn}, yaitu model RNN topologinya dinamis sesuai sekuens yang diberikan. Padahal, metode \en{memory swapping} seharusnya adalah teknik yang umum (tidak terikat arsitektur tertentu).

\subsection{\en{Recurrent Neural Network} sebagai Model Uji}

Arsitektur \en{deep neural network} yang dipilih untuk menjadi fokus optimisasi ini adalah \en{recurrent neural network} (RNN) yang antara \en{input layer} atau \en{output layer}-nya (atau keduanya) berjenis \en{sequence}. Alasan pemilihan arsitektur yang demikian adalah karena topologinya yang dinamis sesuai panjang \en{input} atau \en{output}, penggunaan memori tidak dapat diprediksi sebelum eksekusi, sehingga optimisasi memori pun menjadi lebih krusial.

Alasan lainnya adalah fitur \en{memory swapping} pada TensorFlow hanya terdapat pada \code{tf.nn.dynamic\_rnn}, sehingga dengan memilih model berarsitektur RNN, dapat diuji pula \en{memory swapping} hasil optimisasi terhadap \en{memory swapping} bawaan dari TensorFlow.

\section{Rancangan Solusi} \label{rancangansolusi}

Setelah menganalisis permasalahan didapatkan bahwa terdapat potensi optimisasi dengan \en{memory swapping} pada TensorFlow. Pada bagian sebelumnya, disebutkan bahwa \en{memory swapping} akan dioptimisasi dengan memprioritaskan \en{tensor} dengan interval waktu yang lama hingga digunakan kembali saat \en{backpropagation}. Untuk itu, metode optimisasi tersebut perlu diimplementasikan ke TensorFlow.

Pada Studi Literatur bagian Penelitian Terkait, disebutkan bahwa \en{paper} \citep{meng2017training} melakukan optimisasi dengan ide yang kurang lebih sama. Pada penelitian tersebut, optimisasi diimplementasi dengan menambahkan \en{op} baru ke TensorFlow, yaitu \code{swap\_out} dan \code{swap\_in} yang berfungsi memindahkan \en{tensor} dari \en{device} ke \en{host}, dan sebaliknya. Kedua \en{op} tersebut kemudian disisipkan ke graf TensorFlow, seperti ditunjukkan oleh Gambar~\ref{fig:SwapOutIn}. Sayangnya, \en{paper} tersebut kurang menjelaskan bagaimana kedua \en{op} diimplementasikan dan bagaimana disisipkan ke graf TensorFlow.

Karena studi kasus yang digunakan adalah arsitektur RNN, maka solusi terhadap permasalahan berfokus pada kelas-kelas RNN yang disediakan oleh TensorFlow, seperti \code{tf.nn.nn\_cell.RNNCell}. Akan dianalisis \en{source code} kelas-kelas RNN tersebut, terutama bagaimana \en{StackPushOp} pada Kode~\ref{lst:SwapMemory} dipanggil, kemudian memodifikasinya mengikuti ide yang diajukan pada analisis permasalahan dan pada \en{paper} \citep{meng2017training}.

Kemudian akan dilakukan pengujian program TensorFlow asli dengan program TensorFlow hasil modifikasi tersebut. Pengujian tersebut dilakukan pada sebuah model RNN, dengan variabel-variabel berupa jumlah \en{layer} dan neuron, ukuran \en{batch}, dan ukuran data. Dari pengujian tersebut, ingin diketahui perbandingan penggunaan memori dan durasi \en{training} antara program TensorFlow asli dan hasil modifikasi.
