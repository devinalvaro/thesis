\chapter{Analisis Permasalahan dan Rancangan Solusi}

\section{Analisis Permasalahan}

\en{Deep learning} merupakan metode pembelajaran mesin menggunakan \en{neural network} dengan \en{hidden layer} berjumlah banyak. Di luar kinerja model, tantangan yang terdapat pada pelatihan model \en{deep learning} meliputi panjangnya durasi pelatihan dan besarnya sumber daya komputasi (seperti kapasitas CPU dan GPU) yang dibutuhkan. Salah satunya penyebabnya adalah banyaknya jumlah \en{hidden layer} yang mengakibatkan banyaknya pula parameter-parameter yang harus di-\en{maintain}, sehingga meningkatkan durasi dan komputasi yang dilakukan saat pelatihan model. Selain itu, pelatihan model \en{deep learning} seringkali menggunakan data berukuran besar, menambah durasi dan sumber daya yang dibutuhkan.

Pada tugas akhir ini, fokus penelitian terletak pada penggunaan memori untuk pelatihan model \en{deep learning}. Pada pelatihan model \en{deep learning}, memori terutama digunakan untuk me-\en{maintain} nilai parameter-parameter model seperti \en{weight}, \en{bias}, hasil fungsi aktivasi, serta data yang sedang diproses. Kelebihan penggunaan memori dari kapasitas yang ada dapat mengakibatkan gagalnya pelatihan model karena masalah \en{out of memory}.

Saat ini, terdapat berbagai penanganan terhadap masalah tersebut. Salah satunya adalah mengurangi jumlah \en{hidden layer} serta neuron-neuronnya yang secara langsung mengurangi jumlah parameter yang harus di-\en{maintain} di memori. Akan tetapi, metode ini berpotensi mengurangi kinerja, apabila topologi model menjadi kurang dari optimal.

Metode penanganan lain disebut \en{stochastic gradient descent} (SGD), di mana data pelatihan dibagi menjadi beberapa \en{mini-batch}. Seperti yang telah disebutkan sebelumnya, data pelatihan \en{deep learning} seringkali berukuran besar, sehingga mahal secara waktu dan komputasi. Tujuan SGD adalah mengurangi ukuran data yang diproses pada suatu waktu, sehingga mengurangi komputasi (termasuk penggunaan memori) dan durasi pelatihan. Namun, SGD memiliki \en{trade-off} tersendiri, yaitu karena data yang diproses pada suatu waktu hanya sebagian, maka data yang diproses kurang representatif dari data sebenarnya. Seperti sebelumnya, hal ini berpotensi mengurangi keoptimalan kinerja model.

Dari penjabaran tersebut ditunjukkan bahwa optimisasi penggunaan memori GPU pada pelatihan model \en{deep learning} merupakan permasalahan yang layak dipecahkan, karena dengan sumber daya yang sama dapat dilatih model dengan jumlah \en{layer} dan parameter yang lebih banyak serta meningkatkan ukuran data yang dapat diproses pada suatu waktu (ukuran \en{batch}).

Satu lagi permasalahan memori pada pelatihan model \en{deep learning} adalah kapasitas GPU. Karena sifat paralelismenya yang tinggi, pelatihan menggunakan GPU lebih cepat dari CPU. Akan tetapi, pada umumnya memori GPU berukuran 8-12 kali lebih kecil dari memori CPU. Terutama pada pelatihan model dengan satu GPU, masalah ini menjadi signifikan karena \en{memory load} tidak dapat didistribusi ke banyak GPU sekaligus.

Salah satu penanganan permasalahan tersebut pada saat ini adalah \en{memory swapping}. \en{Memory swapping} adalah metode pengurangan beban memori dengan menyimpan data secara sementara di \en{memory pool} yang lebih besar (dan biasanya lebih lambat). Pada kasus \en{deep learning}, sebagian memori di GPU dapat disimpan di memori CPU secara sementara.

\en{Memory swapping} telah diterapkan di beberapa \en{framework} pembelajaran mesin seperti TensorFlow. Akan tetapi, pada TensorFlow pengambilan keputusan mengenai data apa yang di-\en{swap} dan kapan melakukan \en{swapping} hanya menggunakan heuristik sederhana. Heuristik tersebut akan dijelaskan secara lebih detail di bawah.

Melihat celah optimisasi tersebut, demikian fokus tugas akhir ini adalah mengoptimisasi penggunaan memori pada pelatihan model \en{deep learning} dengan satu GPU di TensorFlow.

\subsection{Penggunaan Memori pada Pelatihan Model}

TBD. Penulis akan melakukan eksperimen untuk menunjukkan penggunaan memori pada pelatihan model RNN/LSTM terhadap beberapa variabel (jumlah \en{hidden layer} dan neuronnya, ukuran data dan \en{batch}).

\subsection{Heuristik \en{Memory Swapping} di TensorFlow}

Seperti disebutkan sebelumnya, pada TensorFlow dapat digunakan \en{memory swapping} untuk mengurangi penggunaan memori. Dari sisi \en{high-level}, TensorFlow menyediakan kelas-kelas seperti \code{tf.nn.dynamic\_rnn} (model RNN dinamis) yang menerima parameter \code{swap\_memory} untuk memberi pilihan kepada pengguna apakah akan digunakan metode \en{memory swapping} saat pelatihan model.

Selanjutnya akan dijelaskan mekanisme \en{memory swapping} di Tensorflow dari sisi yang lebih \en{low-level}. Seperti telah dijelaskan pada Bab Studi Literatur, program TensorFlow dapat dilihat sebagai \en{dataflow graph}, di mana \en{tensor} merupakan sisi graf dan simpul graf adalah operasi-operasi (\en{op}) terhadap \en{tensor}. Pada setiap \en{op}, TensorFlow meletakkan \en{input tensor} ke atas \en{memory stack} agar kemudian dapat diproses.

Berikut adalah cuplikan kode dari \en{codebase} TensorFlow mengenai \code{StackPushOp}, kelas yang bertanggung jawab dalam peletakkan tensor ke \en{memory stack} tersebut. Kode telah dimodifikasi agar hanya menampilkan bagian-bagian yang menjadi perhatian pembahasan.

\begin{lstlisting}[language=C++, caption=\code{tensorflow/core/kernels/stack\_ops.cc}, label={lst:swapmemory}]
template <typename Device>
class StackPushOp : public AsyncOpKernel {

  void ComputeAsync(OpKernelContext* ctx, DoneCallback done) override {
    static constexpr int kCopyThreshold = 2048;
    static constexpr double kOccupancy = 0.7;
    if (swap_memory_ && tensor.TotalBytes() > kCopyThreshold) {
      // ... retrieve device statistics
      if (stats.bytes_in_use > (stats.bytes_limit * kOccupancy)) {
        // ... copy the tensor from GPU to CPU memory (async)

        // push to stack the pointer to CPU memory
        // ...
          ctx->SetStatus(stack->Push({*cpu_tensor, alloc_attrs, true}));
        // ...
      }
      return
    }

    // if no memory swapping, just push tensor value to stack
    OP_REQUIRES_OK_ASYNC(ctx, stack->Push({tensor, alloc_attrs, false}), done);
    // ...
  }
}
\end{lstlisting}

Seperti terlihat pada Kode \ref{lst:swapmemory}, kelas \code{StackPushOp} memiliki \en{method} \code{ComputeAsync} yang berfungsi meletakkan \en{tensor} ke \en{memory stack}. \code{ComputeAsync} memiliki cara kerja sebagai berikut. Apabila \code{swap\_memory\_} bernilai \code{true} dan kondisi heuristiknya terpenuhi, maka dimulai pemindahan tensor dari GPU ke memori CPU. Setelahnya, yang diletakkan ke atas \en{memory stack} hanyalah \en{pointer} ke memori CPU tempat \en{tensor} tadi dipindahkan. Untuk memperjelas konteks, \en{pointer} ke memori CPU ini nantinya digunakan untuk menyalin kembali \en{tensor} dari CPU ke GPU ketika dibutuhkan.

Sebaliknya, bila \en{memory swapping} tidak dilakukan baik karena \code{swap\_memory\_} bernilai \code{false} atau heuristik tidak terpenuhi, \code{ComputeAsync} \en{by value} meletakkan \en{tensor} ke \en{memory stack}.

Kembali menelaah Kode \code{ComputeAsync}, heuristik yang digunakan untuk menentukan apakah \en{tensor} layak di-\en{swap} ke CPU cukup sederhana. Pertama, \code{ComputeAsync} memeriksa apakah ukuran \en{tensor} melebihi \code{kCopyThreshold} yang bernilai cukup \en{arbitrary}, yaitu 2048 \en{bytes}. Selanjutnya, \code{ComputeAsync} mengevaluasi apakah persentase penggunaan memori GPU saat ini melebihi \code{kOccupancy}, yang ditetapkan bernilai 0.7 (70\%).

\subsection{Potensi Optimisasi \en{Memory Swapping}}

Pada umumnya, pelatihan model \en{deep learning} dilakukan dengan teknik \en{backpropagation}. Secara singkat, satu iterasi pelatihan terdiri dari 2 tahap, \en{forward propagation} dan \en{backward propagation} (atau \en{backpropagation}). Pada \en{forward propagation}, \en{input layer} menerima input kemudian mempropagasinya \en{layer} demi \en{layer} hingga didapatkan \en{output} di \en{output layer}. Setelahnya, \en{output} dibandingkan dengan label, kemudian dari selisihnya (\en{loss}) tersebut disesuaikan parameter-parameter model dari \en{output layer} sampai \en{input layer} sedemikian rupa sehingga secara keseluruhan, \en{loss} bernilai minimum.

Melihat proses pelatihan dengan \en{backpropagation} tersebut, terdapat jeda antara digunakannya suatu \en{layer} saat \en{forward propagation} sampai digunakannya kembali saat \en{backpropagation}. Dilihat dari sisi penggunaan memori, pada jeda ini \en{layer} tetap mengonsumsi memori walaupun tidak sedang digunakan. Konsumsi memori yang tidak perlu ini tentunya lebih baik bila dapat digunakan untuk keperluan lain.

Dari penjelasan tersebut, terlihat potensi optimisasi penggunaan memori pada pelatihan model \en{deep learning}. Memori untuk \en{layer}-\en{layer} yang sedang tidak digunakan tersebut lebih baik bila didelegasi ke \en{memory pool} yang lebih besar. Pada kasus pelatihan model dengan satu GPU, lebih baik bila memori yang digunakan \en{layer} di-\en{swap} dari GPU ke CPU apabila tidak sedang digunakan.

Bila melihat heuristik \en{memory swapping} di TensorFlow yang dijelaskan pada bagian sebelumnya, terlihat bahwa yang dilakukan TensorFlow justru kebalikannya. Karena \en{swapping} dilakukan bila memori melebihi \code{kOccupancy}, berarti \en{tensor} yang di-\en{swap} adalah \en{tensor} pada \en{layer-layer} akhir. Di sinilah terletak potensi optimisasi \en{memory swapping} di TensorFlow tersebut.

Selain itu, perlu diingat bahwa \en{memory swapping} di TensorFlow hingga saat ini (v1.12) masih terbatas pada satu model saja, yaitu kelas \code{tf.nn.dynamic\_rnn}, yaitu model RNN topologinya dinamis sesuai sekuens yang diberikan. Padahal, metode \en{memory swapping} seharusnya adalah teknik yang umum (tidak terikat arsitektur tertentu).

\section{Rancangan Solusi}

TBD. Penulis akan menjabarkan rancangan solusi terutama dengan merujuk pada \en{paper} Alibaba.
