\chapter{Analisis Permasalahan dan Rancangan Solusi}

\section{Analisis Permasalahan} \label{ProblemAnalysis}

Di luar kinerja model, tantangan dalam melatih model \en{deep learning} meliputi lamanya durasi pelatihan dan besarnya sumber daya komputasi yang dibutuhkan, seperti kapasitas memori. Salah satu penyebabnya adalah besarnya jumlah dan ukuran \en{hidden layer} yang meningkatkan jumlah nilai-nilai yang harus dikomputasi. Selain itu, pelatihan model \en{deep learning} seringkali menggunakan \en{dataset} berukuran besar, menambah durasi pelatihan dan sumber daya yang dibutuhkan.

Fokus penelitian di tugas akhir ini terletak pada optimisasi penggunaan memori untuk pelatihan model, khususnya di bagian \en{memory swapping} yang akan dijelaskan kemudian. Seperti yang dijelaskan di Bagian~\ref{MemoryUsage}, memori digunakan untuk me-\en{maintain} nilai-nilai seperti bobot, bias, hasil fungsi aktivasi, serta data yang sedang diproses. Kelebihan penggunaan memori dari kapasitas yang ada dapat mengakibatkan gagalnya pelatihan model karena masalah \en{out of memory} (OOM).

Satu hal yang  memperparah masalah ini adalah kapasitas memori GPU. Saat ini, GPU banyak digunakan untuk melatih model \en{deep learning} karena sifat paralelismenya yang tinggi, sehingga dapat melatih model dengan lebih cepat dari CPU. Namun, ukuran memori GPU cenderung lebih kecil dari ukuran memori CPU. Sebagai gambaran, saat ini memori GPU umumnya berukuran sekitar 2-16 GB sedangkan memori CPU dapat berukuran hingga 1 TB.

Salah satu solusi masalah tersebut adalah \en{memory swapping}, yaitu metode pengurangan beban memori dengan memindahkan isi memori secara sementara ke \en{memory pool} yang lebih besar (dan biasanya memiliki waktu akses lebih lambat). Pada kasus ini, sebagian isi memori GPU dapat dipindahkan ke memori CPU secara sementara. Dalam berbagai kasus, terutama dengan kapasitas memori yang terbatas, \en{memory swapping} merupakan satu-satunya solusi terhadap masalah OOM.

Namun, \en{memory swapping} memiliki \en{trade-off} tersendiri. Seperti pemindahan data apapun, memindahkan data dari GPU ke CPU (dan sebaliknya) juga membutuhkan waktu tersendiri, berbanding lurus dengan jumlah dan ukuran data yang dipindahkan. Akibatnya, \en{memory swapping} menambah durasi pelatihan yang bertambahnya semakin besar seiring  bertambahnya data yang di-\en{swap}. Maka, \en{memory swapping} yang optimal perlu selektif dalam memilih data mana yang di-\en{swap} agar pelatihan model dapat berjalan tanpa OOM, juga tanpa menambah durasi pelatihan secara terlalu besar.

Salah satu \en{framework} pembelajaran mesin, TensorFlow, memiliki fitur \en{memory swapping}. Namun, terdapat potensi bahwa \en{memory swapping} di TensorFlow masih kurang optimal, seperti yang dijelaskan secara lebih detail di Bagian~\ref{OptimizationPotential}.

Melihat potensi optimisasi tersebut, fokus tugas akhir ini adalah meneliti dan mengembangkan \en{memory swapping} di TensorFlow sehingga lebih optimal dari yang sudah ada, agar mengurangi durasi pelatihan dibandingkan dengan \en{memory swapping} bawaan TensorFlow.

\subsection{Mekanisme \en{Memory Swapping} di TensorFlow} \label{SwappingMechanism}

Mengenai \en{memory swapping}, TensorFlow menyediakan kelas \code{tf.nn.dynamic\_rnn} (model RNN dinamis) yang menerima parameter \code{swap\_memory} untuk memberi pilihan kepada pengguna apakah akan digunakan metode \en{memory swapping} saat pelatihan model.

Selanjutnya, akan dijelaskan mekanisme \en{memory swapping} di TensorFlow dari sisi yang lebih \en{low-level}. Untuk itu, akan dibahas mengenai 4 kelas yang menjadi komponen utama \en{memory swapping} di TensorFlow, yaitu \code{Tensor}, \code{Stack}, \code{StackPushOp}, dan \code{StackPopOp}.

\subsubsection{\code{Tensor}} \label{Tensor}

Sedikit mengulang yang telah dijelaskan di Bagian~\ref{Tensorflow}, sebuah program TensorFlow dapat dilihat sebagai sebuah \en{dataflow graph}, di mana \en{tensor} melewati sisi graf dan simpul graf adalah operasi (\en{op}) terhadap \en{tensor}-\en{tensor} yang melewatinya. \en{Tensor} di sini merupakan vektor multi-dimensional yang berisi nilai-nilai yang dikomputasi oleh TensorFlow.

Pada \en{codebase} TensorFlow, terdapat kelas \code{Tensor}. Meskipun kelas ini merepresentasikan \en{tensor}, namun kelas ini hanya memiliki \en{pointer} yang menunjuk ke nilai-nilai \en{tensor} yang sebenarnya di memori, baik memori CPU ataupun GPU. Maka, kelas ini dapat dilihat sebagai \en{handle} terhadap nilai \en{tensor} di memori.

Kegunaan kelas ini sebagai \en{handle}, selain untuk membaca nilai-nilai \en{tensor}-nya, juga digunakan untuk \en{memory swapping}, seperti yang akan ditunjukkan kemudian.

Pada bagian-bagian selanjutnya, penulisan \code{tensor} seperti demikian merujuk pada sebuah \en{instance} kelas \code{Tensor}, sedangkan penulisan \en{tensor} seperti demikian merujuk pada \en{tensor} dalam artian vektor-multidimensional.

\subsubsection{\code{Stack}} \label{Stack}

Saat \en{forward pass} berlangsung, \en{output tensor} dari sebuah \en{op} diletakkan ke sebuah \en{stack} agar ketika \en{backpropagation}, \en{op} bersangkutan dapat mengambil kembali \en{tensor} yang diletakkan ke \en{stack} tadi dengan urutan yang benar \citep{tensorflow2016implementation}.

\en{Stack} yang dimaksud adalah kelas \code{Stack} yang terdapat di \en{codebase} TensorFlow, tepatnya pada \en{file} \code{tensorflow/core/kernels/stack.cc}. Perlu diperhatikan bahwa setiap \en{op} memiliki \code{stack}-nya sendiri.

Dalam konteks \en{backpropagation} biasa, penggunaan \code{stack} ini menimbulkan pertanyaan, karena bukankah dalam \en{backpropagation}, hanya terdapat satu nilai \en{tensor} saja yang perlu diambil untuk setiap \en{op}. Kalau begitu, mengapa TensorFlow harus menggunakan \en{stack} untuk menyimpan \en{tensor}?

Hal ini dikarenakan pada pelatihan model RNN, terdapat faktor \en{timestep} pada \en{backpropagation} (\en{through time}). Karena itu, setiap elemen di \code{stack} merepresentasikan \en{output tensor} dari \en{op} bersangkutan pada setiap \en{timestep}, dengan elemen teratas berasal dari \en{timestep} terakhir.

Selain \code{Stack}, terdapat kelas \code{StackPushOp} dan \code{StackPopOp}, dua \en{op} yang bertanggung jawab meletakkan dan mengambil \en{tensor} seperti yang disebutkan sebelumnya.

\subsubsection{\code{StackPushOp}}

Berikut adalah cuplikan kode \code{StackPushOp} untuk menunjukkan mekanisme \en{memory swapping} di TensorFlow, khususnya pada bagian \en{swapping out}. Kode telah dimodifikasi agar hanya menampilkan bagian-bagian yang menjadi perhatian pembahasan.

\begin{lstlisting}[language=C++, caption=\code{StackPushOp} (\code{tensorflow/core/kernels/stack.cc}), label={lst:SwapOut}]
void StackPushOp::ComputeAsync(OpKernelContext* ctx, DoneCallback done) {
  // ... initializations

  static constexpr int kCopyThreshold = 2048;
  static constexpr double kOccupancy = 0.7;
  if (swap_memory_ && tensor.TotalBytes() > kCopyThreshold) {
    // ... retrieve device statistics
    if (stats.bytes_in_use > (stats.bytes_limit * kOccupancy)) {
      // ... copy the tensor from GPU to CPU memory (async)

      // push to stack the pointer to CPU memory
      // ...
        ctx->SetStatus(stack->Push({*cpu_tensor, alloc_attrs, true}));
      // ...
    }
    return
  }

  // else if no memory swapping, just push tensor to stack
  OP_REQUIRES_OK_ASYNC(ctx, stack->Push({tensor, alloc_attrs, false}), done);
  // ...
}
\end{lstlisting}

Terlihat pada Kode~\ref{lst:SwapOut} bahwa \code{StackPushOp} memiliki \en{method} \code{ComputeAsync} yang melakukan peletakkan \code{tensor} ke \code{stack}. \code{ComputeAsync} memiliki cara kerja sebagai berikut. Apabila \code{swap\_memory\_} bernilai \code{true} dan kondisi heuristiknya terpenuhi, maka dimulai pemindahan nilai \en{current tensor} dari GPU ke memori CPU (\en{swap out}).

Setelah pemindahan selesai, yang diletakkan ke atas \code{stack} adalah \en{instance} \code{tensor} baru (pada Kode~\ref{lst:SwapOut} dinamakan \code{cpu\_tensor}) sebagai \en{handle} terhadap nilai \en{tensor} yang sama, namun sekarang terletak di memori CPU. Untuk memperjelas konteks, \code{tensor} ini nantinya digunakan sebagai \en{handle} dalam pemindahan kembali nilai \en{tensor} dari CPU ke GPU (\en{swap in}) ketika dibutuhkan saat \en{backpropagation}.

Sebaliknya, bila \en{memory swapping} tidak dilakukan baik karena \code{swap\_memory\_} bernilai \code{false} atau heuristik tidak terpenuhi, \code{tensor} langsung diletakkan saja ke atas \code{stack}.

\subsubsection{\code{StackPopOp}}

Setelah membahas \code{StackPushOp}, selanjutnya adalah pembahasan mengenai \code{StackPopOp}, kelas yang bertanggung jawab dalam pengambilan \code{tensor} dari atas \code{stack}. Berikut adalah cuplikan kode \code{StackPopOp} yang telah dimodifikasi sehingga hanya menampilkan bagian yang menjadi perhatian.

\begin{lstlisting}[language=C++, caption=\code{StackPopOp} (\code{tensorflow/core/kernels/stack.cc}), label={lst:SwapIn}]
void StackPopOp::ComputeAsync(OpKernelContext* ctx, DoneCallback done) {
  // ... initializations

  Stack::TensorAndAllocation value;
  OP_REQUIRES_OK_ASYNC(ctx, stack->Pop(&value), done);
  if (value.swapped_to_cpu) {
    // Asynchronously copy the tensor back from CPU to GPU memory.
    // ...
    Tensor* cpu_tensor = &value.tensor;
    Tensor* device_tensor = ...
    device_ctxt->CopyCPUTensorToDevice(
        cpu_tensor, device, device_tensor,
        [device_tensor, ctx, done](const Status& s) {
          // ...
          if (s.ok()) {
            ctx->set_output(0, *device_tensor);
          }
          done();
          // ...
        });
  } else {
    // Execute synchronously if not swapped.
    ctx->set_output(0, value.tensor);
    done();
  }
}
\end{lstlisting}

Dari Kode~\ref{lst:SwapIn} di atas, terlihat bahwa pada \code{StackPopOp}, TensorFlow memeriksa terlebih dahulu apakah sebelumnya \code{tensor} telah dipindahkan ke CPU. Jika iya, maka nilai \en{tensor} dipindahkan terlebih dahulu dari CPU ke GPU secara \en{asynchronous}. Jika tidak, maka nilai \en{tensor} dapat langsung dipakai karena sudah ada di GPU.

Meskipun pemindahan dilakukan secara \en{asynchronous} (tidak mem-\en{block} CPU), namun penggunaan \code{stack} tetap \en{blocked}, karena menunggu pemanggilan \code{done()} dilakukan. Dapat diperhatikan bahwa pemanggilan \code{done()} pada kasus \en{swapping in} (baris 19 pada Kode~\ref{lst:SwapIn}) dilakukan pada \en{callback} fungsi \code{CopyCPUTensorToDevice} (berarti setelah \en{swapping in} selesai). Seperti disebutkan di Bagian~\ref{stack}, karena masing-masing \en{op} memiliki \en{stack}-nya sendiri, dan setiap elemen di \en{stack} merepresentasikan setiap \en{timestep}, maka implikasi dari \en{blocking} ini adalah \en{op} tidak dapat digunakan untuk \en{timestep} berikutnya hingga \en{swapping in} untuk \en{timestep} sekarang telah selesai (memanggil \code{done()}).

Kembali menelaah kedua kode tersebut, dapat diambil beberapa kesimpulan. Terdapat heuristik yang harus dipenuhi untuk menentukan apakah sebuah \en{tensor} layak di-\en{swap out} ke CPU, yaitu ketika ukuran \en{tensor} melebihi \code{kCopyThreshold} (2048 \en{bytes}) dan persentase penggunaan memori GPU melebihi \code{kOccupancy} (70\%). Hal yang perlu diperhatikan juga adalah ketika heuristik terpenuhi, \en{tensor} yang di-\en{swap out} adalah \en{tensor} yang baru akan diletakkan ke atas \code{stack} (\en{tensor} terbaru). Selain itu, dari Kode~\ref{lst:SwapIn} terlihat bahwa \en{swap in} sebuah \en{tensor} baru dilakukan saat \en{tensor} tersebut akan dipakai.

\subsection{Potensi Optimisasi dengan \en{Memory Swapping}} \label{OptimizationPotential}

Pada umumnya, pelatihan model \en{deep learning} dilakukan dengan teknik \en{backpropagation}. Secara singkat, satu iterasi pelatihan terdiri dari 2 tahap, \en{forward pass} dan \en{backpropagation}. Saat \en{forward pass}, \en{input layer} menerima input kemudian mempropagasinya \en{layer} demi \en{layer} hingga didapatkan \en{output} di \en{output layer}. Setelahnya, \en{output} dibandingkan dengan label, kemudian dari selisihnya (\en{loss}) tersebut disesuaikanlah parameter-parameter model (bobot dan bias) dari \en{output layer} hingga \en{input layer} sedemikian rupa sehingga secara keseluruhan, \en{loss} bernilai minimum. Mengenai \en{backpropagation} dijelaskan secara lebih mendalam pada Bagian~\ref{Backpropagation} dan \ref{BPTT}.

Melihat proses pelatihan dengan \en{backpropagation} tersebut, terdapat interval antara digunakannya suatu \en{layer} saat \en{forward pass} hingga digunakannya kembali saat \en{backpropagation}. Dilihat dari sisi penggunaan memori, pada interval ini \en{layer} tetap mengonsumsi memori walaupun tidak sedang digunakan. Konsumsi memori yang tidak perlu ini tentunya lebih baik bila dapat digunakan untuk keperluan lain.

Dari penjelasan tersebut, terlihat potensi optimisasi penggunaan memori pada pelatihan model \en{deep learning}. Memori untuk \en{tensor}-\en{tensor} yang sedang tidak digunakan tersebut lebih baik bila didelegasikan ke \en{memory pool} yang lebih besar.

Maka dari itu, seperti yang disebutkan oleh \citep{meng2017training}, \en{tensor} yang paling baik untuk di-\en{swap} adalah \en{tensor} yang interval dari \en{forward pass} hingga digunakannya kembali saat \en{backpropagation} yang terlama. Dengan begitu, diharapkan bahwa \en{overlapping} antara komputasi dengan \en{memory swapping} meningkat, sehingga mengurangi waktu yang dibutuhkan (karena \en{memory copy} antara GPU dan CPU berjalan secara \en{asynchronous}).

Bila melihat heuristik \en{memory swapping} di TensorFlow yang dijelaskan di bagian sebelumnya, terlihat bahwa yang dilakukan TensorFlow justru kebalikannya. Karena ketika heuristik terpenuhi, \en{swapping} dilakukan terhadap \en{current tensor} (\en{tensor} saat ini), yang berarti interval hingga digunakannya kembali tidak terlalu lama. Terlebih lagi, \en{tensor} baru di-\en{swap in} saat akan digunakan kembali di \en{backpropagation}, mengurangi \en{asynchronicity} dari eksekusi program. Di sinilah terletak potensi optimisasi \en{memory swapping} di TensorFlow.

\subsection{\en{Recurrent Neural Network} sebagai Model Uji} \label{WhyRNN}

Arsitektur \en{deep neural network} yang dipilih untuk menjadi fokus optimisasi ini adalah \en{recurrent neural network} (RNN). Secara lebih spesifik, model berbasis RNN yang antara \en{input layer} atau \en{output layer}-nya (atau keduanya) berjenis \en{sequence}.

Alasan utama pemilihan arsitektur RNN adalah karena fitur \en{memory swapping} di TensorFlow hanya terdapat pada \code{tf.nn.dynamic\_rnn} (kelas RNN dinamis), sehingga dengan memilih model berarsitektur RNN, \en{memory swapping} hasil optimisasi dapat dibandingkan dengan \en{memory swapping} bawaan dari TensorFlow.

Alasan lainnya adalah karena topologi RNN yang dinamis menurut panjang \en{input} atau \en{output}, penggunaan memori tidak dapat diprediksi sebelum eksekusi, sehingga optimisasi memori pun menjadi lebih krusial.

Selain itu, seperti yang dijelaskan pada Bagian~\ref{BPTT}, pelatihan RNN dengan \en{backpropagation through time} lebih kompleks dan sulit dari \en{backpropagation} biasa, karena perhitungan harus ikut memperhitungkan \en{timestep} yang ada. Faktor ini membuat pelatihan RNN cenderung membutuhkan lebih banyak waktu dan memori dari arsitektur lainnya.

\section{Rancangan Solusi} \label{SolutionDesign}

Setelah menganalisis permasalahan, didapatkan bahwa terdapat potensi optimisasi \en{memory swapping} di TensorFlow, dengan memprioritaskan \en{tensor} dengan interval waktu yang terlama hingga digunakannya kembali saat \en{backpropagation}. Selanjutnya, ide optimisasi tersebut perlu diimplementasikan ke TensorFlow.

Seperti yang disebutkan pada Bagian~\ref{OptimizationPotential}, ide optimisasi tersebut terinspirasi dari \citep{meng2017training}. Namun, sebenarnya \en{paper} tersebut menceritakan mekanisme dari \en{memory swapping} yang diimplementasikan sendiri oleh para penulis (bukan bawaan TensorFlow) beserta optimisasinya (dijelaskan secara lebih detail di Bagian~\ref{RelatedWorks}). Sedangkan fokus tugas akhir ini adalah mengoptimisasi \en{memory swapping} bawaan TensorFlow, sehingga pendekatan dan detail optimisasi yang dilakukan di tugas akhir ini berbeda dari \en{paper} tersebut, karena memang implementasi \en{memory swapping} bawaan TensorFlow berbeda jauh dari implementasi \en{memory swapping} di \en{paper} tersebut.

Dengan demikian, diperlukan implementasi yang dapat mengoptimisasi sesuai ide tersebut, namun terhadap mekanisme \en{memory swapping} bawaan TensorFlow yang dijelaskan di Bagian~\ref{SwappingMechanism}. Berikut akan dijelaskan rancangan implementasi yang dibuat oleh penulis.

Pada Bagian~\ref{SwappingMechanism} dijelaskan bahwa setelah melewati heuristik yang ada (apakah memori GPU hampir penuh), dilakukan \en{swapping out} terhadap \en{current tensor}. Karena heuristik cenderung baru terpenuhi saat memori hampir penuh, maka \en{current tensor} adalah \en{tensor} yang dalam waktu lebih dekat akan digunakan kembali ketika \en{backpropagation}, ketimbang \en{tensor}-\en{tensor} yang sebelumnya. Ide penulis adalah ketika heuristik terpenuhi, dilakukan \en{swapping out} terhadap \en{oldest tensor} yang belum di-\en{swap}. \en{Oldest} di sini berarti \en{least recent} atau yang paling awal waktu digunakannya pada \en{forward pass}, sehingga paling lama juga waktu hingga akan digunakannya kembali pada \en{backpropagation}.

Kode~\ref{lst:SwapOutPseudocodeOptimized} berikut menggambarkan alur \en{swapping out} menurut implementasi tersebut, beserta Kode~\ref{lst:SwapOutPseudocode} yang menggambarkan alur \en{swapping out} bawaan TensorFlow untuk perbandingan.

Perlu diperhatikan bahwa \code{swap\_out} (dan \code{swap\_in}) adalah \en{asynchronous function} yang argumen ke-2-nya adalah sebuah \en{callback function}. \en{Callback function} di kedua fungsi tersebut dinotasikan seperti berikut: \code{(args) => \{ ... \}}.

\begin{lstlisting}[language=C++, caption=Pseudocode \en{Swapping Out} Bawaan, label={lst:SwapOutPseudocode}]
if (gpu_memory_almost_full) {
    swap_out(current_tensor, (cpu_tensor) => {
        stack.push(cpu_tensor)
    })
} else {
    stack.push(current_tensor)
}
\end{lstlisting}

\begin{lstlisting}[language=C++, caption=Pseudocode \en{Swapping Out} Teroptimisasi, label={lst:SwapOutPseudocodeOptimized}]
stack.push(current_tensor)

if (gpu_memory_almost_full) {
    id = stack.get_oldest_unswapped_tensor_id()
    swap_out(stack[id].tensor, (cpu_tensor) => {
        // set respective stack element as swapped
        stack[id].tensor = cpu_tensor
        stack[id].swapped = true
    })
}
\end{lstlisting}

Terlihat pada Kode~\ref{lst:SwapOutPseudocodeOptimized} bahwa \code{current\_tensor} (\en{tensor} saat ini) langsung saja di-\en{push} ke \code{stack}, karena \code{current\_tensor} tidak akan menjadi subjek \en{swapping}, berbeda dengan alur program bawaan di Kode~\ref{lst:SwapOutPseudocode}.

Selanjutnya, program teroptimisasi memeriksa apakah memori GPU hampir penuh (heuristik terpenuhi). Bila iya, program mengambil \code{id} dari \en{oldest tensor} yang belum di-\en{swap}. Kemudian \code{stack[id].tensor} di-\en{swap out}, lalu setelah selesai \code{stack[id].tensor} di-\en{assign} dengan \code{cpu\_tensor}, yaitu \en{handle} dari \en{tensor} yang sama namun sekarang telah dipindah ke CPU. Selain itu, \code{stack[id].swapped} juga ditandai sebagai \code{true}, agar dapat diproses dengan benar oleh \code{StackPopOp} saat \en{backpropagation}.

Pengaksesan \code{stack} dengan indeks di sini memang melanggar hakikat \en{stack} secara konsep (di mana hanya elemen teratas \en{stack} yang dapat dimodifikasi), namun dimungkinkan karena \code{Stack} di TensorFlow diimplementasi dengan \en{array} (\code{vector}).

Demikian \en{pseudocode} program teroptimisasi dalam melakukan \en{swapping out} terhadap \en{oldest tensor} ketimbang \en{tensor} saat ini. Selanjutnya adalah \en{pseudocode} optimisasi terhadap mekanisme \en{swapping in}.

\begin{lstlisting}[language=C++, caption=Pseudocode \en{Swapping In} Bawaan, label={lst:SwapInPseudocode}]
tensor = stack.pop()
if (tensor.swapped) {
    swap_in(tensor, (gpu_tensor) => {
        current_tensor = gpu_tensor
    })
} else {
    current_tensor = tensor
}
\end{lstlisting}

\begin{lstlisting}[language=C++, caption=Pseudocode \en{Swapping In} Teroptimisasi, label={lst:SwapInPseudocodeOptimized}]
tensor = stack.pop()
if (current_tensor.swapped) {
    swap_in(tensor, (gpu_tensor) => {
        current_tensor = gpu_tensor
    })
} else {
    current_tensor = tensor
}

if (not gpu_memory_almost_full) {
    id = stack.get_newest_swapped_tensor_id()
    swap_in(stack[id].tensor, (gpu_tensor) => {
        // set respective stack element as unswapped
        stack[id].tensor = gpu_tensor
        stack[id].swapped = false
    })
}
\end{lstlisting}

Pada Kode~\ref{lst:SwapInPseudocodeOptimized} terlihat bahwa hingga baris ke-9, alur program sama dengan alur program bawaan TensorFlow pada Kode~\ref{lst:SwapInPseudocode}. Namun, pada kode teroptimisasi, setelahnya diperiksa apakah heuristik terpenuhi (apakah memori GPU tidak penuh, kebalikan dari heuristik \en{swapping out}). Jika iya, program mulai melakukan \en{swapping in} terhadap \en{tensor} terbaru yang belum di-\en{swap in}. Seperti disebutkan pada Bagian~\ref{OptimizationPotential}, tujuan bagian program ini adalah untuk meningkatkan \en{asynchronicity} dari program, yaitu meningkatkan \en{overlapping} antara komputasi biasa dengan \en{memory transfer} pada \en{memory swapping}.

Demikianlah rancangan implementasi oleh penulis untuk mengoptimisasi \en{memory swapping} di TensorFlow. Pada \nameref{ImplementationEvaluation} dibahas mengenai implementasi dan hasil pengujian optimisasi tersebut.
