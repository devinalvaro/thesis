\chapter{Analisis Permasalahan dan Rancangan Solusi}

\section{Analisis Permasalahan} \label{problemanalysis}

Di luar kinerja model, tantangan dalam melatih model \en{deep learning} meliputi lamanya durasi pelatihan dan besarnya sumber daya komputasi yang dibutuhkan, seperti kapasitas memori. Salah satunya penyebabnya adalah besarnya jumlah dan ukuran \en{hidden layer} yang meningkatkan jumlah nilai-nilai yang harus dikomputasi. Selain itu, pelatihan model \en{deep learning} seringkali menggunakan \en{dataset} berukuran besar, menambah durasi pelatihan dan sumber daya yang dibutuhkan.

Fokus penelitian di tugas akhir ini terletak pada optimisasi penggunaan memori untuk pelatihan model, khususnya di bagian \en{memory swapping} yang akan dijelaskan kemudian. Seperti dijelaskan di Bagian~\ref{memoryusage}, mayoritas memori digunakan untuk me-\en{maintain} nilai-nilai seperti bobot, bias, hasil fungsi aktivasi, serta data yang sedang diproses. Kelebihan penggunaan memori dari kapasitas yang ada dapat mengakibatkan gagalnya pelatihan model karena masalah \en{out of memory} (OOM).

Satu hal yang  memperparah masalah ini adalah kapasitas memori GPU. Saat ini, GPU banyak digunakan untuk melatih model \en{deep learning} karena sifat paralelismenya yang tinggi, sehingga dapat melatih model dengan lebih cepat dari CPU. Namun, ukuran memori GPU cenderung lebih kecil dari ukuran memori CPU. Sebagai gambaran, saat ini memori GPU umumnya berukuran sekitar 2-16 GB sedangkan memori CPU dapat berukuran hingga 1 TB.

Salah satu penanganan masalah tersebut adalah \en{memory swapping}, yaitu metode pengurangan beban memori dengan memindahkan isi memori secara temporer ke \en{memory pool} yang lebih besar (dan biasanya memiliki waktu akses lebih lambat). Pada kasus ini, sebagian isi memori GPU dapat dipindahkan ke memori CPU secara sementara. Dalam berbagai kasus, terutama dengan kapasitas memori yang terbatas, \en{memory swapping} merupakan satu-satunya solusi terhadap masalah OOM.

Namun, \en{memory swapping} memiliki \en{trade-off} tersendiri. Seperti pemindahan data apapun, memindahkan data dari GPU ke CPU (dan sebaliknya) juga membutuhkan waktu tersendiri, berbanding lurus dengan jumlah data yang dipindahkan. Akibatnya, \en{memory swapping} menambah durasi pelatihan yang bertambahnya semakin besar seiring  akumulasi data yang di-\en{swap}. Maka, \en{memory swapping} yang optimal perlu selektif memilih isi memori mana yang di-\en{swap} agar pelatihan model dapat berjalan tanpa OOM, juga tanpa menambah durasi pelatihan secara terlalu besar.

Salah satu \en{framework} pembelajaran mesin, TensorFlow, memiliki fitur \en{memory swapping}. Namun, terdapat potensi bahwa \en{memory swapping} di TensorFlow masih kurang optimal, seperti yang dijelaskan secara lebih detail di Bagian~\ref{optimization}

Melihat potensi optimisasi tersebut, fokus tugas akhir ini adalah meneliti dan mengembangkan \en{memory swapping} di TensorFlow sehingga lebih optimal dari yang sudah ada, dilihat dari segi durasi pelatihan.

\subsection{Mekanisme \en{Memory Swapping} di TensorFlow} \label{mechanism}

Mengenai \en{memory swapping}, TensorFlow menyediakan kelas \code{tf.nn.dynamic\_rnn} (model RNN dinamis) yang menerima parameter \code{swap\_memory} untuk memberi pilihan kepada pengguna apakah akan digunakan metode \en{memory swapping} saat pelatihan model.

Selanjutnya, akan dijelaskan mekanisme \en{memory swapping} di TensorFlow dari sisi yang lebih \en{low-level}. Untuk itu, akan dibahas mengenai 4 kelas yang menjadi komponen utama \en{memory swapping} di TensorFlow, yaitu \code{Tensor}, \code{Stack}, \code{StackPushOp}, dan \code{StackPopOp}.

\subsubsection{\code{Tensor}}

Sedikit mengulang yang telah dijelaskan di Bagian~\ref{tensorflow}, sebuah program TensorFlow dapat dilihat sebagai sebuah \en{dataflow graph}, di mana \en{tensor} melewati sisi graf dan simpul graf adalah operasi (\en{op}) terhadap \en{tensor}-\en{tensor} yang melewatinya. \en{Tensor} di sini merupakan vektor multi-dimensional yang berisi nilai-nilai yang dikomputasi oleh TensorFlow.

Pada \en{codebase} TensorFlow, terdapat kelas \code{Tensor}. Meskipun kelas ini merepresentasikan \en{tensor}, namun kelas ini hanya memiliki \en{pointer} yang menunjuk ke nilai-nilai \en{tensor} yang sebenarnya di memori, baik memori CPU maupun GPU. Maka, kelas ini dapat dilihat sebagai \en{handle} terhadap nilai \en{tensor} di memori.

Kegunaan kelas ini sebagai \en{handle}, selain untuk membaca nilai-nilai \en{tensor}-nya, juga digunakan dalam \en{memory swapping}, seperti yang akan ditunjukkan kemudian.

Pada bagian-bagian selanjutnya, penulisan \code{tensor} seperti demikian merujuk pada sebuah \en{instance} kelas \code{Tensor}, sedangkan penulisan \en{tensor} seperti demikian merujuk pada \en{tensor} dalam artian vektor-multidimensional.

\subsubsection{\code{Stack}} \label{stack}

Saat \en{forward pass} berlangsung, \en{output tensor} dari \en{op} tertentu diletakkan ke sebuah \en{stack} agar ketika \en{backpropagation}, \en{op} bersangkutan dapat mengambil kembali \en{tensor} yang diletakkan ke \en{stack} tadi dengan urutan yang benar \citep{tensorflow2016implementation}.

\en{Stack} yang dimaksud adalah kelas \code{Stack} yang terdapat di \en{codebase} TensorFlow, tepatnya pada \en{file} \code{tensorflow/core/kernels/stack.cc}. Perlu diperhatikan bahwa setiap \en{op} memiliki \code{stack}-nya sendiri.

Dalam konteks \en{backpropagation} biasa, penggunaan \code{stack} ini menimbulkan pertanyaan, karena bukankah dalam \en{backpropagation}, hanya terdapat satu nilai \en{tensor} saja yang perlu diambil untuk setiap \en{op}. Kalau begitu, mengapa TensorFlow harus menggunakan \en{stack} untuk menyimpan \en{tensor}?

Hal ini dikarenakan pada pelatihan model RNN, terdapat faktor \en{timestep} pada \en{backpropagation} (\en{through time}). Karena itu, setiap elemen di \code{stack} merepresentasikan \en{output tensor} dari \en{op} bersangkutan pada setiap \en{timestep}, dengan elemen teratas berasal dari \en{timestep} terakhir.

Selain \code{Stack}, terdapat kelas \code{StackPushOp} dan \code{StackPopOp}, dua \en{op} yang bertanggung jawab meletakkan dan mengambil \en{tensor} seperti yang disebutkan sebelumnya.

\subsubsection{\code{StackPushOp}}

Berikut adalah cuplikan kode \code{StackPushOp} untuk menunjukkan mekanisme \en{memory swapping} di TensorFlow, khususnya pada bagian \en{swapping out}. Kode telah dimodifikasi agar hanya menampilkan bagian-bagian yang menjadi perhatian pembahasan.

\begin{lstlisting}[language=C++, caption=\code{StackPushOp} (\code{tensorflow/core/kernels/stack.cc}), label={lst:SwapOut}]
void StackPushOp::ComputeAsync(OpKernelContext* ctx, DoneCallback done) {
  // ... initializations

  static constexpr int kCopyThreshold = 2048;
  static constexpr double kOccupancy = 0.7;
  if (swap_memory_ && tensor.TotalBytes() > kCopyThreshold) {
    // ... retrieve device statistics
    if (stats.bytes_in_use > (stats.bytes_limit * kOccupancy)) {
      // ... copy the tensor from GPU to CPU memory (async)

      // push to stack the pointer to CPU memory
      // ...
        ctx->SetStatus(stack->Push({*cpu_tensor, alloc_attrs, true}));
      // ...
    }
    return
  }

  // else if no memory swapping, just push tensor to stack
  OP_REQUIRES_OK_ASYNC(ctx, stack->Push({tensor, alloc_attrs, false}), done);
  // ...
}
\end{lstlisting}

Terlihat pada Kode~\ref{lst:SwapOut} bahwa \code{StackPushOp} memiliki \en{method} \code{ComputeAsync} yang melakukan peletakkan \code{tensor} ke \code{stack}. \code{ComputeAsync} memiliki cara kerja sebagai berikut. Apabila \code{swap\_memory\_} bernilai \code{true} dan kondisi heuristiknya terpenuhi, maka dimulai pemindahan nilai \en{tensor} dari GPU ke memori CPU (\en{swap out}).

Setelah pemindahan selesai, yang diletakkan ke atas \code{stack} adalah \en{instance} \code{tensor} baru (pada Kode~\ref{lst:SwapOut} dinamakan \code{cpu\_tensor}) sebagai \en{handle} terhadap nilai \en{tensor} yang sekarang terletak di memori CPU. Untuk memperjelas konteks, \code{tensor} ini nantinya digunakan sebagai \en{handle} dalam pemindahan kembali nilai \en{tensor} dari CPU ke GPU (\en{swap in}) ketika dibutuhkan saat \en{backpropagation}.

Sebaliknya, bila \en{memory swapping} tidak dilakukan baik karena \code{swap\_memory\_} bernilai \code{false} atau heuristik tidak terpenuhi, \code{tensor} langsung diletakkan saja ke atas \code{stack}.

\subsubsection{\code{StackPopOp}}

Setelah membahas \code{StackPushOp}, selanjutnya adalah pembahasan mengenai \code{StackPopOp}, kelas yang bertanggung jawab dalam pengambilan \code{tensor} dari atas \code{stack}. Berikut adalah cuplikan kode \code{StackPopOp} yang telah dimodifikasi sehingga hanya menampilkan bagian yang menjadi perhatian.

\begin{lstlisting}[language=C++, caption=\code{StackPopOp} (\code{tensorflow/core/kernels/stack.cc}), label={lst:SwapIn}]
void StackPopOp::ComputeAsync(OpKernelContext* ctx, DoneCallback done) {
  // ... initializations

  Stack::TensorAndAllocation value;
  OP_REQUIRES_OK_ASYNC(ctx, stack->Pop(&value), done);
  if (value.swapped_to_cpu) {
    // Asynchronously copy the tensor back from CPU to GPU memory.
    // ...
    Tensor* cpu_tensor = &value.tensor;
    Tensor* device_tensor = ...
    device_ctxt->CopyCPUTensorToDevice(
        cpu_tensor, device, device_tensor,
        [device_tensor, ctx, done](const Status& s) {
          // ...
          if (s.ok()) {
            ctx->set_output(0, *device_tensor);
          }
          done();
          // ...
        });
  } else {
    // Execute synchronously if not swapped.
    ctx->set_output(0, value.tensor);
    done();
  }
}
\end{lstlisting}

Dari Kode~\ref{lst:SwapIn} di atas, terlihat bahwa pada \code{StackPopOp}, TensorFlow memeriksa terlebih dahulu apakah sebelumnya \code{tensor} telah dipindahkan ke CPU. Jika iya, maka nilai \en{tensor} dipindahkan terlebih dahulu dari CPU ke GPU secara \en{asynchronous}. Jika tidak, maka nilai \en{tensor} dapat langsung dipakai karena sudah ada di GPU.

Meskipun pemindahan dilakukan secara \en{asynchronous} (tidak mem-\en{block} CPU), namun penggunaan \code{stack} tetap \en{blocked}, karena menunggu pemanggilan \code{done()} dilakukan. Dapat diperhatikan bahwa pemanggilan \code{done()} pada kasus \en{swapping in} (baris 19 pada Kode~\ref{lst:SwapIn}) dilakukan pada \en{callback} (berarti setelah \en{swapping in} selesai). Seperti disebutkan di Bagian~\ref{stack}, karena masing-masing \en{op} memiliki \en{stack}-nya sendiri, dan setiap elemen di \en{stack} merepresentasikan setiap \en{timestep}, maka implikasi dari \en{blocking} ini adalah \en{op} tidak dapat digunakan untuk \en{timestep} berikutnya hingga \en{swapping in} untuk \en{timestep} sekarang telah selesai (memanggil \code{done()}).

Kembali menelaah kedua kode tersebut, dapat diambil beberapa kesimpulan. Terdapat heuristik yang harus dipenuhi untuk menentukan apakah sebuah \en{tensor} layak di-\en{swap out} ke CPU, yaitu ketika ukuran \en{tensor} melebihi \code{kCopyThreshold} (2048 \en{bytes}) dan persentase penggunaan memori GPU melebihi \code{kOccupancy} (70\%). Hal yang perlu diperhatikan juga adalah ketika heuristik terpenuhi, \en{tensor} yang di-\en{swap out} adalah \en{tensor} yang baru akan diletakkan ke atas \code{stack} (\en{tensor} terbaru). Selain itu, dari Kode~\ref{lst:SwapIn} terlihat bahwa \en{swap in} sebuah \en{tensor} baru dilakukan saat \en{tensor} tersebut akan dipakai.

\subsection{Potensi Optimisasi dengan \en{Memory Swapping}} \label{optimization}

Pada umumnya, pelatihan model \en{deep learning} dilakukan dengan teknik \en{backpropagation}. Secara singkat, satu iterasi pelatihan terdiri dari 2 tahap, \en{forward pass} dan \en{backpropagation}. Saat \en{forward pass}, \en{input layer} menerima input kemudian mempropagasinya \en{layer} demi \en{layer} hingga didapatkan \en{output} di \en{output layer}. Setelahnya, \en{output} dibandingkan dengan label, kemudian dari selisihnya (\en{loss}) tersebut disesuaikanlah parameter-parameter model (bobot dan bias) dari \en{output layer} hingga \en{input layer} sedemikian rupa sehingga secara keseluruhan, \en{loss} bernilai minimum. Mengenai \en{backpropagation} dijelaskan secara lebih mendalam pada Bagian~\ref{backpropagation} dan \ref{bptt}.

Melihat proses pelatihan dengan \en{backpropagation} tersebut, terdapat interval antara digunakannya suatu \en{layer} saat \en{forward pass} hingga digunakannya kembali saat \en{backpropagation}. Dilihat dari sisi penggunaan memori, pada interval ini \en{layer} tetap mengonsumsi memori walaupun tidak sedang digunakan. Konsumsi memori yang tidak perlu ini tentunya lebih baik bila dapat digunakan untuk keperluan lain.

Dari penjelasan tersebut, terlihat potensi optimisasi penggunaan memori pada pelatihan model \en{deep learning}. Memori untuk \en{layer}-\en{layer} yang sedang tidak digunakan tersebut lebih baik bila didelegasikan ke \en{memory pool} yang lebih besar.

Maka dari itu, seperti yang disebutkan oleh \citep{meng2017training}, \en{tensor} yang paling baik untuk di-\en{swap} adalah \en{tensor} dari \en{layer}-\en{layer} awal, karena interval dari \en{forward pass} hingga digunakannya kembali saat \en{backpropagation} yang terlama. Dengan begitu, diharapkan bahwa \en{overlapping} antara komputasi dengan \en{memory swapping} meningkat, sehingga mengurangi waktu yang dibutuhkan (\en{memory copy} antara GPU dan CPU berjalan secara \en{asynchronous}).

Bila melihat heuristik \en{memory swapping} di TensorFlow yang dijelaskan di bagian sebelumnya, terlihat bahwa yang dilakukan TensorFlow justru kebalikannya. Karena ketika heuristik terpenuhi, \en{swapping} dilakukan terhadap \en{tensor} saat ini (\en{tensor} terbaru), yang berarti interval hingga digunakannya kembali tidak terlalu lama. Terlebih lagi, \en{tensor} baru di-\en{swap in} saat akan digunakan kembali di \en{backpropagation}, mengurangi \en{asynchronicity} dari eksekusi program. Di sinilah terletak potensi optimisasi \en{memory swapping} di TensorFlow.

\subsection{\en{Recurrent Neural Network} sebagai Model Uji}

Arsitektur \en{deep neural network} yang dipilih untuk menjadi fokus optimisasi ini adalah \en{recurrent neural network} (RNN) yang antara \en{input layer} atau \en{output layer}-nya (atau keduanya) berjenis \en{sequence}. Alasan pemilihan arsitektur RNN adalah karena topologinya yang dinamis menurut panjang \en{input} atau \en{output}, penggunaan memori tidak dapat diprediksi sebelum eksekusi, sehingga optimisasi memori pun menjadi lebih krusial.

Selain itu, seperti yang dijelaskan pada Bagian~\ref{bptt}, pelatihan RNN dengan \en{backpropagation through time} lebih kompleks dan sulit dari \en{backpropagation} biasa, karena perhitungan harus ikut memperhitungkan \en{timestep} yang ada. Faktor ini membuat pelatihan RNN cenderung membutuhkan lebih banyak waktu dan memori dari arsitektur lainnya.

Alasan lainnya adalah fitur \en{memory swapping} di TensorFlow hanya terdapat pada \code{tf.nn.dynamic\_rnn} (kelas RNN dinamis), sehingga dengan memilih model berarsitektur RNN, \en{memory swapping} hasil optimisasi dapat dibandingkan dengan \en{memory swapping} bawaan dari TensorFlow.

\section{Rancangan Solusi} \label{rancangansolusi}

Setelah menganalisis permasalahan, didapatkan bahwa terdapat potensi optimisasi \en{memory swapping} di TensorFlow, dengan memprioritaskan \en{tensor} dengan interval waktu yang terlama hingga digunakannya kembali saat \en{backpropagation}. Selanjutnya, ide optimisasi tersebut perlu diimplementasikan ke TensorFlow.

Seperti yang disebutkan pada Bagian~\ref{optimization}, ide optimisasi tersebut terinspirasi dari \citep{meng2017training}. Namun, sebenarnya \en{paper} tersebut menceritakan mekanisme dari \en{memory swapping} yang diimplementasikan sendiri oleh para penulis (bukan bawaan TensorFlow) beserta optimisasinya (dijelaskan secara lebih detail di Bagian~\ref{relatedworks}). Sedangkan fokus tugas akhir ini adalah mengoptimisasi \en{memory swapping} bawaan TensorFlow, sehingga pendekatan dan detail optimisasi yang dilakukan di tugas akhir ini berbeda dari \en{paper} tersebut, karena memang implementasi \en{memory swapping} bawaan TensorFlow berbeda jauh dari implementasi \en{memory swapping} di \en{paper} tersebut.

Dengan demikian, diperlukan implementasi yang dapat mengoptimisasi sesuai ide tersebut, namun terhadap mekanisme \en{memory swapping} bawaan TensorFlow yang dijelaskan di Bagian~\ref{mechanism}. Berikut akan dijelaskan rancangan implementasi yang dibuat oleh penulis.

\begin{lstlisting}[language=C++, caption=Pseudocode \en{Swap Out} Bawaan, label={lst:SwapOutPseudocode}]
if (passes_heuristic) {
    swap_out(current_tensor, (cpu_tensor) => {
        stack.push(cpu_tensor)
    })
} else {
    stack.push(current_tensor)
}
\end{lstlisting}

\begin{lstlisting}[language=C++, caption=Pseudocode \en{Swap Out} Teroptimisasi, label={lst:SwapOutPseudocodeOptimized}]
stack.push(current_tensor)

if (passes_heuristic) {
    id = stack.get_oldest_unswapped_tensor_id()
    swap_out(stack[id].tensor, (cpu_tensor) => {
        // set respective stack element to swapped
        stack[id].tensor = cpu_tensor
        stack[id].swapped = true
    })
}
\end{lstlisting}

\begin{lstlisting}[language=C++, caption=Pseudocode \en{Swap In} Asli, label={lst:SwapInPseudocode}]
tensor = stack.pop()
if (tensor.swapped) {
    swap_in(tensor, (gpu_tensor) => {
        current_tensor = gpu_tensor
    })
} else {
    current_tensor = tensor
}
\end{lstlisting}

\begin{lstlisting}[language=C++, caption=Pseudocode \en{Swap In} Teroptimisasi, label={lst:SwapInPseudocodeOptimized}]
tensor = stack.pop()
if (current_tensor.swapped) {
    swap_in(tensor, (gpu_tensor) => {
        current_tensor = gpu_tensor
    })
} else {
    current_tensor = tensor
}

if (passes_heuristic) {
    id = stack.get_newest_swapped_tensor_id()
    swap_in(stack[id].tensor, (gpu_tensor) => {
        // set respective stack element to unswapped
        stack[id].tensor = gpu_tensor
        stack[id].swapped = false
    })
}
\end{lstlisting}
