\chapter{Implementasi dan Pengujian} \label{ImplementationEvaluation}

\section{Implementasi}

Pada bagian ini dijelaskan mengenai implementasi optimisasi \en{memory swapping} di TensorFlow menurut \nameref{SolutionDesign}. Implementasi yang dilakukan berupa modifikasi terhadap \en{source code} TensorFlow, terutama di \en{file} \code{tensorflow/core/\\kernels/stack.cc}.

Secara garis besar, implementasi dapat dibagi menjadi 2 bagian, yaitu modifikasi \en{swapping out} menurut Kode~\ref{lst:SwapOutPseudocodeOptimized} dan \en{swapping in} menurut Kode~\ref{lst:SwapInPseudocodeOptimized}.

\subsection{Modifikasi \en{Swapping Out}}

Pada Kode~\ref{lst:SwapOutPseudocodeOptimized} yang menjelaskan optimisasi \en{swapping out}, terdapat metode \code{get\_\\oldest\_unswapped\_tensor\_id} yang berfungsi mengembalikan indeks dari \en{oldest tensor} yang belum di-\en{swap}. Dari sini, \code{Stack} perlu dimodifikasi untuk mengakomodasi metode tersebut.

Pertama, dibutuhkan sebuah \en{queue} untuk menyimpan indeks-indeks \en{tensor} yang belum di-\en{swap}. \en{Queue} dipilih agar indeks \en{tensor} yang di-\en{dequeue} adalah yang terawal (\en{oldest}). Selanjutnya, metode \code{Push} pada \code{Stack} juga perlu dimodifikasi agar ketika sebuah \en{tensor} di-\en{push} ke \en{stack}, indeksnya juga di-\en{enqueue} ke \en{queue} tersebut. Kode~\ref{lst:UnswappedTensorIds} menunjukkan deklarasi \en{queue} dan modifikasi \code{Push} tersebut.

\begin{lstlisting}[language=C++, caption=Deklarasi \code{unswapped\_ids\_} dan modifikasi \code{Push}, label={lst:UnswappedTensorIds}]
class Stack {
  private:
    // ...
    std::queue<int> unswapped_ids_;

  public:
    // ...
    Status Push(const TensorAndAllocation& value) {
      // ...
      stack_.push_back(value);
      int index = stack_.size() - 1;
      unswapped_ids_.push_back(index);
      return Status::OK();
  }
}
\end{lstlisting}

Setelahnya, dapat diimplementasi metode yang mengambil indeks \en{oldest tensor} yaitu \code{GetTensorToSwapOut}. Metode tersebut mengambil indeks \en{oldest tensor} dari \code{unswapped\_ids\_} kemudian meng-\en{assign} \en{tensor} tersebut ke parameter \en{output} \code{value}. Selain itu, indeks juga di-\en{push} ke \code{swapped\_ids\_} yang akan dijelaskan kemudian. Kode~\ref{lst:GetTensorToSwapOut} merupakan simplifikasi dari implementasi \code{GetTensorToSwapOut}.

\begin{lstlisting}[language=C++, caption=Definisi \code{GetTensorToSwapOut}, label={lst:GetTensorToSwapOut}]
class Stack {
  public:
    // ...
    void GetTensorToSwapOut(TensorAndAllocation** value) {
      if (unswapped_ids_.empty())
        return;

      int index = unswapped_ids_.front();
      unswapped_ids_.pop();
      swapped_ids_.push_back(index);
      *value = &stack_[index];
    }
}
\end{lstlisting}

Berikutnya adalah modifikasi terhadap \code{StackPushOp} mengikuti Kode~\ref{lst:SwapOutPseudocodeOptimized}. Seperti pada \en{pseudocode} tersebut, modifikasi tetap mengikuti heuristik yang ada, yaitu melakukan \en{swapping} ketika memori GPU 70\% penuh, namun yang di-\en{swap out} bukanlah \en{tensor} sekarang, malainkan \en{oldest tensor} yang didapat dari \code{GetTensorToSwapOut}. Implementasi modifikasi ini ditunjukkan oleh Kode~\ref{lst:StackPushOp} dengan beberapa penyederhanaan.

\begin{lstlisting}[language=C++, caption=Modifikasi \code{StackPushOp}, label={lst:StackPushOp}]
void StackPushOp::ComputeAsync(OpKernelContext* ctx, DoneCallback done) {
  // ...

  // Push current tensor to Stack.
  ctx, stack->Push({tensor, alloc_attrs, false});
  ctx->set_output(0, tensor);

  // ...

  // If device memory <= 70% full no need to swap.
  // ...
  static constexpr double kOccupancy = 0.7;
  if (stats.bytes_in_use <= (stats.bytes_limit * kOccupancy)) {
    done();
    return;
  }

  // Obtain the oldest unswapped TensorAndAllocation pointer from queue.
  Stack::TensorAndAllocation* oldest_tensor;
  stack->GetTensorToSwapOut(&oldest_tensor);

  // Asynchronously swap out the oldest tensor.
  // ...
  Tensor* cpu_tensor = ...
  device_ctxt->CopyDeviceTensorToCPU(
      &(oldest_tensor->tensor), "StackPush", device, cpu_tensor,
      [stack, oldest_tensor, cpu_tensor, done](const Status& s) {
        if (s.ok()) {
          oldest_tensor->tensor = *cpu_tensor;
          oldest_tensor->swapped_to_cpu = true;
        }
        done();
        delete cpu_tensor;
      });
}
\end{lstlisting}

Pada bagian terakhir Kode~\ref{lst:StackPushOp} di atas terlihat proses \en{swapping out} yang dilakukan terhadap \en{oldest tensor}. \en{Swapping out} dilakukan secara \en{asynchronous} yang setelah selesai dijalankan memanggil sebuah \en{callback function}. \en{Callback} ini mengubah elemen \en{oldest tensor} di \en{stack}, dari \code{tensor} yang merujuk ke \code{device\_tensor} menjadi \code{cpu\_tensor} dan \code{swapped\_to\_cpu} menjadi \code{true} agar pada \en{swapping in} dapat diketahui bahwa \en{tensor} bersangkutan telah di-\en{swap}. (Lebih detail mengenai \code{tensor} dijelaskan pada Bagian~\ref{Tensor}.)

Demikian implementasi pada bagian \en{swapping out}, di mana \en{oldest tensor} di-\en{swap out} agar pada \en{backpropagation} dapat di-\en{swap in} bersamaan dengan berjalannya komputasi, sehingga meningkatkan \en{asynchronicity}. Berikutnya adalah penjelasan mengenai implementasi pada bagian \en{swapping in}.

\subsection{Modifikasi \en{Swapping In}}

Pada bagian ini dijelaskan implementasi menurut Kode~\ref{lst:SwapInPseudocodeOptimized} yang menjelaskan optimisasi pada bagian \en{swapping in}. Namun, sebelum masuk ke bagian tersebut perlu ditunjukkan kode-kode yang mendukung implementasi tersebut.

Sebelumnya, pada Kode~\ref{lst:GetTensorToSwapOut}, indeks di-\en{push} ke \code{swapped\_ids\_}, yaitu sebuah \en{stack} yang menyimpan indeks-indeks \en{tensor} yang di-\en{swap}. Digunakan \en{stack} karena kebalikan dari \en{swapping out}, \en{swapping in} dilakukan terhadap \en{swapped tensor} yang terbaru (\en{most recent}) dahulu. Kode~\ref{lst:SwappedTensorIds} menunjukkan deklarasi \en{swapped\_ids\_}.

\begin{lstlisting}[language=C++, caption=Deklarasi \code{swapped\_ids\_}, label={lst:SwappedTensorIds}]
class Stack {
  private:
    // ...
    std::queue<int> swapped_ids_;
}
\end{lstlisting}

Selanjutnya adalah penjelasan mengenai \code{GetTensorToSwapIn}, metode yang mengambil indeks \en{tensor} selanjutnya harus di-\en{code}. Secara garis besar, metode ini mirip dengan \code{GetTensorToSwapOut} namun terhadap \code{swapped\_ids\_}, seperti ditunjukkan oleh Kode~\ref{lst:GetTensorToSwapIn}

\begin{lstlisting}[language=C++, caption=Definisi \code{GetTensorToSwapIn}, label={lst:GetTensorToSwapIn}]
class Stack {
  public:
    // ...
    void GetTensorToSwapIn(TensorAndAllocation** value) {
      if (swapped_ids_.empty())
        return;

      int index = swapped_ids_.top();
      swapped_ids_.pop();
      *value = &stack_[index];
    }
}
\end{lstlisting}

Berikutnya adalah modifikasi terhadap \code{StackPopOp} mengikuti Kode~\ref{lst:SwapInPseudocodeOptimized}. Seperti pada \en{pseudocode} tersebut, \code{StackPopOp} dimodifikasi sehingga setelah mem-\en{pop} \en{tensor} teratas dari \en{stack}, dilakukan juga \en{swapping in} terhadap \en{tensor}-\en{tensor} berikutnya secara \en{asynchronous} untuk meningkatkan \en{overlapping} antara komputasi dengan \en{memory transfer}. Implementasi modifikasi ini ditunjukkan oleh Kode~\ref{lst:StackPopOp} dengan beberapa penyederhanaan.

\begin{lstlisting}[language=C++, caption=Modifikasi \code{StackPopOp}, label={lst:StackPopOp}]
void StackPopOp::ComputeAsync(OpKernelContext* ctx, DoneCallback done) {
  // ...

  // Pop a tensor from Stack.
  Stack::TensorAndAllocation value;
  OP_REQUIRES_OK_ASYNC(ctx, stack->Pop(&value), done);

  // If the tensor was swapped out, then swap in (not shown for clarity).
  // ...

  // Asynchronously swap "future" tensors.

  // If device memory still > 90% full don't swap in yet.
  // ...
  static constexpr double kOccupancy = 0.7;
  if (stats.bytes_in_use > (stats.bytes_limit * kOccupancy)) {
    return;
  }

  Stack::TensorAndAllocation* swapped_tensor;
  stack->GetTensorToSwapIn(&swapped_tensor);

  Tensor* device_tensor = ...
  device_ctxt->CopyCPUTensorToDevice(
      &(swapped_tensor->tensor), device, device_tensor,
      [stack, swapped_tensor, device_tensor, index](const Status& s) {
        if (s.ok()) {
          swapped_tensor->tensor = *device_tensor;
          swapped_tensor->swapped_to_cpu = false;
        }
        delete device_tensor;
      });
}
\end{lstlisting}

Pada bagian Kode~\ref{lst:StackPopOp} di atas terlihat proses \en{swapping in} terhadap "\en{future tensors}", yaitu \en{swapped tensors} yang belum akan digunakan untuk komputasi, dengan tujuan meningkatkan \en{overlapping} antara komputasi dengan \en{memory transfer}, seperti yang telah disebutkan sebelumnya.

Terdapat satu lagi hal yang perlu dibahas, yaitu herustik pada \en{swapping in}. Seperti heuristik pada \en{swapping out}, \en{swapping in} tersebut dilakukan ketika memori GPU hampir penuh. Dipilih nilai yang \code{kOccupancy} yang sama yaitu 0.7, untuk menyamai nilai yang digunakan di \en{swapping out}, karena nilai heuristik tersebut sudah ada di versi bawaan TensorFlow.

Demikian penjelasan mengenai implementasi optimisasi \en{memory swapping} di TensorFlow. Berikutnya dilakukan pengujian terhadap optimisasi ini dibandingkan dengan versi bawaan TensorFlow.

\section{Pengujian}

Pada bagian ini dijelaskan mengenai pengujian terhadap optimisasi \en{memory swapping} di TensorFlow yang telah diimplementasikan. Berikut dijelaskan lingkungan pengujian, metode pengujian, dan hasil pengujian dalam berbagai kasus pengujian.

\subsection{Lingkungan Pengujian}

Tabel~\ref{tbl:Specification} memaparkan spesifikasi lingkungan yang digunakan penulis dalam melakukan pengujian.

{\renewcommand{\arraystretch}{0.75}
\begin{table}[]
\centering
\caption{Spesifikasi Lingkungan Pengujian}
\label{tbl:Specification}
\begin{tabular}{|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{Original}} & \multicolumn{1}{c|}{\textbf{Optimized}} \\ \hline
\en{Operating System}                   & Ubuntu 18.04 64-bit                                                                                            \\ \hline
\en{Processor}                          & Intel Xeon CPU @ 2.20GHz                                                                                       \\ \hline
\en{Memory}                             & 12 GB                                                                                                          \\ \hline
\en{GPU}                                & \begin{tabular}[c]{@{}l@{}}NVIDIA Tesla P4\\ \en{Memory}: 8 GB\\ \en{Driver}: 418.56\\ CUDA: 10.0\end{tabular} \\ \hline
\end{tabular}
\end{table}}

\subsection{Metode Pengujian}

Setiap pengujian dilakukan dengan beberapa kali melakukan \en{training} suatu model RNN menggunakan TensorFlow asli dan teroptimisasi, kemudian durasi \en{training} kedua perlakuan dibandingkan dan dihitung rataannya. Selain itu, dilakukan uji statistik lain seperti \en{t-test} untuk memastikan bahwa durasi \en{training} kedua perlakuan memang berbeda.

Model yang digunakan untuk pengujian adalah char-rnn yang berdasarkan pada \citep{karpathy2015unreasonable}, namun telah diadaptasi ke TensorFlow. Secara singkat, char-rnn adalah model RNN yang menerima sekuens teks kemudian membangkitkan sekuens teks lanjutannya, huruf per huruf. Data yang digunakan merupakan teks berisi 26172 karakter.

Implementasi model tersebut dengan TensorFlow menggunakan kelas \code{dynamic\_rnn} (disebutkan di Bagian~\ref{WhyRNN}) dengan parameter \code{swap\_memory=True} untuk mengijinkan TensorFlow melakukan \en{memory swapping}. Selain itu, parameter-parameter yang digunakan dalam menjalankan pengujian dipilih sedemikian rupa agar ukuran memori yang dibutuhkan melebihi ukuran memori yang ada, sehingga TensorFlow terpaksa melakukan \en{memory swapping}.

\subsection{Hasil Pengujian}

Berikut ditunjukkan hasil pengujian antara TensorFlow asli dan teroptimisasi dalam berbagai kasus pengujian.

\subsubsection{Kasus Pertama}

Pada pengujian pertama, digunakan parameter-parameter sebagai berikut. Pada pengujian pertama dan kedua dipilih jumlah \en{unrolling} yang relatif kecil (20) agar dapat dibandingkan dengan pengujian-pengujian setelahnya yang menggunakan jumlah \en{unrolling} lebih besar.

\begin{enumerate}
  \item Ukuran \en{batch}: 6000
  \item Jumlah \en{hidden layer}: 8
  \item Ukuran \en{hidden layer}: 512
  \item Jumlah \en{unrolling}: 20
  \item Jumlah \en{epoch}: 200
\end{enumerate}

Tabel~\ref{tbl:First} menunjukkan durasi \en{training} (dalam satuan detik) kedua perlakuan pada masing-masing 10 kali percobaan. Dari hasil pengujian tersebut, terlihat bahwa versi teroptimisasi berhasil mengurangi durasi \en{training} sekitar 1.44\%. Menurut uji statistik \en{t-test}, didapatkan \en{p-value} bernilai 1.8493031002146812e-06, sehingga secara statistik dapat dinyatakan bahwa kedua perlakuan berbeda secara signifikan.

\begin{table}[]
\centering
\caption{Hasil pengujian kasus pertama}
\label{tbl:First}
\small
\begin{tabular}{cll}
\hline
\multicolumn{1}{|c|}{i}  & \multicolumn{1}{c|}{\textbf{Original}}  & \multicolumn{1}{c|}{\textbf{Optimized}} \\ \hline
\multicolumn{1}{|c|}{1}  & \multicolumn{1}{l|}{252.15827584266663} & \multicolumn{1}{l|}{252.97898483276367} \\ \hline
\multicolumn{1}{|c|}{2}  & \multicolumn{1}{l|}{256.4153354167938}  & \multicolumn{1}{l|}{252.72823286056519} \\ \hline
\multicolumn{1}{|c|}{3}  & \multicolumn{1}{l|}{256.832067489624}   & \multicolumn{1}{l|}{252.96671104431152} \\ \hline
\multicolumn{1}{|c|}{4}  & \multicolumn{1}{l|}{257.4575080871582}  & \multicolumn{1}{l|}{252.72692227363586} \\ \hline
\multicolumn{1}{|c|}{5}  & \multicolumn{1}{l|}{256.82346057891846} & \multicolumn{1}{l|}{253.96517038345337} \\ \hline
\multicolumn{1}{|c|}{6}  & \multicolumn{1}{l|}{256.21726179122925} & \multicolumn{1}{l|}{251.7913670539856}  \\ \hline
\multicolumn{1}{|c|}{7}  & \multicolumn{1}{l|}{256.92102813720703} & \multicolumn{1}{l|}{252.7251443862915}  \\ \hline
\multicolumn{1}{|c|}{8}  & \multicolumn{1}{l|}{258.0138609409332}  & \multicolumn{1}{l|}{252.35582065582275} \\ \hline
\multicolumn{1}{|c|}{9}  & \multicolumn{1}{l|}{257.0869014263153}  & \multicolumn{1}{l|}{252.23767232894897} \\ \hline
\multicolumn{1}{|c|}{10} & \multicolumn{1}{l|}{256.1273946762085}  & \multicolumn{1}{l|}{252.53468418121338} \\ \hline
\textbf{Mean}            & 256.40531                               & 252.70107
\end{tabular}
\end{table}

\subsubsection{Kasus Kedua}

Pada pengujian kedua, digunakan parameter-parameter yang sama dengan sebelumnya namun dengan jumlah \en{epoch} yang 5 kali lebih besar.

\begin{enumerate}
  \item Ukuran \en{batch}: 6000
  \item Jumlah \en{hidden layer}: 8
  \item Ukuran \en{hidden layer}: 512
  \item Jumlah \en{unrolling}: 20
  \item Jumlah \en{epoch}: 1000
\end{enumerate}

Tabel~\ref{tbl:Second} menunjukkan durasi \en{training} (dalam satuan detik) kedua perlakuan pada masing-masing 10 kali percobaan. Dari hasil pengujian tersebut, terlihat bahwa versi teroptimisasi berhasil mengurangi durasi \en{training} sekitar 1.58\%. Menurut uji statistik \en{t-test}, didapatkan \en{p-value} bernilai 9.21356126659428e-10, sehingga secara statistik dapat dinyatakan bahwa kedua perlakuan berbeda secara signifikan.

\begin{table}[]
\centering
\caption{Hasil pengujian kasus kedua}
\label{tbl:Second}
\small
\begin{tabular}{cll}
\hline
\multicolumn{1}{|c|}{i}  & \multicolumn{1}{c|}{\textbf{Original}}  & \multicolumn{1}{c|}{\textbf{Optimized}} \\ \hline
\multicolumn{1}{|c|}{1}  & \multicolumn{1}{l|}{1254.0009925365448} & \multicolumn{1}{l|}{1239.5168986320496} \\ \hline
\multicolumn{1}{|c|}{2}  & \multicolumn{1}{l|}{1261.5856878757477} & \multicolumn{1}{l|}{1236.1409561634064} \\ \hline
\multicolumn{1}{|c|}{3}  & \multicolumn{1}{l|}{1255.5529029369354} & \multicolumn{1}{l|}{1235.9044954776764} \\ \hline
\multicolumn{1}{|c|}{4}  & \multicolumn{1}{l|}{1255.6840405464172} & \multicolumn{1}{l|}{1233.5171205997467} \\ \hline
\multicolumn{1}{|c|}{5}  & \multicolumn{1}{l|}{1258.378760099411}  & \multicolumn{1}{l|}{1238.562110900879}  \\ \hline
\multicolumn{1}{|c|}{6}  & \multicolumn{1}{l|}{1250.3999752998352} & \multicolumn{1}{l|}{1239.8265266418457} \\ \hline
\multicolumn{1}{|c|}{7}  & \multicolumn{1}{l|}{1257.8104090690613} & \multicolumn{1}{l|}{1239.4132153987885} \\ \hline
\multicolumn{1}{|c|}{8}  & \multicolumn{1}{l|}{1259.7040483951569} & \multicolumn{1}{l|}{1237.5983362197876} \\ \hline
\multicolumn{1}{|c|}{9}  & \multicolumn{1}{l|}{1250.1268837451935} & \multicolumn{1}{l|}{1232.3407201766968} \\ \hline
\multicolumn{1}{|c|}{10} & \multicolumn{1}{l|}{1258.8046193122864} & \multicolumn{1}{l|}{1230.5381321907043} \\ \hline
\textbf{Mean}            & 1256.20483                              & 1236.33585
\end{tabular}
\end{table}

\subsubsection{Kasus Ketiga}

Pada pengujian ketiga, digunakan parameter-parameter sebagai berikut. Dipilih jumlah \en{unrolling} yang lebih besar dari kedua pengujian sebelumnya (50), sehingga ukuran \en{batch} juga dikurangi agar menyeimbangkan kebutuhan memori.

\begin{enumerate}
  \item Ukuran \en{batch}: 3000
  \item Jumlah \en{hidden layer}: 8
  \item Ukuran \en{hidden layer}: 512
  \item Jumlah \en{unrolling}: 50
  \item Jumlah \en{epoch}: 1000
\end{enumerate}

Tabel~\ref{tbl:Third} menunjukkan durasi \en{training} (dalam satuan detik) kedua perlakuan pada masing-masing 10 kali percobaan. Dari hasil pengujian tersebut, terlihat bahwa versi teroptimisasi berhasil mengurangi durasi \en{training} sekitar 3.1\%. Menurut uji statistik \en{t-test}, didapatkan \en{p-value} bernilai 1.1680156931633348e-15, sehingga secara statistik dapat dinyatakan bahwa kedua perlakuan berbeda secara signifikan.

\begin{table}[]
\centering
\caption{Hasil pengujian kasus ketiga}
\label{tbl:Third}
\small
\begin{tabular}{cll}
\hline
\multicolumn{1}{|c|}{i}  & \multicolumn{1}{c|}{\textbf{Original}}  & \multicolumn{1}{c|}{\textbf{Optimized}} \\ \hline
\multicolumn{1}{|c|}{1}  & \multicolumn{1}{l|}{1710.8475177288055} & \multicolumn{1}{l|}{1659.8958191871643} \\ \hline
\multicolumn{1}{|c|}{2}  & \multicolumn{1}{l|}{1708.529179573059}  & \multicolumn{1}{l|}{1657.4155213832855} \\ \hline
\multicolumn{1}{|c|}{3}  & \multicolumn{1}{l|}{1718.1575810909271} & \multicolumn{1}{l|}{1657.545005083084}  \\ \hline
\multicolumn{1}{|c|}{4}  & \multicolumn{1}{l|}{1711.9896774291992} & \multicolumn{1}{l|}{1664.026943206787}  \\ \hline
\multicolumn{1}{|c|}{5}  & \multicolumn{1}{l|}{1716.2954456806183} & \multicolumn{1}{l|}{1660.8334894180298} \\ \hline
\multicolumn{1}{|c|}{6}  & \multicolumn{1}{l|}{1710.3181750774384} & \multicolumn{1}{l|}{1650.7439186573029} \\ \hline
\multicolumn{1}{|c|}{7}  & \multicolumn{1}{l|}{1710.8965182304382} & \multicolumn{1}{l|}{1659.6190533638}    \\ \hline
\multicolumn{1}{|c|}{8}  & \multicolumn{1}{l|}{1722.2678709030151} & \multicolumn{1}{l|}{1670.1134555339813} \\ \hline
\multicolumn{1}{|c|}{9}  & \multicolumn{1}{l|}{1714.1357491016388} & \multicolumn{1}{l|}{1656.9861009120941} \\ \hline
\multicolumn{1}{|c|}{10} & \multicolumn{1}{l|}{1710.631816148758}  & \multicolumn{1}{l|}{1659.0270841121674} \\ \hline
\textbf{Mean}            & 1713.40685                              & 1659.62064
\end{tabular}
\end{table}

\subsubsection{Kasus Keempat}

Pada pengujian keempat, digunakan parameter-parameter yang sama dengan pengujian sebelumnya, tetapi dengan jumlah \en{epoch} digandakan.

\begin{enumerate}
  \item Ukuran \en{batch}: 3000
  \item Jumlah \en{hidden layer}: 8
  \item Ukuran \en{hidden layer}: 512
  \item Jumlah \en{unrolling}: 50
  \item Jumlah \en{epoch}: 2000
\end{enumerate}

Tabel~\ref{tbl:Fourth} menunjukkan durasi \en{training} (dalam satuan detik) kedua perlakuan pada masing-masing 10 kali percobaan. Dari hasil pengujian tersebut, terlihat bahwa versi teroptimisasi berhasil mengurangi durasi \en{training} sekitar 3.4\%. Menurut uji statistik \en{t-test}, didapatkan \en{p-value} bernilai 3.3438e-16, sehingga secara statistik dapat dinyatakan bahwa kedua perlakuan berbeda secara signifikan.

\begin{table}[]
\centering
\caption{Hasil pengujian kasus keempat}
\label{tbl:Fourth}
\small
\begin{tabular}{cll}
\hline
\multicolumn{1}{|c|}{i}           & \multicolumn{1}{c|}{\textbf{Original}}  & \multicolumn{1}{c|}{\textbf{Optimized}} \\ \hline
\multicolumn{1}{|c|}{1}           & \multicolumn{1}{l|}{3419.5844151973724} & \multicolumn{1}{l|}{3310.971428632736}  \\ \hline
\multicolumn{1}{|c|}{2}           & \multicolumn{1}{l|}{3432.882806301117}  & \multicolumn{1}{l|}{3315.9460911750793} \\ \hline
\multicolumn{1}{|c|}{3}           & \multicolumn{1}{l|}{3413.0236432552338} & \multicolumn{1}{l|}{3304.504233598709}  \\ \hline
\multicolumn{1}{|c|}{4}           & \multicolumn{1}{l|}{3421.1436865329742} & \multicolumn{1}{l|}{3314.9957132339478} \\ \hline
\multicolumn{1}{|c|}{5}           & \multicolumn{1}{l|}{3402.4240250587463} & \multicolumn{1}{l|}{3328.4635627269745} \\ \hline
\multicolumn{1}{|c|}{6}           & \multicolumn{1}{l|}{3415.1220932006836} & \multicolumn{1}{l|}{3312.9601073265076} \\ \hline
\multicolumn{1}{|c|}{7}           & \multicolumn{1}{l|}{3414.628209590912}  & \multicolumn{1}{l|}{3313.3753986358643} \\ \hline
\multicolumn{1}{|c|}{8}           & \multicolumn{1}{l|}{3411.1480510234833} & \multicolumn{1}{l|}{3307.7781500816345} \\ \hline
\multicolumn{1}{|c|}{9}           & \multicolumn{1}{l|}{3414.513864994049}  & \multicolumn{1}{l|}{3327.114767551422}  \\ \hline
\multicolumn{1}{|c|}{10}          & \multicolumn{1}{l|}{3422.7600326538086} & \multicolumn{1}{l|}{3325.095719099045}  \\ \hline
\multicolumn{1}{l}{\textbf{Mean}} & 3416.72308                              & 3316.12052
\end{tabular}
\end{table}

\subsubsection{Kasus Kelima}

Pada pengujian ketiga, digunakan parameter-parameter sebagai berikut. Dipilih jumlah \en{unrolling} yang lebih besar dari pengujian-pengujian sebelumnya (80), sehingga ukuran \en{batch} juga dikurangi agar menyeimbangkan kebutuhan memori.

\begin{enumerate}
  \item Ukuran \en{batch}: 2400
  \item Jumlah \en{hidden layer}: 8
  \item Ukuran \en{hidden layer}: 512
  \item Jumlah \en{unrolling}: 80
  \item Jumlah \en{epoch}: 500
\end{enumerate}

Tabel~\ref{tbl:Fifth} menunjukkan durasi \en{training} (dalam satuan detik) kedua perlakuan pada masing-masing 10 kali percobaan. Dari hasil pengujian tersebut, terlihat bahwa versi teroptimisasi berhasil mengurangi durasi \en{training} sekitar 3.11\%. Menurut uji statistik \en{t-test}, didapatkan \en{p-value} bernilai 2.426062620387277e-18, sehingga secara statistik dapat dinyatakan bahwa kedua perlakuan berbeda secara signifikan.

\begin{table}[]
\centering
\caption{Hasil pengujian kasus kelima}
\label{tbl:Fifth}
\small
\begin{tabular}{cll}
\hline
\multicolumn{1}{|c|}{i}           & \multicolumn{1}{c|}{\textbf{Original}}  & \multicolumn{1}{c|}{\textbf{Optimized}} \\ \hline
\multicolumn{1}{|c|}{1}           & \multicolumn{1}{l|}{1165.6820361614227} & \multicolumn{1}{l|}{1126.7230513095856} \\ \hline
\multicolumn{1}{|c|}{2}           & \multicolumn{1}{l|}{1163.4759526252747} & \multicolumn{1}{l|}{1130.8513023853302} \\ \hline
\multicolumn{1}{|c|}{3}           & \multicolumn{1}{l|}{1162.6871263980865} & \multicolumn{1}{l|}{1126.5956711769104} \\ \hline
\multicolumn{1}{|c|}{4}           & \multicolumn{1}{l|}{1165.6165866851807} & \multicolumn{1}{l|}{1127.9965732097626} \\ \hline
\multicolumn{1}{|c|}{5}           & \multicolumn{1}{l|}{1164.6632671356201} & \multicolumn{1}{l|}{1129.0060029029846} \\ \hline
\multicolumn{1}{|c|}{6}           & \multicolumn{1}{l|}{1163.5531344413757} & \multicolumn{1}{l|}{1126.5449559688568} \\ \hline
\multicolumn{1}{|c|}{7}           & \multicolumn{1}{l|}{1163.979398727417}  & \multicolumn{1}{l|}{1129.5302891731262} \\ \hline
\multicolumn{1}{|c|}{8}           & \multicolumn{1}{l|}{1165.8543801307678} & \multicolumn{1}{l|}{1127.9539105892181} \\ \hline
\multicolumn{1}{|c|}{9}           & \multicolumn{1}{l|}{1165.2025880813599} & \multicolumn{1}{l|}{1134.730679988861}  \\ \hline
\multicolumn{1}{|c|}{10}          & \multicolumn{1}{l|}{1166.7084350585938} & \multicolumn{1}{l|}{1124.3404479026794} \\ \hline
\multicolumn{1}{l}{\textbf{Mean}} & 1164.74229                              & 1128.42729
\end{tabular}
\end{table}

\subsubsection{Kasus Keenam}

Pada pengujian keenam, digunakan parameter-parameter yang sama dengan pengujian sebelumnya, tetapi dengan jumlah \en{epoch} digandakan.

\begin{enumerate}
  \item Ukuran \en{batch}: 2400
  \item Jumlah \en{hidden layer}: 8
  \item Ukuran \en{hidden layer}: 512
  \item Jumlah \en{unrolling}: 80
  \item Jumlah \en{epoch}: 1000
\end{enumerate}

Tabel~\ref{tbl:Sixth} menunjukkan durasi \en{training} (dalam satuan detik) kedua perlakuan pada masing-masing 10 kali percobaan. Dari hasil pengujian tersebut, terlihat bahwa versi teroptimisasi berhasil mengurangi durasi \en{training} sekitar 3.09\%. Menurut uji statistik \en{t-test}, didapatkan \en{p-value} bernilai 1.921296333185339e-18, sehingga secara statistik dapat dinyatakan bahwa kedua perlakuan berbeda secara signifikan.

\begin{table}[]
\centering
\caption{Hasil pengujian kasus keenam}
\label{tbl:Sixth}
\small
\begin{tabular}{cll}
\hline
\multicolumn{1}{|c|}{i}           & \multicolumn{1}{c|}{\textbf{Original}}  & \multicolumn{1}{c|}{\textbf{Optimized}} \\ \hline
\multicolumn{1}{|c|}{1}           & \multicolumn{1}{l|}{2325.881348848343}  & \multicolumn{1}{l|}{2259.6721198558807} \\ \hline
\multicolumn{1}{|c|}{2}           & \multicolumn{1}{l|}{2324.937485218048}  & \multicolumn{1}{l|}{2248.6250746250153} \\ \hline
\multicolumn{1}{|c|}{3}           & \multicolumn{1}{l|}{2324.66451215744}   & \multicolumn{1}{l|}{2257.6384501457214} \\ \hline
\multicolumn{1}{|c|}{4}           & \multicolumn{1}{l|}{2326.308651447296}  & \multicolumn{1}{l|}{2252.499354362488}  \\ \hline
\multicolumn{1}{|c|}{5}           & \multicolumn{1}{l|}{2329.8989474773407} & \multicolumn{1}{l|}{2242.113233089447}  \\ \hline
\multicolumn{1}{|c|}{6}           & \multicolumn{1}{l|}{2322.9741718769073} & \multicolumn{1}{l|}{2255.954655647278}  \\ \hline
\multicolumn{1}{|c|}{7}           & \multicolumn{1}{l|}{2327.4178791046143} & \multicolumn{1}{l|}{2256.2117512226105} \\ \hline
\multicolumn{1}{|c|}{8}           & \multicolumn{1}{l|}{2327.0376517772675} & \multicolumn{1}{l|}{2257.8214254379272} \\ \hline
\multicolumn{1}{|c|}{9}           & \multicolumn{1}{l|}{2328.337978363037}  & \multicolumn{1}{l|}{2256.3289828300476} \\ \hline
\multicolumn{1}{|c|}{10}          & \multicolumn{1}{l|}{2330.1756098270416} & \multicolumn{1}{l|}{2261.2293424606323} \\ \hline
\multicolumn{1}{l}{\textbf{Mean}} & 2326.76342                              & 2254.80944
\end{tabular}
\end{table}
