\chapter{Studi Literatur}

\section{\en{Deep Learning}}

\en{Deep learning} adalah suatu bagian dari bidang \en{machine learning}, di mana pembelajaran dilakukan pada \en{neural network} dengan \en{layer} berjumlah banyak \citep{deng2014deep}. Berbagai arsitektur \en{deep learning} seperti \en{convolutional neural network} (CNN) dan \en{recurrent neural network} (RNN) telah banyak diaplikasikan pada berbagai bidang AI, seperti \en{computer vision} dan \en{speech recognition}.

Berikut akan dijelaskan lebih dalam mengenai \en{deep learning}, termasuk salah satu jenis arsitektur, metode pelatihan, dan penggunaan memorinya.

\subsection{\en{Overview}}

Ide dasar \en{deep learning} adalah menggunakan model \en{neural network} dengan \en{hidden layer} berjumlah lebih dari satu, yang disebut juga \en{deep neural network}. Gambar ~\ref{fig:DeepNeuralNetwork} mengilustrasikan \en{deep neural network} yang memiliki 3 \en{hidden layer}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/deep-neural-network.png}
    \caption{Arsitektur \en{deep neural network} \citep{nielsen2015neural}.}
    \label{fig:DeepNeuralNetwork}
\end{figure}

Seperti \en{neural network} pada \en{shallow learning}, setiap hubungan neuron memiliki bobot (\en{weight}) yang nilainya merepresentasikan signifikansi hubungan antar neuron tersebut. Selain bobot, terdapat juga bias yang secara langsung mempengaruhi nilai setiap neuron (tidak seperti bobot yang mempengaruhi hubungan antara neuron), dengan nilainya merepresentasikan signifikasi suatu neuron. Secara garis besar, tujuan setiap neuron dan hubungan antar neuron adalah mengenali pola atau fitur dari \en{dataset} sehingga dapat memprediksi \en{output} dengan benar \citep{nielsen2015neural}.

Maka, pelatihan \en{neural network} itu sendiri adalah mencari kombinasi bobot dan bias setiap neuron dan hubungannya sedemikian sehingga dari \en{input dataset} dihasilkan \en{output} yang paling mendekati \en{target dataset}. Ukuran dekatnya \en{output} dengan \en{target dataset} adalah \en{loss} atau \en{error function}, yaitu fungsi yang menghasilkan selisih antara \en{output} dan \en{target}. Semakin kecil \en{loss}, maka model semakin optimal.

Pada umumnya, pelatihan dilakukan dengan \en{gradient descent}, yaitu proses pencarian gradien yang optimal untuk setiap bobot dan bias pada model terhadap seluruh \en{training dataset} yang ada. Perhitungan \en{gradient descent} secara matematis tidak dijelaskan pada tulisan ini, bagi pembaca yang tertarik dapat merujuk ke \citep{nielsen2015neural}.

\subsection{\en{Recurrent Neural Network}}

\en{Recurrent neural network} (RNN) merupakan pengembangan dari \en{neural network} biasa, di mana pada setiap \en{layer} terdapat bobot yang menghubungkannya ke dirinya sendiri. Lebih tepatnya, bobot tersebut menghubungkan suatu \en{layer} ke dirinya sendiri pada urutan (atau \en{timestep}) sebelumnya (dan sesudahnya). Tujuan dari bobot ini adalah agar model dapat menangkap hubungan data secara temporal.

Gambar ~\ref{fig:RecurrentNeuralNetwork} mengilustrasikan sebuah \en{layer} RNN. Dari gambar tersebut terlihat bahwa selain berhubungan dengan neuron-neuron di \en{layer} sebelumnya, setiap neuron di suatu \en{layer} juga berhubungan dengan dirinya sendiri pada \en{timestep} sebelumnya (dan sesudahnya).

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/recurrent-neural-network.png}
    \caption{Arsitektur sebuah \en{layer} RNN \citep{geron2017hands}.}
    \label{fig:RecurrentNeuralNetwork}
\end{figure}

Karena kemampuannya menangkap hubungan data secara temporal, RNN banyak digunakan untuk data yang bersifat sekuens atau \en{time series}, di mana suatu data dipengaruhi oleh data-data pada urutan atau waktu sebelumnya. Contoh data sekuens atau \en{time series} adalah teks, \en{speech}, harga saham, dan lainnya.

Pada RNN, terdapat istilah \en{sequence} dan \en{vector}. Apabila suatu \en{input/output layer} ber-\en{timestep} 1, maka disebut \en{vector}. Bila \en{timestep} suatu \en{input/output layer} lebih dari 1, disebut \en{sequence}. Dari kombinasi \en{sequence} dan \en{vector} tersebut terbentuk beberapa jenis RNN, yaitu \en{vector to sequence}, \en{sequence to vector}, dan \en{sequence to sequence}. Pemilihan jenis RNN ini tergantung dari data dan masalah yang dihadapi, misalnya \en{sequence to vector} cocok untuk \en{sentiment analysis} (dari sekuens teks menjadi skor sentimen) sedangkan \en{sequence to sequence} tepat untuk translasi bahasa \en{dari teks suatu bahasa ke bahasa lain} \citep{geron2017hands}.

\en{Layer} berjenis \en{sequence} seringkali nilai \en{timestep}-nya tergantung pada panjang \en{input/output}. Misalnya, pada model \en{sequence to sequence} untuk translasi bahasa, nilai \en{timestep} dari \en{input layer} tergantung pada panjang kalimat yang akan ditranslasi, begitu juga nilai \en{timestep} dari \en{output layer} tergantung pada panjang kalimat hasil translasi. Oleh karena itu, model RNN seperti demikian harus dapat secara dinamis mengadaptasikan topologinya sesuai \en{input/output} yang sedang diproses.

\subsection{Proses Pelatihan}

Terdapat berbagai macam metode pelatihan model \en{deep learning}, salah satunya adalah \en{stochastic gradient descent} (SGD). Pada SGD, keseluruhan \en{dataset} dibagi menjadi beberapa \en{mini-batch} (atau \en{batch}). Pada setiap \en{batch}, gradien dihitung kemudian digunakan untuk meng-\en{update} parameter, agar menurunkan \en{loss} atau \en{error}.

Apabila pelatihan telah selesai memproses seluruh \en{batch}, maka disebut telah menyelesaikan satu \en{epoch}. Biasanya, pelatihan terdiri dari banyak \en{epoch}. Tujuan SGD membagi \en{dataset} ke \en{batch}-\en{batch} adalah mempercepat konvergensi dan menghemat penggunaan memori karena pada satu waktu hanya perlu diproses sebagian \en{training data} saja. Namun, karena gradien yang didapat pada suatu \en{batch} hanya berdasarkan sebagian sampel data saja, maka gradien ini sejatinya adalah estimasi dari gradien yang seharusnya (gradien populasi). Hal ini dapat menyebabkan penurunan kinerja model, meski pada praktiknya penurunan kinerja ini dapat saja diterima demi meningkatkan kecepatan pelatihan dan mengurangi penggunaan memori.

\subsection{Penggunaan Memori}

Berbagai penelitian menunjukkan bahwa pada model \en{deep learning}, menambah jumlah neuron dan \en{hidden layer} dapat meningkatkan kinerja model \citep{dean2012large,nielsen2015neural}. Tetapi, meningkatkan kompleksitas model seperti demikian juga meningkatkan \en{training cost}, seperti waktu pelatihan dan ukuran memori yang dibutuhkan. Gambar ~\ref{fig:MemoryVSBatchSize} menunjukkan utilisasi memori beberapa \en{pre-trained model} terhadap ukuran \en{batch}. Berikutnya, akan dibahas beberapa penyebab meningkatnya kebutuhan memori pada model \en{deep learning} seiring meningkatnya kompleksitas model.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/memory-vs-batch-size.png}
    \caption{Penggunaan memori berbagai model \en{deep learning} terhadap ukuran \en{batch} \citep{canziani2016analysis}.}
    \label{fig:MemoryVSBatchSize}
\end{figure}

Selain untuk memuat \en{dataset} yang digunakan, memori juga digunakan untuk variabel yang memuat parameter model (bobot dan bias) dan hasil komputasi seperti fungsi aktivasi. Katakanlah bahwa variabel direpresentasikan oleh 32-bit \en{floating point}, maka ukuran memori yang digunakan adalah 4 \en{byte} dikali dengan jumlah variabel. Sebagai contoh, model ResNet50 memiliki sekitar 26 juta parameter dan 16 juta fungsi aktivasi pada sekali \en{forward pass}. Maka, ukuran memori yang digunakan pada satu sampel sekitar \(\num{26e6} \times \SI{4}{byte} = \SI{104}{\mega\byte}\) untuk parameter dan \(\num{16e6} \times \SI{4}{byte} = \SI{64}{\mega\byte}\) untuk fungsi aktivasi \citep{hanlon2016why}.

Ukuran tersebut barulah ukuran memori yang digunakan oleh satu sampel pada \en{batch}. Seperti dijelaskan sebelumnya, pelatihan model dengan SGD meng-\en{update} parameter model (dengan gradien yang didapat dari fungsi aktivasi setiap neuron) sekaligus per \en{batch}. Oleh karena itu, untuk setiap \en{batch} perlu dimuat seluruh fungsi aktivasi yang didapat dari setiap sampel pada \en{batch}. Misalnya \en{batch} berukuran 16, maka diperlukan \(16 * \SI{64}{\mega\byte} = \SI{1024}{\mega\byte}\) untuk memuat seluruh hasil fungsi aktivasi pada satu \en{batch}. Bila dijumlah dengan ukuran parameter tadi, maka diperlukan sekitar \(\SI{104}{\mega\byte} + \SI{1024}{\mega\byte} = \SI{1.128}{\giga\byte}\) memori pada pelatihan model ResNet50 untuk \en{batch} berukuran 16. Perhitungan ini didukung oleh Gambar ~\ref{fig:MemoryVSBatchSize} di mana pada \({batch\ size} = 16\), penggunaan memori ResNet50 (merah muda) sekitar \(\SI{1000}{\mega\byte}\).

Dari hasil perhitungan tersebut, ditunjukkan bahwa penggunaan terbesar memori terletak pada pemuatan hasil fungsi aktivasi yang dibutuhkan untuk mencari gradien pada suatu \en{batch}. Selain itu, ditunjukkan juga bagaimana penggunaan memori meningkat seiring peningkatan ukuran \en{batch}.

\section{TensorFlow}

TensorFlow adalah sistem pembelajaran mesin yang didesain untuk menangani pelatihan model berskala besar dan secara terdistribusi, baik untuk kebutuhan penelitian maupun industri. Dikembangkan oleh tim Google Brain dan dirilis secara \en{open source}, TensorFlow telah digunakan secara ekstensif pada berbagai proyek pembelajaran mesin, baik di dalam maupun di luar Google.

Pada dasarnya, TensorFlow adalah \en{framework} yang menyediakan operasi-operasi komputasi matematika (seperti aljabar linear dan kalkulus), dengan operasi-operasi tambahan yang memudahkan pendefinisian proses pembelajaran mesin. Dengan TensorFlow, eksekusi operasi-operasi tersebut dapat dilakukan secara terdistribusi menggunakan berbagai \en{device} seperti CPU, GPU, maupun TPU (Tensor Processing Units) yang terdapat pada satu atau lebih \en{machine} yang terletak pada satu atau lebih \en{cluster}. Selain itu, TensorFlow juga berjalan di berbagai macam platform, mulai dari \en{data center} hingga \en{smartphone} \citep{abadi2016tensorflow}.

\subsection{Komputasi}

Seluruh operasi dan data pada program TensorFlow beserta hubungannya membentuk sebuah \en{dataflow graph}. Operasi merupakan \en{vertice} dari graf tersebut. Sedangkan data yang pada TensorFlow direpresentasikan sebagai \en{tensor} (vektor multi-dimensional) mengalir (\en{flowing}) melalui \en{edge}-\en{edge} dari graf.

Salah satu tujuan representasi program dengan \en{dataflow graph} adalah memudahkan komputasi program secara terdistribusi. Seperti diilustrasikan oleh Gambar ~\ref{fig:TensorFlowGraph}, graf dapat dibagi-bagi menjadi beberapa subgraf, di mana setiap subgraf dieksekusi oleh \en{device} yang berbeda-beda. Dengan membagi menjadi beberapa subgraf, sebuah \en{device} baru perlu berkomunikasi dengan \en{device} lainnya apabila masuk \en{vertice} yang memiliki \en{dependency} ke \en{vertice} di \en{device} lain.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/tensorflow-graph.png}
    \caption{\en{Dataflow graph} dari program TensorFlow \citep{geron2017hands}.}
    \label{fig:TensorFlowGraph}
\end{figure}

\subsection{Arsitektur}

Gambar ~\ref{fig:TensorFlowArch} menunjukkan arsitektur umum TensorFlow. Berikut ini akan dijelaskan beberapa bagian dari arsitektur tersebut.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.50\linewidth]{figures/tensorflow-arch.png}
    \caption{Arsitektur umum TensorFlow \citep{tensorflow2018architecture}.}
    \label{fig:TensorFlowArch}
\end{figure}

\textbf{Client}. Menerima pendefinisian program TensorFlow dari \en{user}, seperti \en{tensor}-\en{tensor} yang ada dan operasi-operasi yang dijalankan. Kemudian \en{client} mengkonstruksi \en{dataflow graph} yang sesuai, lalu mengeksekusi graf tersebut menggunakan \en{session}. \en{Client} tersedia dalam beberapa bahasa, seperti Python, Go, dan C++.

\textbf{Distributed Master.} Merupakan koordinator dari keseluruhan program TensorFlow. Setelah menerima \en{dataflow graph} dari \en{client}, \en{distributed master} mempartisi graf menjadi beberapa subgraf, kemudian mendistribusikannya ke \en{worker services}.

\textbf{Worker Services.} Menerima \en{request} dari \en{master}, menjalankannya pada \en{devices} di bawahnya, kemudian mengirim hasilnya kembali ke \en{master}. \en{Worker service} berelasi satu-satu dengan \en{task}. \en{Task} pada TensorFlow merujuk pada suatu proses TensorFlow, di mana suatu \en{machine} dapat memiliki satu atau lebih \en{task}. Bahkan, suatu \en{device} dari suatu \en{machine} dapat dibagi-bagi ke beberapa \en{task}.

\textbf{Kernel Implementations.} Merupakan implementasi operasi-operasi TensorFlow yang \en{device-specific}. Tanpa \en{kernel implementation} yang sesuai, \en{backend} TensorFlow tidak dapat berjalan pada suatu \en{device}.

Gambar ~\ref{fig:TensorFlowDist} mengilustrasikan hubungan antar bagian TensorFlow tersebut dalam pelatihan terdistribusi.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/tensorflow-dist.png}
    \caption{Hubungan bagian-bagian terdistribusi di TensorFlow \citep{tensorflow2018architecture}.}
    \label{fig:TensorFlowDist}
\end{figure}

\subsection{Penambahan Operasi}

TensorFlow mendukung penambahan jenis operasi baru, apabila jenis-jenis operasi yang disediakan belum memenuhi kebutuhan pengguna, atau mengoptimisasi operasi yang ada secara \en{native}. Untuk membuat operasi baru, pengguna perlu menambahkan definisi (seperti \en{input} dan \en{output}, bentuk \en{tensor}) dan implementasi operasi tersebut dalam bahasa C++. Selanjutnya, pengguna dapat menambahkan API ke operasi tersebut pada \en{client}, namun langkah ini bersifat opsional.

\section{Graphics Processing Unit}

Penggunaan GPU pada pelatihan \en{deep learning} merupakan salah satu faktor yang memajukan perkembangan bidang ini secara signifikan \citep{dean2012large}. Hal ini dikarenakan GPU mampu meningkatkan kecepatan dengan menjalankan pelatihan secara paralel. Sayangnya, ukuran memori GPU lebih terbatas, yakni 8-10 kali lebih kecil dari memori CPU. Sebagai gambaran, pada saat penulisan tulisan ini, \en{high-end} GPU rata-rata memiliki 12-16GB memori, sedangkan memori CPU dapat mencapai 128GB \citep{meng2017training}.

Padahal, kekurangan memori dapat menyebabkan kegagalan pelatihan model (\en{out of memory}). Solusi yang biasa diterapkan adalah mengurangi ukuran \en{dataset}, ukuran \en{batch}, atau jumlah parameter model, meskipun hal ini dapat menurunkan kinerja model \citep{meng2017training}.

Selain solusi-solusi tersebut, terdapat juga metode \en{memory swapping}, yaitu memindahkan sebagian data dari memori GPU ke CPU saat pelatihan. Akan tetapi, \en{memory swapping} yang terlalu sering dapat menurunkan kecepatan pelatihan secara signifikan karena adanya \en{data transfer overhead} \citep{dean2012large}.

TensorFlow sendiri mendukung penggunaan GPU untuk menjalankan programnya, sehingga operasi-operasi yang disediakan oleh TensorFlow dapat dijalankan pada GPU (dengan beberapa pengecualian). Untuk dukungan terhadap GPU Nvidia, operasi-operasi tersebut diimplementasi dengan menggunakan CUDA.

\subsection{CUDA}

CUDA merupakan platform komputasi paralel \en{general purpose} untuk GPU Nvidia. CUDA memberikan abstraksi dan model pemrograman bagi \en{programmer} untuk menggunakan kemampuan komputasi paralel di GPU Nvidia \citep{nvidia2010programming}. Program CUDA berupa kode (disebut \en{kernel}) yang disisipkan ke dalam bahasa C (berekstensi ".cu").

Arsitektur CUDA dapat dilihat sebagai sebuah \en{grid} yang terdiri dari \en{blocks}, di mana \en{block} terdiri dari \en{threads}, seperti diilustrasikan oleh Gambar ~\ref{fig:CUDAArchitecture}. Jumlah \en{block} dan \en{thread} tidak konstan, melainkan ditentukan oleh \en{programmer}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/cuda-architecture.png}
    \caption{Arsitektur CUDA \citep{nvidia2010programming}.}
    \label{fig:CUDAArchitecture}
\end{figure}

Selain \en{block} dan \en{thread}, CUDA juga mengabstraksikan memori GPU. Pada CUDA, memori GPU terbagi menjadi beberapa jenis. Beberapa di antaranya adalah \en{global memory}, \en{shared memory}, dan \en{local memory}. \en{Global memory} adalah memori yang dapat diakses oleh \en{thread} di semua \en{block}, \en{shared memory} hanya dapat diakses oleh \en{thread} di suatu \en{block}, dan \en{local memory} terbatas untuk masing-masing \en{thread} saja \citep{wilt2013cuda}.

Pada CUDA, CPU disebut \en{host} sehingga memori CPU disebut \en{host memory} dan GPU disebut \en{device} sehingga memori GPU disebut \en{device memory}. Pada umumnya, kode yang dijalankan di \en{host} tidak dapat langsung mengakses \en{device memory} dan begitu juga sebaliknya. Untuk itu, perlu dilakukan transfer data dari \en{device} ke \en{host} atau sebaliknya. Penjelasan lebih lanjut mengenai transfer data ini akan dibahas pada bagian selanjutnya.

Sejak CUDA 6 terdapat fitur \en{unified memory}, yaitu abstraksi memori yang dapat diakses secara langsung baik oleh \en{host} maupun \en{device}. Di balik layar, tetap dilakukan transfer data antar \en{host} dan \en{device}, namun secara otomatis dilakukan oleh kernel CUDA, sehingga memudahkan \en{programmer} meskipun cenderung mengakibatkan kinerja yang lebih buruk. Hingga versi 1.12, dukungan TensorFlow terhadap \en{unified memory} masih eksperimental sehingga pada tulisan ini \en{unified memory} diabaikan.

% TBD. Penjelasan transfer data host <-> device.

\section{Penelitian Terkait}

"Training deeper models by GPU memory optimization on TensorFlow" \citep{meng2017training} merupakan penelitian untuk mengoptimisasi penggunaan memori GPU pada pelatihan model \en{deep learning} dengan TensorFlow \citep{meng2017training}. Penelitian tersebut terbatas pada sebuah komputer dengan satu GPU. Ide dari \en{paper} tersebut adalah menyimpan sebagian model dari memori GPU ke memori CPU saat pelatihan untuk mengurangi beban memori GPU.

Optimisasi dilakukan dengan mengimplementasikan operasi baru yang disebut (\textbf{swap out/in}) untuk men-\en{swap} \en{tensor} dari memori GPU ke CPU. Dengan operasi baru ini, graf TensorFlow yang sudah ada dimodifikasi sehingga operasi disisipkan ke hubungan \en{vertice} yang mengalirkan tensor yang di-\en{swap}. Operasi \textbf{swap out/in} ini diilustrasikan oleh Gambar ~\ref{fig:SwapOutIn}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/swap-out-in.png}
    \caption{Graf yang dimodifikasi dengan menyisipkan operasi \textbf{swap out/in} \citep{meng2017training}.}
    \label{fig:SwapOutIn}
\end{figure}

Agar dapat mengoptimisasi dengan baik, perlu dipertimbangkan \en{tensor} mana yang perlu di-\en{swap}. Apabila semua \en{tensor} di-\en{swap}, akan menyebabkan penurunan kinerja model karena \en{data transfer overhead}. Maka dari itu, \en{tensor} terbaik untuk di-\en{swap} adalah yang interval hingga diperlukannya kembali cukup lama.

Eksperimen pada \en{paper} dilakukan dengan beberapa \en{pre-trained model} seperti ResNet, Inception, dan NMT, pada sebuah komputer dengan satu GPU (Tesla M40) dengan kapasitas memori 12GB. Menurut hasil eksperimen yang ditunjukkan Gambar ~\ref{fig:SwapOutInResult} tersebut, didapatkan bahwa pada model-model yang diuji, ukuran \en{batch} dapat ditingkatkan hingga 3 kali, sedangkan penggunaan memori maksimum menurun hingga 2-3 kali lipat. Bahkan, model-model seperti ResNet-1001 dan ResNet-2000 yang sebelumnya tidak dapat dilatih karena \en{out of memory} (OOM), menjadi dapat dilatih. Di sisi lain, \en{training speed} mengalami penurunan kurang dari 10\%, karena adanya \en{data transfer overhead}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/swap-out-in-result.png}
    \caption{Perbandingan penggunaan memori dengan \textbf{swap out/in} \citep{meng2017training}.}
    \label{fig:SwapOutInResult}
\end{figure}
