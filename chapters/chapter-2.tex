\chapter{Studi Literatur}

\section{\en{Deep Learning}}

\en{Deep learning} adalah suatu bagian dari bidang \en{machine learning} di mana pembelajaran dilakukan pada beberapa tingkatan dengan tujuan memodelkan hubungan yang kompleks antar data \citep{deng2014deep}. Biasanya, \en{deep learning} menggunakan \en{neural network} dengan banyak \en{hidden layer} (disebut juga \en{deep neural network}). Berbagai arsitektur \en{deep neural network} seperti \en{convolutional neural network} (CNN) dan \en{recurrent neural network} (RNN) telah banyak diaplikasikan pada berbagai bidang AI, seperti penglihatan komputer dan pengenalan ucapan.

Ide dasar \en{deep neural network} adalah memodelkan fitur-fitur dari data beserta relasinya dengan banyak \en{layer} yang saling berhubungan. Untuk memberikan gambaran, Gambar~\ref{fig:DeepNeuralNetwork} mengilustrasikan \en{deep neural network} yang memiliki 3 \en{hidden layer}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/deep-neural-network.png}
    \caption{Arsitektur \en{deep neural network} \citep{nielsen2015neural}.}
    \label{fig:DeepNeuralNetwork}
\end{figure}

Seperti \en{neural network} pada \en{shallow learning}, setiap hubungan neuron antar \en{layer} memiliki bobot (\en{weight}) yang nilainya merepresentasikan signifikansi hubungan antar neuron tersebut. Selain bobot, terdapat juga bias yang secara langsung mempengaruhi nilai setiap neuron (tidak seperti bobot yang mempengaruhi hubungan antara neuron), dengan nilainya merepresentasikan signifikasi suatu neuron. Secara garis besar, tujuan setiap neuron dan hubungan antar neuron adalah mengenali pola atau fitur dari data sehingga dapat memprediksi \en{output} dengan benar \citep{nielsen2015neural}.

Pelatihan \en{neural network} itu sendiri adalah mencari kombinasi bobot dan bias setiap neuron dan hubungannya sedemikian sehingga dari input dihasilkan \en{output} yang paling mendekati target. Ukuran dekatnya \en{output} dengan target adalah \en{loss} atau \en{cost function}, yaitu fungsi yang menghasilkan selisih antara \en{output} dan target. Semakin kecil \en{loss}, maka model semakin optimal.

Pada umumnya, pelatihan dilakukan dengan \en{gradient descent}, yaitu proses pencarian gradien yang optimal untuk setiap bobot dan bias dari model terhadap data yang ada. Gradien ini kemudian digunakan untuk meng-\en{update} bobot dan bias dari model. Pencarian gradien untuk parameter-parameter ini biasanya menggunakan teknik \en{backpropagation}. Pada bagian berikutnya akan dijelaskan secara singkat mengenai \en{backpropagation} berdasarkan \citep{guo2013backpropagation} dan \citep{michalski2013machine}.

\subsection{\en{Backpropagation}} \label{backpropagation}

Pada pelatihan \en{deep neural network}, pertama dilakukan \en{forward pass} dengan input, kemudian didapatkan \en{output}-nya. Dari \en{output} ini kemudian dihitung selisihnya dari target dengan \en{loss} atau \en{cost function}. Salah satu \en{cost function} yang umum dipakai adalah SSE (\en{summed squared error}) seperti berikut.

\begin{equation}
    C = \frac{1}{2}\sum_{k}(t_{k} - o_{k})^{2}
    \label{eq:loss}
\end{equation}

di mana $C$ adalah nilai \en{cost function}, $t$ adalah nilai target, $o$ adalah nilai \en{output}, dan $k$ adalah indeks neuron pada \en{output layer}.

Seperti disebutkan sebelumnya, pada \en{gradient descent} dicari gradien masing-masing bobot dan bias yang kemudian digunakan untuk meng-\en{update} parameter bersangkutan. Gradien dicari dengan mencari turunan $C$ terhadap masing-masing bobot ($w$) dan bias ($b$). Untuk mempersingkat, hanya akan dijelaskan pencarian gradien terhadap bobot, namun pencarian gradien terhadap bias serupa.

Berikut adalah gradien terhadap bobot $w_{kj}$, yaitu bobot yang berasal dari sebuah neuron (berindeks $j$) pada sebuah \en{layer} menuju neuron (berindeks $k$) pada \en{layer} selanjutnya.

\begin{equation}
    \frac{\partial{C}}{\partial{w_{kj}}} = \frac{\partial{C}}{\partial{net_{k}}} \frac{\partial{net_{k}}}{\partial{w_{kj}}}
\end{equation}

di mana $net_{k} = \sum_{j} w_{kj} o_{j} + b_{k}$ dengan $o_{j}$ adalah \en{output} dari neuron ke-$j$ di \en{layer} sebelumnya. Cukup jelas bahwa $\frac{\partial{net_{k}}}{\partial{w_{kj}}} = o_{j}$, sehingga yang menjadi perhatian berikutnya adalah $\frac{\partial{C}}{\partial{net_{k}}}$ yaitu \en{error} pada neuron berindeks $k$, dinotasikan dengan simbol $\delta_{k}$ \citep{guo2013backpropagation}.

Dengan menjabarkan $\delta_{k}$ menggunakan \en{chain rule}, didapat persamaan berikut.

\begin{equation}
    \delta_{k} = \frac{\partial{C}}{\partial{net_{k}}} = \frac{\partial{C}}{\partial{o_{k}}} \frac{\partial{o_{k}}}{\partial{net_{k}}} = (t_{k} - o_{k}) a'(net_{k})
    \label{eq:delta}
\end{equation}

di mana $\frac{\partial{C}}{\partial{o_{k}}} = (t_{k} - o_{k})$ merupakan penurunan persamaan (\ref{eq:loss}) terhadap $o_{k}$. Selanjutnya, karena $o_{k} = a(net_{k})$, di mana $a$ adalah fungsi aktivasi neuron (misal tanh atau ReLU), maka $\frac{\partial{o_{k}}}{\partial{net_{k}}}$ adalah turunan $a$ terhadap $net_{k}$, yaitu $a'(net_{k})$ seperti ditunjukkan persamaan (\ref{eq:delta}) di atas.

Demikian didapatkan \en{error} untuk parameter di \en{output layer}. Untuk \en{error} di \en{layer}-\en{layer} sebelumnya, dipropagasikan \en{error} dari \en{layer} sesudahnya dengan \en{chain rule} seperti berikut.

\begin{equation}
    \delta_{j}
    = \sum_{k} \frac{\partial{C}}{\partial{net_{k}}} \frac{\partial{net_{k}}}{\partial{o_{j}}} \frac{\partial{o_{j}}}{\partial{net_{j}}}
    = \sum_{k} \delta_{k} \frac{\partial{net_{k}}}{\partial{o_{j}}} \frac{\partial{o_{j}}}{\partial{net_{j}}}
    = \sum_{k} \delta_{k} w_{kj} a'(net_{j})
\end{equation}

Dapat diperhatikan bahwa untuk mendapatkan $\delta_{j}$ diperlukan $\delta_{k}$ untuk semua $k$, yaitu indeks neuron-neuron pada \en{layer} sesudahnya. Demikianlah mengapa teknik ini disebut \en{backpropagation} karena pencarian gradien yang dilakukan dengan mempropagasikan \en{error} dari \en{layer} belakang ke depan. Seperti disebutkan sebelumnya, gradien yang didapat kemudian digunakan untuk meng-\en{update} parameter bersangkutan.

\subsection{\en{Recurrent Neural Network}}

\en{Recurrent neural network} (RNN) merupakan pengembangan dari \en{deep neural network} biasa, di mana pada setiap \en{layer} terdapat bobot yang menghubungkannya ke dirinya sendiri. Lebih tepatnya, bobot tersebut menghubungkan suatu \en{layer} ke dirinya sendiri pada urutan (atau \en{timestep}) sebelumnya (dan sesudahnya). Tujuan dari bobot rekuren ini adalah agar model dapat menangkap hubungan data secara temporal.

Gambar~\ref{fig:RecurrentNeuralNetwork} mengilustrasikan sebuah \en{layer} RNN. Dari gambar tersebut terlihat bahwa selain berhubungan dengan neuron-neuron di \en{layer} sebelumnya, setiap neuron di suatu \en{layer} juga berhubungan dengan dirinya sendiri pada \en{timestep} sebelumnya (dan sesudahnya). Perlu diperhatikan bahwa untuk suatu \en{layer}, bobot dan bias yang sama digunakan pada seluruh \en{timestep}. Bobot $U$ menghubungkan \en{hidden layer} dengan input, bobot $V$ menghubungkan \en{hidden layer} dengan \en{output}, dan bobot $W$ menghubungkan \en{hidden layer} pada \en{timestep} $t$ dengan \en{hidden layer} pada \en{timestep} $t-1$ dan $t+1$.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/recurrent-neural-network.jpg}
    \caption{Arsitektur sebuah \en{layer} RNN \citep{lecun2015deep}.}
    \label{fig:RecurrentNeuralNetwork}
\end{figure}

Karena kemampuannya menangkap hubungan data secara temporal, RNN banyak digunakan untuk data yang bersifat sekuens atau \en{time series}, di mana suatu data dipengaruhi oleh data-data pada urutan atau waktu sebelumnya. Contoh data sekuens atau \en{time series} adalah teks, \en{speech}, harga saham, dan lainnya.

Pada RNN, terdapat istilah \en{sequence} dan \en{vector}. Apabila suatu \en{input/output layer} ber-\en{timestep} 1, maka disebut \en{vector}. Bila \en{timestep} suatu \en{input/output layer} lebih dari 1, disebut \en{sequence}. Dari kombinasi \en{sequence} dan \en{vector} tersebut terbentuk beberapa jenis RNN, yaitu \en{vector to sequence}, \en{sequence to vector}, dan \en{sequence to sequence}. Pemilihan jenis RNN ini tergantung dari data dan masalah yang dihadapi, misalnya \en{sequence to vector} cocok untuk \en{sentiment analysis} (dari sekuens teks menjadi skor sentimen) sedangkan \en{sequence to sequence} tepat untuk translasi bahasa \en{dari teks suatu bahasa ke bahasa lain} \citep{geron2017hands}. Sebagai contoh, Gambar~\ref{fig:RecurrentNeuralNetwork} menunjukkan \en{sequence to sequence} RNN.

\en{Layer} berjenis \en{sequence} seringkali nilai \en{timestep}-nya tergantung pada panjang \en{input/output}. Misalnya, pada model \en{sequence to sequence} untuk translasi bahasa, nilai \en{timestep} dari \en{input layer} tergantung pada panjang kalimat yang akan ditranslasi, begitu juga nilai \en{timestep} dari \en{output layer} tergantung pada panjang kalimat hasil translasi. Oleh karena itu, model RNN yang seperti demikian harus dapat secara dinamis mengadaptasikan topologinya sesuai panjang \en{input/output} yang sedang diproses.

\subsubsection{\en{Backpropagation Through Time}} \label{bptt}

Seperti \en{neural network} biasa, pelatihan RNN pada umumnya menggunakan \en{gradient descent} dengan \en{backpropagation} seperti pada Bagian~\ref{backpropagation}. Hanya saja, untuk setiap \en{layer} terdapat 3 jenis bobot dengan perhitungan gradien yang berbeda. Selain itu, terdapat faktor \en{timestep} yang harus diperhitungkan. Pada Gambar~\ref{fig:RecurrentNeuralNetwork}, terdapat bobot $u$, $v$, dan $w$. Berikut ditunjukkan perhitungan gradien untuk ketiga bobot tersebut.

Bobot $v$ hanya mempengaruhi \en{output layer}, sehingga perhitungan gradiennya relatif sederhana. Berikut perhitungan gradien bobot $v_{kj}$ (bobot yang menghubungkan neuron berindeks $j$ pada \en{hidden layer} ke neuron berindeks $k$ pada \en{output layer}) pada \en{timestep} $t$.

\begin{equation}
    \frac{\partial{C_{t}}}{\partial{v_{kj}}}
    = \frac{\partial{C_{t}}}{\partial{net_{t_k}}} \frac{\partial{net_{t_k}}}{\partial{v_{kj}}}
    = \frac{\partial{C_{t}}}{\partial{net_{t_k}}} s_{t_j}
\end{equation}

dengan $net_{t_k} = \sum_{j} v_{kj} s_{t_j} + b_{k}$, di mana $s_{t_j}$ adalah \en{output} neuron ke-$j$ di \en{hidden layer} (di RNN disebut \en{hidden state}) pada \en{timestep} $t$.

Karena bobot $w$ dipengaruhi oleh \en{hidden state} seluruh \en{timestep}, maka pencarian gradien perlu memperhitungkan faktor tersebut. Berikut adalah perhitungan gradien bobot $w_{ji}$ (bobot yang menghubungkan neuron berindeks $i$ pada suatu \en{layer} ke neuron berindeks $j$ pada \en{layer} sesudahnya) pada \en{timestep} $t$ \citep{guo2013backpropagation}.

\begin{equation}
    \frac{\partial{C_{t}}}{\partial{w_{ji}}}
    = \sum_{k} \frac{\partial{C_{t}}}{\partial{net_{t_k}}} \frac{\partial{net_{t_k}}}{\partial{s_{t_j}}} \frac{\partial{s_{t_j}}}{\partial{w_{ji}}}
    \label{eq:W}
\end{equation}

$w_{ji}$ menghubungkan neuron berindeks $i$ dengan neuron berindeks $j$ pada \en{layer} sesudahnya bukan hanya pada satu \en{timestep} saja, melainkan di seluruh \en{timestep} (seperti disebutkan sebelumnya, bobot yang sama digunakan untuk seluruh \en{timestep}). Maka dari itu, terdapat hubungan dengan seluruh neuron (berindeks $l$) pada seluruh \en{timestep} (berindeks $t - \tau$). Hubungan ini memberikan lanjutan persamaan (\ref{eq:W}) berikut.

\begin{equation}
    \frac{\partial{C_{t}}}{\partial{w_{ji}}}
    = \sum_{k} (\frac{\partial{C_{t}}}{\partial{net_{t_k}}} \frac{\partial{net_{t_k}}}{\partial{s_{t_j}}} \sum_{\tau = 0}^{t} \sum_{l} \frac{\partial{s_{t_j}}}{s_{(t-\tau)_l}} \frac{s_{(t-\tau)_l}}{\partial{w_{ji}}})
    \label{eq:W}
\end{equation}

Perhitungan gradien $u$ tidak jauh berbeda dengan $w$ karena dipengaruhi oleh \en{hidden state} seluruh \en{timestep} juga.

\begin{equation}
    \frac{\partial{C_{t}}}{\partial{u_{ji}}}
    = \sum_{k} (\frac{\partial{C_{t}}}{\partial{net_{t_k}}} \frac{\partial{net_{t_k}}}{\partial{s_{t_j}}} \sum_{\tau = 0}^{t} \sum_{l} \frac{\partial{s_{t_j}}}{s_{(t-\tau)_l}} \frac{s_{(t-\tau)_l}}{\partial{u_{ji}}})
    \label{eq:v}
\end{equation}
\subsection{Proses Pelatihan}

Terdapat berbagai macam metode pelatihan model \en{deep learning}, salah satunya adalah \en{mini-batch gradient descent} (MBGD). Pada MBGD, keseluruhan \en{dataset} dibagi menjadi beberapa \en{mini-batch} (atau \en{batch}). Pada setiap \en{batch}, gradien dihitung kemudian digunakan untuk meng-\en{update} parameter, agar menurunkan \en{loss} atau \en{cost}.

Apabila pelatihan telah selesai memproses seluruh \en{batch}, maka disebut telah menyelesaikan satu \en{epoch}. Biasanya, pelatihan terdiri dari banyak \en{epoch}. Tujuan MBGD membagi \en{dataset} ke \en{batch}-\en{batch} adalah mempercepat konvergensi dan menghemat penggunaan memori karena pada satu waktu hanya perlu diproses sebagian \en{training data} saja. Namun, karena gradien yang didapat pada suatu \en{batch} hanya berdasarkan sebagian sampel data saja, maka gradien ini sejatinya adalah estimasi dari gradien yang seharusnya (gradien populasi). Hal ini dapat menyebabkan penurunan kinerja model, meski pada praktiknya penurunan kinerja ini dapat saja diterima demi meningkatkan kecepatan pelatihan dan mengurangi penggunaan memori.

\section{TensorFlow} \label{tensorflow}

TensorFlow adalah \en{framework} pembelajaran mesin yang didesain untuk menangani pelatihan model berskala besar dan secara terdistribusi, baik untuk kebutuhan penelitian maupun industri. Dikembangkan oleh tim Google Brain dan dirilis secara \en{open source}, TensorFlow telah digunakan secara ekstensif pada berbagai proyek pembelajaran mesin, baik di dalam maupun di luar Google.

Pada dasarnya, TensorFlow adalah \en{framework} yang menyediakan operasi-operasi komputasi matematika (seperti aljabar linear dan kalkulus), dengan operasi-operasi tambahan yang memudahkan pendefinisian proses pembelajaran mesin. Dengan TensorFlow, eksekusi operasi-operasi tersebut dapat dilakukan secara terdistribusi menggunakan berbagai \en{device} seperti CPU, GPU, maupun TPU (Tensor Processing Units) yang terdapat pada satu atau lebih \en{machine} yang terletak pada satu atau lebih \en{cluster}. Selain itu, TensorFlow juga berjalan di berbagai macam platform, mulai dari \en{data center} hingga \en{smartphone} \citep{abadi2016tensorflow}.

\subsection{Komputasi}

Seluruh operasi dan data pada program TensorFlow beserta hubungannya membentuk sebuah \en{dataflow graph}. Operasi merupakan \en{vertice} dari graf tersebut. Sedangkan data yang pada TensorFlow direpresentasikan sebagai \en{tensor} (vektor multi-dimensional) mengalir (\en{flowing}) melalui \en{edge}-\en{edge} dari graf.

Salah satu tujuan representasi program dengan \en{dataflow graph} adalah memudahkan komputasi program secara terdistribusi. Seperti diilustrasikan oleh Gambar~\ref{fig:TensorFlowGraph}, graf dapat dibagi-bagi menjadi beberapa subgraf, di mana setiap subgraf dieksekusi oleh \en{device} yang berbeda-beda. Dengan membagi menjadi beberapa subgraf, sebuah \en{device} baru perlu berkomunikasi dengan \en{device} lainnya apabila masuk \en{vertice} yang memiliki \en{dependency} ke \en{vertice} di \en{device} lain.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/tensorflow-graph.png}
    \caption{\en{Dataflow graph} dari program TensorFlow \citep{geron2017hands}.}
    \label{fig:TensorFlowGraph}
\end{figure}

\subsection{Arsitektur}

Gambar~\ref{fig:TensorFlowArch} menunjukkan arsitektur umum TensorFlow. Berikut ini akan dijelaskan beberapa bagian dari arsitektur tersebut.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.50\linewidth]{figures/tensorflow-arch.png}
    \caption{Arsitektur umum TensorFlow \citep{tensorflow2018architecture}.}
    \label{fig:TensorFlowArch}
\end{figure}

\textbf{Client}. Menerima pendefinisian program TensorFlow dari \en{user}, seperti \en{tensor}-\en{tensor} yang ada dan operasi-operasi yang dijalankan. Kemudian \en{client} mengkonstruksi \en{dataflow graph} yang sesuai, lalu mengeksekusi graf tersebut menggunakan \en{session}. \en{Client} tersedia dalam beberapa bahasa, seperti Python, Go, dan C++.

\textbf{Distributed Master.} Merupakan koordinator dari keseluruhan program TensorFlow. Setelah menerima \en{dataflow graph} dari \en{client}, \en{distributed master} mempartisi graf menjadi beberapa subgraf, kemudian mendistribusikannya ke \en{worker services}.

\textbf{Worker Services.} Menerima \en{request} dari \en{master}, menjalankannya pada \en{devices} di bawahnya, kemudian mengirim hasilnya kembali ke \en{master}. \en{Worker service} berelasi satu-satu dengan \en{task}. \en{Task} pada TensorFlow merujuk pada suatu proses TensorFlow, di mana suatu \en{machine} dapat memiliki satu atau lebih \en{task}. Bahkan, suatu \en{device} dari suatu \en{machine} dapat dibagi-bagi ke beberapa \en{task}.

\textbf{Kernel Implementations.} Merupakan implementasi operasi-operasi TensorFlow yang \en{device-specific}. Tanpa \en{kernel implementation} yang sesuai, \en{backend} TensorFlow tidak dapat berjalan pada suatu \en{device}.

Gambar~\ref{fig:TensorFlowDist} mengilustrasikan hubungan antar bagian TensorFlow tersebut dalam pelatihan terdistribusi.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/tensorflow-dist.png}
    \caption{Hubungan bagian-bagian terdistribusi di TensorFlow \citep{tensorflow2018architecture}.}
    \label{fig:TensorFlowDist}
\end{figure}

\subsection{Penambahan Operasi}

TensorFlow mendukung penambahan jenis operasi baru, apabila jenis-jenis operasi yang disediakan belum memenuhi kebutuhan pengguna, atau mengoptimisasi operasi yang ada secara \en{native}. Untuk membuat operasi baru, pengguna perlu menambahkan definisi (seperti input dan \en{output}, bentuk \en{tensor}) dan implementasi operasi tersebut dalam bahasa C++. Selanjutnya, pengguna dapat menambahkan API ke operasi tersebut pada \en{client}, namun langkah ini bersifat opsional.

\section{\en{Graphics Processing Unit}}

Penggunaan GPU pada pelatihan \en{deep learning} merupakan salah satu faktor yang memajukan perkembangan bidang ini secara signifikan \citep{dean2012large}. Hal ini dikarenakan GPU mampu meningkatkan kecepatan dengan menjalankan pelatihan secara paralel. Sayangnya, ukuran memori GPU lebih terbatas  dari memori CPU. Umumnya, memori GPU berukuran 8-16GB sedangkan memori CPU dapat mencapai 1TB.

Padahal, kekurangan memori dapat menyebabkan kegagalan pelatihan model (\en{out of memory}). Solusi yang biasa diterapkan adalah mengurangi ukuran \en{dataset}, ukuran \en{batch}, atau jumlah parameter model, meskipun hal ini dapat menurunkan kinerja model \citep{meng2017training}.

Selain solusi-solusi tersebut, terdapat juga metode \en{memory swapping}, yaitu memindahkan sebagian data dari memori GPU ke CPU saat pelatihan. Akan tetapi, \en{memory swapping} yang terlalu sering dapat menurunkan kecepatan pelatihan secara signifikan karena adanya \en{data transfer overhead} \citep{dean2012large}.

TensorFlow sendiri mendukung penggunaan GPU untuk menjalankan programnya, sehingga operasi-operasi yang disediakan oleh TensorFlow dapat dijalankan pada GPU (dengan beberapa pengecualian). Untuk dukungan terhadap GPU Nvidia, operasi-operasi tersebut diimplementasi dengan menggunakan CUDA.

\subsection{CUDA}

CUDA merupakan platform komputasi paralel \en{general purpose} untuk GPU Nvidia. CUDA memberikan abstraksi dan model pemrograman bagi \en{programmer} untuk menggunakan kemampuan komputasi paralel di GPU Nvidia \citep{nvidia2011cuda}. Program CUDA berupa kode (disebut \en{kernel}) yang disisipkan ke dalam bahasa C (berekstensi ".cu").

Arsitektur CUDA dapat dilihat sebagai sebuah \en{grid} yang terdiri dari \en{blocks}, di mana \en{block} terdiri dari \en{threads}, seperti diilustrasikan oleh Gambar~\ref{fig:CUDAArchitecture}. Jumlah \en{block} dan \en{thread} tidak konstan, melainkan ditentukan oleh \en{programmer}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.5\linewidth]{figures/cuda-architecture.png}
    \caption{Arsitektur CUDA \citep{nvidia2011cuda}.}
    \label{fig:CUDAArchitecture}
\end{figure}

Selain \en{block} dan \en{thread}, CUDA juga mengabstraksikan memori GPU. Pada CUDA, memori GPU terbagi menjadi beberapa jenis. Beberapa di antaranya adalah \en{global memory}, \en{shared memory}, dan \en{local memory}. \en{Global memory} adalah memori yang dapat diakses oleh \en{thread} di semua \en{block}, \en{shared memory} hanya dapat diakses oleh \en{thread} di suatu \en{block}, dan \en{local memory} terbatas untuk masing-masing \en{thread} saja \citep{wilt2013cuda}.

Pada CUDA, CPU disebut \en{host} sehingga memori CPU disebut \en{host memory} dan GPU disebut \en{device} sehingga memori GPU disebut \en{device memory}. Pada umumnya, kode yang dijalankan di \en{host} tidak dapat langsung mengakses \en{device memory} dan begitu juga sebaliknya. Untuk itu, perlu dilakukan transfer data dari \en{device} ke \en{host} atau sebaliknya. Penjelasan lebih lanjut mengenai transfer data ini akan dibahas pada bagian selanjutnya.

Sejak CUDA 6 terdapat fitur \en{unified memory}, yaitu abstraksi memori yang dapat diakses secara langsung baik oleh \en{host} maupun \en{device}. Di balik layar, tetap dilakukan transfer data antar \en{host} dan \en{device}, namun secara otomatis dilakukan oleh kernel CUDA, sehingga memudahkan \en{programmer} meskipun cenderung mengakibatkan kinerja yang lebih buruk. Hingga versi 1.12, dukungan TensorFlow terhadap \en{unified memory} masih eksperimental sehingga pada tulisan ini \en{unified memory} diabaikan.

% TBD. Penjelasan transfer data host <-> device.

\section{Penelitian Terkait} \label{relatedworks}

"Training deeper models by GPU memory optimization on TensorFlow" \citep{meng2017training} merupakan \en{paper} yang berisi penelitian untuk mengoptimisasi penggunaan memori GPU pada pelatihan model \en{deep learning} dengan TensorFlow \citep{meng2017training}. Penelitian tersebut terbatas pada sebuah komputer dengan satu GPU. Ide dari \en{paper} tersebut adalah menyimpan sebagian model dari memori GPU ke memori CPU saat pelatihan untuk mengurangi beban memori GPU.

Optimisasi dilakukan dengan mengimplementasikan operasi baru yang disebut (\textbf{swap out/in}) untuk men-\en{swap} \en{tensor} dari memori GPU ke CPU. Dengan operasi baru ini, graf TensorFlow yang sudah ada dimodifikasi sehingga operasi disisipkan ke hubungan \en{vertice} yang mengalirkan tensor yang di-\en{swap}. Operasi \textbf{swap out/in} ini diilustrasikan oleh Gambar~\ref{fig:SwapOutIn}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/swap-out-in.png}
    \caption{Graf yang dimodifikasi dengan menyisipkan operasi \textbf{swap out/in} \citep{meng2017training}.}
    \label{fig:SwapOutIn}
\end{figure}

Agar dapat mengoptimisasi dengan baik, perlu dipertimbangkan \en{tensor} mana yang perlu di-\en{swap}. Apabila semua \en{tensor} di-\en{swap}, akan menyebabkan penurunan kinerja model karena \en{data transfer overhead} yang besar. Menurut \en{paper} tersebut, \en{tensor}-\en{tensor} yang terbaik untuk di-\en{swap} adalah yang interval hingga diperlukannya kembali saat \en{backpropagation} paling lama. Harapannya, \en{overlapping} antara komputasi dengan \en{asynchronous data transfer} meningkat, sehingga mengurangi durasi pelatihan karena semakin banyak \en{data transfer} yang berjalan bersama dengan komputasi..

Eksperimen pada \en{paper} tersebut dilakukan dengan beberapa \en{pre-trained model} seperti ResNet, Inception, dan NMT, pada sebuah komputer dengan satu GPU (Tesla M40) dengan kapasitas memori 12GB. Menurut hasil eksperimen yang ditunjukkan Gambar~\ref{fig:SwapOutInResult} tersebut, didapatkan bahwa pada model-model yang diuji, ukuran \en{batch} dapat ditingkatkan hingga 3 kali, sedangkan penggunaan memori maksimum menurun hingga 2-3 kali lipat. Bahkan, model-model seperti ResNet-1001 dan ResNet-2000 yang sebelumnya tidak dapat dilatih karena \en{out of memory} (OOM), menjadi dapat dilatih. Di sisi lain, \en{training speed} mengalami penurunan kurang dari 10\%, karena adanya \en{data transfer overhead} seperti yang telah disebutkan sebelumnya.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/swap-out-in-result.png}
    \caption{Perbandingan penggunaan memori dengan \textbf{swap out/in} \citep{meng2017training}.}
    \label{fig:SwapOutInResult}
\end{figure}
