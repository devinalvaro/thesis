\chapter{Studi Literatur}

\section{Deep Learning}

\en{Deep learning} adalah suatu bagian dari bidang \en{machine learning}, di mana pembelajaran dilakukan pada \en{neural network} dengan \en{layer} berjumlah banyak \citep{deng2014deep}. Berbagai arsitektur \en{deep learning} seperti \en{convolutional neural network} (CNN) dan \en{recurrent neural network} (RNN) telah banyak diaplikasikan pada berbagai bidang AI, seperti \en{computer vision} dan \en{speech recognition}.

\subsection{Arsitektur}

Arsitektur \en{deep learning} sangat beragam sehingga tidak semuanya dapat dibahas secara detail, oleh karena itu hanya akan dibahas secara singkat beberapa arsitektur \en{deep learning} yang relevan dengan tulisan ini, yaitu CNN.

Pada CNN, neuron-neuron pada suatu \en{layer} tidak selalu tersusun secara 1 dimensi, namun dapat tersusun secara n-dimensi. Selain itu, suatu neuron tidak memiliki hubungan satu-satu dengan neuron-neuron di \en{layer} selanjutnya. Melainkan, kumpulan \en{weight} dan \en{bias} beberapa neuron sekaligus menjadi input dari fungsi aktivasi ke sebuah neuron di \en{layer} selanjutnya. Hubungan antar \en{layer} pada CNN tersebut diilustrasikan oleh Gambar ~\ref{fig:FeatureMap}. Inilah yang disebut \en{feature map}, yaitu \en{mapping} dari kumpulan neuron di suatu \en{layer} ke suatu neuron di \en{layer} selanjutnya.

Secara intuitif, kegunaan \en{feature map} adalah mengenali fitur pada suatu \en{layer}, untuk kemudian diserahkan ke \en{layer} selanjutnya. Maka dari itu, pada umumnya setiap \en{layer} menggunakan banyak \en{feature map} yang diinisialisasi secara acak, dengan harapan bahwa setelah \en{training}, setiap \en{feature map} mengenali fitur-fitur yang berbeda pada suatu \en{layer}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/feature-map.png}
    \caption{\en{Feature map} pada CNN \citep{nielsen2015neural}.}
    \label{fig:FeatureMap}
\end{figure}

Sebagai contoh, arsitektur CNN untuk MNIST ditunjukkan oleh Gambar ~\ref{fig:CNN}. Pada \en{input layer}, terdapat 28x28 neuron (1 neuron untuk setiap pixel pada gambar berukuran 28x28 pixel). Pada gambar tersebut, \en{input layer} terhubung ke \en{layer} selanjutnya dengan 3 \en{feature map}, begitu seterusnya hingga \en{output layer}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/cnn.png}
    \caption{Contoh arsitektur CNN untuk MNIST \citep{nielsen2015neural}.}
    \label{fig:CNN}
\end{figure}

\subsection{Proses Training}

Ada berbagai macam metode pelatihan model \en{deep learning}, tetapi akan dibahas salah satu metode yang paling populer, yaitu \en{stochastic gradient descent} (SGD). Pada SGD, keseluruhan \en{dataset} dibagi menjadi beberapa \en{mini-batch} (atau \en{batch}). Pada setiap \en{batch}, gradien dihitung kemudian digunakan untuk \en{update} parameter, agar menurunkan \en{loss} atau \en{error}.

Apabila \en{training} telah selesai memproses seluruh \en{batch}, maka disebut telah menyelesaikan satu \en{epoch}. Biasanya, \en{training} terdiri dari banyak \en{epoch}. Tujuan SGD membagi \en{dataset} ke \en{batch}-\en{batch} adalah mempercepat konvergensi dan menghemat penggunaan memori karena pada satu waktu hanya perlu diproses sebagian \en{training data} saja. Sayangnya, karena gradien yang didapat pada suatu \en{batch} hanya berdasarkan beberapa \en{training sample} saja, maka gradien ini sejatinya adalah estimasi dari gradien seharusnya. Hal ini dapat menyebabkan penurunan kinerja model, namun pada praktiknya penurunan kinerja ini dapat diterima demi meningkatkan kecepatan pelatihan dan mengurangi penggunaan memori.

\subsection{Penggunaan Memori}

Berbagai penelitian menunjukkan bahwa pada model \en{deep learning}, menambah jumlah neuron dan \en{hidden layer} dapat meningkatkan kinerja model \citep{dean2012large,nielsen2015neural}. Tetapi, meningkatkan kompleksitas model seperti demikian juga meningkatkan \en{training cost}, seperti waktu pelatihan dan ukuran memori yang dibutuhkan. Gambar ~\ref{fig:MemoryVSBatchSize} menunjukkan utilisasi memori beberapa \en{pre-trained model} terhadap ukuran \en{batch}. Berikutnya, akan dibahas beberapa penyebab meningkatnya kebutuhan memori pada model \en{deep learning} seiring meningkatnya kompleksitas model.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/memory-vs-batch-size.png}
    \caption{Utilisasi memori berbagai model \en{deep learning} terhadap ukuran \en{batch} \citep{canziani2016analysis}.}
    \label{fig:MemoryVSBatchSize}
\end{figure}

\textbf{Feature Map.} Jumlah feature map meningkat seiring bertambahnya \en{hidden layer}, karena setiap \en{layer} membutuhkan \en{mapping} ke \en{layer} selanjutnya. Sedangkan ukuran \en{feature map} ditentukan oleh berbagai faktor, seperti ukuran \en{batch} dan \en{stride}. Gambar ~\ref{fig:ResNet50Memory} menunjukkan penggunaan memori model ResNet-50 pada satu iterasi \en{mini-batch training} pada \en{dataset} ImageNet. Penggunaan memory ResNet-50 mengalami \en{peak} pada sekitar 5GB, kemudian mengalami penurunan karena \en{feature map} yang sudah tidak dibutuhkan akan didealokasi.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/ResNet50-memory.png}
    \caption{Penggunaan memori pada model ResNet-50 \citep{meng2017training}.}
    \label{fig:ResNet50Memory}
\end{figure}

\textbf{Bias.} Jumlah \en{bias} meningkat dengan bertambahnya \en{hidden layer}, karena setiap neuron pada setiap \en{hidden layer} memiliki \en{bias} sendiri. Akan tetapi, bila dibandingkan dengan \en{feature map}, penggunaan memori dari \en{bias} relatif sedikit. Hanya saja, tidak seperti \en{feature map}, bias tidak pernah dialokasi di sepanjang proses \en{training}.

\textbf{Temporary memory.} Beberapa algoritma komputasi pada training \en{deep learning} membutuhkan memori, seperti FFT (fast Fourier transform). Tetapi, penggunaan memori seperti ini relatif kecil dan cepat didealokasi, tepatnya setelah algoritma selesai dieksekusi.

Dari beberapa penyebab penggunaan memori pada \en{deep learning} tersebut, \en{feature map} merupakan pengguna memori tertinggi. Selain itu, \en{feature map} memiliki beberapa properti khusus yang dapat dimanfaatkan untuk optimisasi. Salah satu properti tersebut adalah sifatnya yang berbasis \en{layer}, sehingga bila pada \en{feed-forward} proses \en{training} telah maju ke \en{layer} selanjutnya, \en{feature map} dari \en{layer-layer} sebelumnya tidak digunakan hingga \en{backpropagation}.

\section{Graphics Processing Unit}

Penggunaan GPU pada pelatihan \en{deep learning} merupakan salah satu faktor yang memajukan perkembangan bidang ini secara signifikan \citep{dean2012large}. Hal ini dikarenakan GPU mampu meningkatkan kecepatan dengan menjalankan pelatihan secara paralel. Sayangnya, ukuran memori GPU lebih terbatas, yakni 8-10 kali lebih kecil dari memori CPU. Sebagai gambaran, pada saat penulisan tulisan ini, \en{high-end} GPU rata-rata memiliki 12-16GB memori, sedangkan memori CPU dapat mencapai 128GB \citep{meng2017training}.

Padahal, kekurangan memori dapat menyebabkan kegagalan pelatihan model (\en{out of memory}). Solusi yang biasa diterapkan adalah mengurangi ukuran \en{dataset}, ukuran \en{batch}, atau jumlah parameter model, meskipun hal ini dapat menurunkan kinerja model \citep{meng2017training}.

Selain solusi-solusi tersebut, terdapat juga metode \en{memory swapping}, yaitu memindahkan sebagian data dari memori GPU ke CPU saat pelatihan. Akan tetapi, \en{memory swapping} yang terlalu sering dapat menurunkan kecepatan pelatihan secara signifikan karena adanya \en{data transfer overhead} \citep{dean2012large}.

\subsection{Memori}

TBD.

\section{TensorFlow}

TensorFlow adalah sistem pembelajaran mesin yang didesain untuk menangani pelatihan model berskala besar dan secara terdistribusi, baik untuk kebutuhan penelitian maupun industri. Dikembangkan oleh tim Google Brain dan dirilis secara \en{open source}, TensorFlow telah digunakan secara ekstensif pada berbagai proyek pembelajaran mesin, baik di dalam maupun di luar Google.

Pada dasarnya, TensorFlow adalah \en{framework} yang menyediakan operasi-operasi komputasi matematika (seperti aljabar linear dan kalkulus), dengan operasi-operasi tambahan yang memudahkan pendefinisian proses pembelajaran mesin. Dengan TensorFlow, eksekusi operasi-operasi tersebut dapat dilakukan secara terdistribusi menggunakan berbagai \en{device} seperti CPU, GPU, maupun TPU (Tensor Processing Units) yang terdapat pada satu atau lebih \en{machine} yang terletak pada satu atau lebih \en{cluster}. Selain itu, TensorFlow juga berjalan di berbagai macam platform, mulai dari \en{data center} hingga \en{smartphone}.

\subsection{Komputasi}

Seluruh operasi dan data pada program TensorFlow beserta hubungannya membentuk sebuah \en{dataflow graph}. Operasi merupakan \en{vertice} dari graf tersebut. Sedangkan data yang pada TensorFlow direpresentasikan sebagai \en{tensor} (vektor multi-dimensional) mengalir (\en{flowing}) melalui \en{edge}-\en{edge} dari graf.

Salah satu tujuan representasi program dengan \en{dataflow graph} adalah memudahkan komputasi program secara terdistribusi. Seperti diilustrasikan oleh Gambar ~\ref{fig:TensorFlowGraph}, graf dapat dibagi-bagi menjadi beberapa subgraf, di mana setiap subgraf dieksekusi oleh \en{device} yang berbeda-beda. Dengan membagi menjadi beberapa subgraf, sebuah \en{device} baru perlu berkomunikasi dengan \en{device} lainnya apabila masuk \en{vertice} yang memiliki \en{dependency} ke \en{vertice} di \en{device} lain.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/tensorflow-graph.png}
    \caption{\en{Dataflow graph} dari program TensorFlow \citep{geron2017hands}.}
    \label{fig:TensorFlowGraph}
\end{figure}

\subsection{Arsitektur}

Gambar ~\ref{fig:TensorFlowArch} menunjukkan arsitektur umum TensorFlow. Berikut ini akan dijelaskan beberapa bagian dari arsitektur tersebut.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.50\linewidth]{figures/tensorflow-arch.png}
    \caption{Arsitektur umum TensorFlow \citep{tensorflow2018architecture}.}
    \label{fig:TensorFlowArch}
\end{figure}

\textbf{Client}. Menerima pendefinisian program TensorFlow dari \en{user}, seperti \en{tensor}-\en{tensor} yang ada dan operasi-operasi yang dijalankan. Kemudian \en{client} mengkonstruksi \en{dataflow graph} yang sesuai, lalu mengeksekusi graf tersebut menggunakan \en{session}. \en{Client} tersedia dalam beberapa bahasa, seperti Python, Go, dan C++.

\textbf{Distributed Master.} Merupakan koordinator dari keseluruhan program TensorFlow. Setelah menerima \en{dataflow graph} dari \en{client}, \en{distributed master} mempartisi graf menjadi beberapa subgraf, kemudian mendistribusikannya ke \en{worker services}.

\textbf{Worker Services.} Menerima \en{request} dari \en{master}, menjalankannya pada \en{devices} di bawahnya, kemudian mengirim hasilnya kembali ke \en{master}. \en{Worker service} berelasi satu-satu dengan \en{task}. \en{Task} pada TensorFlow merujuk pada suatu proses TensorFlow, di mana suatu \en{machine} dapat memiliki satu atau lebih \en{task}. Bahkan, suatu \en{device} dari suatu \en{machine} dapat dibagi-bagi ke beberapa \en{task}.

\textbf{Kernel Implementations.} Merupakan implementasi operasi-operasi TensorFlow yang \en{device-specific}. Tanpa \en{kernel implementation} yang sesuai, \en{backend} TensorFlow tidak dapat berjalan pada suatu \en{device}.

Gambar ~\ref{fig:TensorFlowDist} mengilustrasikan hubungan antar bagian TensorFlow tersebut dalam \en{training} terdistribusi.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/tensorflow-dist.png}
    \caption{Hubungan bagian-bagian terdistribusi di TensorFlow \citep{tensorflow2018architecture}.}
    \label{fig:TensorFlowDist}
\end{figure}

\subsection{Penambahan Operasi}

TensorFlow mendukung penambahan jenis operasi baru, apabila jenis-jenis operasi yang disediakan belum memenuhi kebutuhan pengguna, atau mengoptimisasi operasi yang ada secara \en{native}. Untuk membuat operasi baru, pengguna perlu menambahkan definisi (seperti \en{input} dan \en{output}, bentuk \en{tensor}) dan implementasi operasi tersebut dalam bahasa C++. Selanjutnya, pengguna dapat menambahkan API ke operasi tersebut pada \en{client}, namun langkah ini bersifat opsional.

\section{Penelitian Terkait}

"Training deeper models by GPU memory optimization on TensorFlow" (Meng, 2017) merupakan penelitian untuk mengoptimisasi penggunaan memori GPU pada pelatihan model \en{deep learning} dengan TensorFlow \citep{meng2017training}. Penelitian tersebut terbatas pada sebuah komputer dengan satu GPU. Ide dari \en{paper} tersebut adalah menyimpan sebagian model dari memori GPU ke memori CPU saat pelatihan untuk mengurangi beban memori GPU.

Optimisasi dilakukan dengan mengimplementasikan operasi baru yang disebut (\textbf{swap out/in}) untuk men-\en{swap} \en{tensor} dari memori GPU ke CPU. Dengan operasi baru ini, graf TensorFlow yang sudah ada dimodifikasi sehingga operasi disisipkan ke hubungan \en{vertice} yang mengalirkan tensor yang di-\en{swap}. Operasi \textbf{swap out/in} ini diilustrasikan oleh Gambar ~\ref{fig:SwapOutIn}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/swap-out-in.png}
    \caption{Graf yang dimodifikasi dengan menyisipkan operasi \textbf{swap out/in} \citep{meng2017training}.}
    \label{fig:SwapOutIn}
\end{figure}

Agar dapat mengoptimisasi dengan baik, perlu dipertimbangkan \en{tensor} mana yang perlu di-\en{swap}. Apabila semua \en{tensor} di-\en{swap}, akan menyebabkan penurunan kinerja model karena \en{data transfer overhead}. Maka dari itu, \en{tensor} terbaik untuk di-\en{swap} adalah yang interval hingga diperlukannya kembali cukup lama.

Eksperimen pada \en{paper} dilakukan dengan beberapa \en{pre-trained model} seperti ResNet, Inception, dan NMT, pada sebuah komputer dengan satu GPU (Tesla M40) dengan kapasitas memori 12GB. Menurut eksperimen tersebut, didapatkan bahwa pada model-model yang diuji, ukuran \en{batch} dapat ditingkatkan hingga 3 kali, sedangkan penggunaan memori maksimum menurun hingga 2-3 kali lipat. Bahkan, model-model seperti ResNet-1001 dan ResNet-2000 yang sebelumnya tidak dapat dilatih karena \en{out of memory} (OOM), menjadi dapat dilatih. Di sisi lain, \en{training speed} mengalami penurunan kurang dari 10\%, karena adanya \en{data transfer overhead}.

\begin{figure}[htp]
    \centering
    \includegraphics[width=0.75\linewidth]{figures/swap-out-in-result.png}
    \caption{Perbandingan penggunaan memori dengan \textbf{swap out/in} \citep{meng2017training}.}
    \label{fig:SwapOutInResult}
\end{figure}
