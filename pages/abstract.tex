\newpage
\clearpage

\chaptertoc{ABSTRAK}
\vspace{-2em}

\begin{center}
    \large \bfseries \MakeUppercase{\thetitle}

    \normalsize \mdseries Oleh

    \MakeUppercase{\theauthor}
\end{center}

\begin{singlespace}
    Memori adalah salah satu kebutuhan terpenting pelatihan model \en{deep learning}, karena kurangnya memori dapat menggagalkan pelatihan. Terlebih, pelatihan seringkali menggunakan GPU, yang ukuran memorinya pada umumnya lebih kecil dari memori CPU. \en{Memory swapping} membantu menangani permasalahan ini, dengan memanfaatkan memori CPU sebagai \en{memory pool} tambahan. Akan tetapi, teknik ini menambah durasi pelatihan karena pemindahan isi memori antara GPU dan CPU membutuhkan waktu. Maka dari itu, \en{memory swapping} yang optimal perlu selektif dalam memilih isi memori yang di-\en{swap}.

    TensorFlow, sebuah \en{framework} pembelajaran mesin, memiliki fitur \en{memory swapping}, yang menurut analisis masih dapat ditingkatkan kinerjanya untuk mengurangi \en{overhead} terhadap durasi pelatihan. peningkatan kinerja tersebut memanfaatkan sebuah properti pada pelatihan, di mana semakin awal sebuah \en{output} dihasilkan pada fase \en{feed forward}, semakin akhir \en{output} tersebut digunakan kembali pada fase \en{backpropagation}. Dari properti tersebut, \en{memory swapping} dapat mengutamakan \en{swapping} pada \en{output}-\en{output} terawal, sehingga meningkatkan \en{asynchronicity} antara komputasi dan \en{swapping}.

    Peningkatan kinerja tersebut diimplementasikan dengan memodifikasi \en{kernel} TensorFlow pada bagian yang menangani \en{memory swapping}. Sebelumnya, \en{swapping} di TensorFlow secara naif dilakukan pada \en{output} terakhir. Setelah ditingkatkan kinerjanya, \en{swapping} diutamakan pada \en{output}-\en{output} terawal, sehingga lebih banyak \en{output} yang dapat di-\en{swap} balik secara \en{asynchronous} bersamaan dengan komputasi.

    Menurut pengujian menggunakan model char-rnn dengan berbagai parameter serta \en{dataset} berukuran 285 KB dan 4.4 MB, didapatkan bahwa melatih model menggunakan versi dengan peningkatan kinerja lebih cepat 2.5\% hingga 3.2\% dari versi asli. Selain itu, karena diimplementasikan di level \en{kernel}, peningkatan kinerja ini transparan dari sudut pandang \en{end-user} TensorFlow.

    Kata kunci: \en{memory swapping}, TensorFlow, \en{asynchronous}, \en{backpropagation}.
\end{singlespace}
