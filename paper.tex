\documentclass[conference]{IEEEtran}

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

% ---- %
\usepackage[hidelinks]{hyperref}

\usepackage{letltxmacro}
\LetLtxMacro\oldttfamily\ttfamily
\DeclareRobustCommand{\ttfamily}{\oldttfamily\csname ttsize\endcsname}
\def\ttsize{\small}

\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  captionpos=b,
  frame=single,
}

\renewcommand{\arraystretch}{1.25}
% ---- %

\begin{document}

\title{Improving TensorFlow's Memory Swapping by Prioritizing Earlier Tensors}

\author{\IEEEauthorblockN{Devin Alvaro}
\IEEEauthorblockA{\textit{School of Electrical Engineering and Informatics} \\
\textit{Bandung Institute of Technology}\\
Bandung, Indonesia \\
devin.alvaro@gmail.com}
\and
\IEEEauthorblockN{Achmad Imam Kistijantoro}
\IEEEauthorblockA{\textit{School of Electrical Engineering and Informatics} \\
\textit{Bandung Institute of Technology}\\
Bandung, Indonesia \\
imam@informatika.org}
}

\maketitle

\begin{abstract}
    The ever-increasing sizes of deep learning models and datasets used increase the need for memory, as insufficient memory may abort a training process. Adding to this problem, deep learning trainings tend to use GPUs over CPUs for better training speed, where in general a GPU has significantly less memory than its CPU counterpart.
    One solution is \textit{memory swapping}, a way to free some memory by temporarily moving data to another memory pool, in this case moving data from a GPU memory to a CPU memory. However, since moving data takes time---the larger the data the more the time---performing memory swapping in a training can significantly increase the training duration. Therefore, an ideal memory swapping has to be selective on how much and which data to swap.
    TensorFlow, a machine learning framework, features memory swapping that, based on our analysis over its implementation, can be improved to reduce the increase on training duration it causes. The improvement is done is by prioritizing earlier tensors (\textit{tensor} is the basic data unit in TensorFlow) which, because of a certain backpropagation property, allows the asynchronicity of program execution to increase, ultimately reducing training duration.
    Based on our experiments on Char-RNN models with various hyperparameters and datasets (of size 285 KB and 4.4 MB), the improvement reduces up to around 3\% of training duration on certain cases.
\end{abstract}

\begin{IEEEkeywords}
memory swapping, TensorFlow, asynchronous, backpropagation
\end{IEEEkeywords}

\section{Introduction}

Training deep learning models requires a certain amount of memory for storing weights, biases, outputs, and the dataset. The lack of memory in a training may cause \textit{out of memory} (OOM) error which aborts the training. Adding to this problem is the widespread use of GPUs for training, where in general a GPU has less memory than a CPU \cite{dean}.

One solution to this problem is \textit{memory swapping}, a way to free some memory in the primary memory pool by temporarily moving data to another memory pool. In the case of training with a GPU, this means moving data from a GPU memory to a CPU memory. However, since moving data takes time---the larger the data the more the time---performing memory swapping in a training can significantly increase the training duration. Therefore, an ideal memory swapping has to be selective on how much and which data to swap \cite{meng}.

TensorFlow, an open source machine learning framework, features memory swapping. Based on our analysis, which is explained in a later section, memory swapping in TensorFlow can be improved to reduce the increase on training duration it causes. In short, the improvement is done is by prioritizing earlier tensors (\textit{tensor} is a multi-dimensional arrays, the basic data unit in TensorFlow \cite{abadi}) which, because of a certain backpropagation property, allows the asynchronicity of program execution to increase, ultimately reducing training duration.

In this paper, we further describe the memory swapping improvement and how it can be implemented to TensorFlow's source code. Based on our experiments, on certain cases the improved version can train in up to around 3\% less time than the original version.

\section{Related Works}

\cite{meng} explores some memory problems in training deep learning models and proposes some solutions specifics to TensorFlow. Firstly, it addresses the OOM error in a training. Then, the paper proposes memory swapping as a solution to the OOM problem, and presents its own implementation of memory swapping in TensorFlow. Lastly, it gives some ideas on improving memory swapping in the context of training deep learning models, specifically on trainings with backpropagation.

This paper differs from \cite{meng} as the authors implemented \textit{their own} version of memory swapping in TensorFlow along with some of the improvements. In contrast, we improve the memory swapping \textit{already present} in TensorFlow, using one of the improvements in \cite{meng}'s implementation, which, based on our observations, is significantly different than TensorFlow's own memory swapping. Thus, we only use the core idea of the improvement, and adapt it to TensorFlow's own memory swapping.

\section{Deep Learning Training and The Improvement} \label{sec:improvement}

Before describing the improvement idea, we will briefly explain about deep learning training. In a training with backpropagation, there are two phases: feed forward and backpropagation. In the feed forward phase, the process goes from the input layer to the output layer, calculating outputs of each (hidden) layer's neurons. Afterwards, in the backpropagation phase, the process goes in reverse from the output layer to the input layer, calculating gradients of each (hidden) layer's neurons, used to adjust model parameters so that the overall error is minimized \cite{nielsen}.

In calculating the gradient of a neuron, the neuron's output calculated in the feed forward phase is needed once more. Consequently, outputs have to be stored to allow recalling them in the backpropagation phase, which means they occupy the memory space. Furthermore, since the feed forwad goes from start-to-end and the backpropagation goes from end-to-start, this means that the earlier the outputs are calculated, the longer they are stored until being resued in the backpropagation phase. Vice versa, the later the outputs are calculated, the sooner they are going to be reused in the backpropagation phase.

Based on those observations, the improvement idea from \cite{meng} is as follows. Earlier outputs (called "tensors" afterwards, since these outputs are represented as tensors in TensorFlow) occupy the memory space the longer until reused. Therefore, earlier tensors are the best candidates to be memory swapped, because they let more time for the memory swapper to move them from the GPU to the CPU, and back. Since tensors are moved asynchronously, this can increase the overlapping between computations and swappings, thus reducing the overall training duration \cite{meng}.

\section{TensorFlow's Memory Swapping} \label{sec:memoryswapping}

In this section we discuss TensorFlow's memory swapping. For the end-users, TensorFlow provides the class \texttt{tf.nn.dynamic\_rnn} which accepts the parameter \texttt{swap\_memory=True} to allow memory swapping. Since only one class allows memory swapping and it's based on recurrent neural network (RNN), from now on we only concern ourselves with RNN models.

Now, we go deeper into the internals of TensorFlow's memory swapping. In TensorFlow, memory swapping is centered around classes such as \texttt{Stack}, \texttt{StackPushOp}, and \texttt{StackPopOp} located at \texttt{tensorflow/core/kernels/stack.cc} in the source code. However, before getting into them, we need to explain about tensors and ops.

\subsection{Tensor and Op}

A TensorFlow program can be seen as a dataflow graph, where operations (op) are the nodes, and tensors flow along the edges, processed as they pass through an op.

Class \texttt{Tensor} in the source code represents a tensor. It is to be noted that this class is only a handle to the real tensor data in the memory, whether the GPU's or the CPU's. From this point on, a tensor written as \texttt{tensor} means an instance of class \texttt{Tensor}.

Meanwhile, different ops are represented by different classes. Some of which are related to memory swapping, our next subjects of discussion.

\subsection{Stack}

As we previously mentioned, outputs from the feed forward phase have to be stored to allow reusing them in the backpropagation phase. In TensorFlow, for some ops, each has a local stack represented by class \texttt{Stack}, so that output tensors can be reused in the right order. Based on our observations, the number of stack elements is directly proportional to the timestep/unrolling (an RNN term not explained in this paper, readers may refer to \cite{lecun} \cite{hinton}), thus we conclude that each stack element (output tensor) comes from a certain timestep, with the uppermost element coming from the last timestep, and the lowest element coming from the first timestep.

\autoref{fig:TensorStack} illustrates \texttt{Stack}, \texttt{Tensor}, and how they relate to TensorFlow's memory swapping. As seen on the figure, the stack contains tensors from the least recent timestep to the most recent timestep, with the uppermost tensor coming from the most recent timestep. Each stack element is an instance of \texttt{Tensor}, each points to the real tensor data in the memory. On the left side, all elements point to the GPU memory. The right side illustrates what happens after swapping the uppermost element, as the tensor data are moved from the GPU to the CPU, it now points to the CPU memory.

\begin{figure}[htp]
    \centering
    \includegraphics[width=1\linewidth]{figures/tensor-stack.png}
    \caption{\texttt{Stack} and \texttt{Tensor}; \textbf{left}: pre-swapping; \textbf{right}: after swapping the latest tensor.}
    \label{fig:TensorStack}
\end{figure}

Next, we will discuss about \texttt{StackPushOp} and \texttt{StackPopOp}.

\subsection{Swapping Out}

\texttt{StackPushOp} is the class responsible for pushing \texttt{tensor} to \texttt{stack}, and possibly swapping out if conditions allow. \autoref{lst:StackPushOp} shows the pseudocode of a part of \texttt{StackPushOp}'s code (located at \texttt{tensorflow/core/kernels/stack.cc}) where it performs swapping out.

\begin{lstlisting}[label=lst:StackPushOp, caption=Swapping out pseudocode]
if (gpu_memory >= 70% full) {
    swap_out(current_tensor, (cpu_tensor) => {
        stack.push(cpu_tensor)
    })
} else {
    stack.push(current_tensor)
}
\end{lstlisting}

From the code, we can see the heuristic for swapping out. If the GPU memory is still less than 70\% full, \texttt{current\_tensor} isn't swapped, it's immediately pushed to the \texttt{stack}. On the contrary, if the GPU memory is 70\% full or more, \texttt{current\_tensor} (located in GPU) is swapped to the CPU first, then a new \texttt{cpu\_tensor} pointing to the same data moved to the CPU memory is pushed to \texttt{stack}. This fits the illustration in \autoref{fig:TensorStack}.

Note the \texttt{(...) => \{ ... \}} syntax which represents an asynchronous callback. Since in TensorFlow tensors are swapped asynchronously, this means \texttt{swap\_out} also runs asynchronously, with the asynchronous callback being called after it's done.

\subsection{Swapping In}

\texttt{StackPopOp} is the class responsible for popping the top \texttt{tensor} from \texttt{stack}, and swapping it back if it was previously swapped out. \autoref{lst:StackPopOp} shows the pseudocode of a part of \texttt{StackPopOp}'s code (located at \texttt{tensorflow/core/kernels/stack.cc}) where it performs swapping in.

\begin{lstlisting}[label=lst:StackPopOp, caption=Swapping in pseudocode]
tensor = stack.pop()
if (tensor.swapped) {
    swap_in(tensor, (gpu_tensor) => {
        current_tensor = gpu_tensor
    })
} else {
    current_tensor = tensor
}
\end{lstlisting}

From the code, we can see when does TensorFlow swap tensors in. When the training process needs to reuse output tensors (in the backpropagation phase, as we previously explained), it pops top \texttt{tensor} from the \texttt{stack}. TensorFlow then checks if it points to the GPU or the CPU memory. If it points to the GPU memory, then the process can immediately proceeds to process \texttt{current\_tensor} as its data is already located in the GPU memory. Conversely, if it points to the CPU memory, the tensor first needs to be swapped in from the CPU memory to the CPU memory. Like the swapping out, this swapping in runs asynchronously.

\subsection{Summary and Analysis}

In this subsection we summarize the mechanism of TensorFlow's memory swapping and briefly show our analysis over it.

TensorFlow begins to swap tensors out once it detects that the GPU memory is almost full, specifically 70\% full. It is to be noted that the tensor it swaps out is the current tensor, which means swapped tensors tend to be the latests. Whether swapped or not, \texttt{current\_tensor} is pushed to the top of the \texttt{stack}.

In the backpropagation phase, output tensors (calculated in the feed forward phase) need to be reused. With \texttt{stack}, TensorFlow popped the top element, which is the \texttt{tensor} to be processed. Before processing the \texttt{tensor}, TensorFlow checks if it was previously swapped out, in which case it's swapped back in. If it wasn't swapped out, then it's in the GPU already, ready to be processed.

There are two main takeaways. The first is that swapped tensors are the latest tensors. The second is that even if tensors are swapped asynchronously, they are only swapped in when needed. For reasons we explained in Section~\ref{sec:improvement}, earlier tensors are hypothetically the better candidates for memory swapping, the main reason is that they let the memory swapper to asynchronously swap them in, along with other computations. This can potentially increase the asynchronicity of the program, reducing the overall training duration.

\section{Design and Implementation} \label{sec:designimplementation}

After discussing the improvement method in Section~\ref{sec:improvement} and how TensorFlow's memory swapping works in Section~\ref{sec:memoryswapping}, in this section we will describe how can the improvement method be implemented in TensorFlow's memory swapping.

The main thing in \texttt{StackPushOp} (\autoref{lst:StackPushOp}) that needs to change is how it swaps out the current tensor, instead of the earliest tensor. To accomodate this, we modify the code to the one roughly depicted by \autoref{lst:StackPushOpImproved}.

\begin{lstlisting}[label=lst:StackPushOpImproved, caption=Swapping out pseudocode (improved)]
stack.push(current_tensor)

if (gpu_memory >= 70% full) {
    id = stack.get_earliest_tensor_id()
    swap_out(stack[id].tensor,
             (cpu_tensor) => {
        stack[id].tensor = cpu_tensor
        stack[id].swapped = true
    })
}
\end{lstlisting}

As seen at the beginning of the code, \texttt{current\_tensor} is pushed immediately to \texttt{stack}, since it won't be swapped. Instead, the earliest unswapped tensor is swapped out, identified with \texttt{id} obtained through \texttt{get\_earliest\_tensor\_id()} method (whose implementation won't be explained). With \texttt{id} obtained, we can access the earliest unswapped tensor at \texttt{stack[id]}. Accessing the non-top element may seem violating the rule of a stack, but this is possible because \texttt{stack} in TensorFlow is implemented with C++ vector. After the swapping out the earliest tensor is done, in the asynchronous callback we mark it as swapped and replace the \texttt{tensor} to the one pointing to the CPU memory. Also, note that the heuristic is unchanged from the original version.

\autoref{fig:TensorStackImproved} illustrates \texttt{Stack} and \texttt{Tensor} with the above improvement, in contrast to the original version in \autoref{fig:TensorStack}. The difference is that on the right side, after swapping out, instead of the top \texttt{tensor} pointing to the CPU memory, it is not the lowest \texttt{tensor} pointing to the CPU memory, since it (earliest tensor) is the one being swapped out.

\begin{figure}[htp]
    \centering
    \includegraphics[width=1\linewidth]{figures/tensor-stack-improved.png}
    \caption{\texttt{Stack} and \texttt{Tensor}; \textbf{left}: pre-swapping; \textbf{right}: after swapping the earliest tensor.}
    \label{fig:TensorStackImproved}
\end{figure}

Next is the improvement for swapping in. The main change is that since now it's the earlier tensors being swapped out, we can swap in tensors more asynchronously, instead of only swapping them at the moment they're needed, as in the original version. This swapping in improvement is depicted in \autoref{lst:StackPopOpImproved}.

\begin{lstlisting}[label=lst:StackPopOpImproved, caption=Swapping in pseudocode (improved)]
tensor = stack.pop()
if (tensor.swapped) {
    swap_in(tensor, (gpu_tensor) => {
        current_tensor = gpu_tensor
    })
} else {
    current_tensor = tensor
}

if (not gpu_memory_almost_full) {
    id = stack.get_newest_swapped_tensor_id()
    swap_in(stack[id].tensor, (gpu_tensor) => {
        stack[id].tensor = gpu_tensor
        stack[id].swapped = false
    })
}
\end{lstlisting}

As can be seen, the first half of the code is exactly the same as the code in \autoref{lst:StackPopOp}. The difference is, aside of that, we also asynchronously swap in "future tensors", swapped tensors that will be needed at a later time. As we previously mentioned in Section~\ref{sec:improvement} and Section~\ref{sec:memoryswapping}, this is to increase the asynchronicity of the program, ultimately reducing the overall training duration. That, is core of the improvement.

The implementation of this improvement is done by modifying the TensorFlow kernel in \texttt{tensorflow/core/kernels/stack.cc}, roughly depicted by the pseudocodes listed above. Since the improvement is implemented at kernel-level, this improvement is transparent to end-users.

\section{Experiments}

In this section we share the experiments on the improvement implementation. We run the experiments on a single machine with Intel Xeon CPU @ 2.20 GHz, 12 GB RAM, and NVIDIA Tesla P4 (with 8 GB memory).

\subsection{Method}

In the experiments, we compare the training duration (in seconds) between the original version and the improved version of TensorFlow. In each experiment case, we run both versions on the same parameter and dataset, 10 times. From the 10 runs, we derive the means of both versions, and test whether they are statistically different with t-test.

We use the \textit{Char-RNN} model as described in \cite{karpathy}, implemented in TensorFlow using \texttt{tf.nn.dynamic\_rnn} (mentioned in Section~\ref{sec:memoryswapping}) with \texttt{swap\_memory=True} to allow memory swapping. In short, Char-RNN is an RNN model that takes a sequence of characters, then generates the next sequence of characters.

The experiment cases can be divided into two parts, the ones using the small dataset and the ones using the large dataset. The small dataset is a text containing 291,268 characters (285 KB) while the large dataset is a text containing 4,573,338 characters (4.4 MB). We deem the contents of the text unimportant for the paper's purpose, but between both datasets, the contents are different.

The variable (hyper)parameters are timestep, batch size, and epoch. Timestep is an experiment variable because we want to observe how it affects the improvement, since timestep is related to \texttt{stack} size. Meanwhile, the batch size's role as an experiment variable is to keep the memory usage relatively constant in all cases, as we want to keep the memory usage above the GPU memory capacity (to force memory swapping). Epoch is also an experiment variable because the number of epoch highly affects training duration (the larger the epoch the longer the training).

\subsection{Result}

\autoref{tbl:Cases} shows the ID of each experiment case, along with the parameters and the datasets used. IDs of experiments using the small dataset are prefixed with "A", while IDs of experiments using the large dataset are prefixed with "B". For both sides, the experiment cases are the combinations of low, medium, and high value of timestep and epoch.

\begin{table}[htp]
\centering
\caption{Parameters and datasets of each case}
\label{tbl:Cases}
\begin{tabular}{|l|l|l|l|l|}
\hline
\textbf{ID} & \textbf{Timestep} & \textbf{Batch size} & \textbf{Epoch} & \textbf{Dataset size} \\ \hline
A1          & 25                & 4125                & 10             & 285 KB                \\ \hline
A2          & 25                & 4125                & 20             & 285 KB                \\ \hline
A3          & 25                & 4125                & 40             & 285 KB                \\ \hline
A4          & 50                & 3000                & 10             & 285 KB                \\ \hline
A5          & 50                & 3000                & 20             & 285 KB                \\ \hline
A6          & 50                & 3000                & 40             & 285 KB                \\ \hline
A7          & 75                & 2500                & 10             & 285 KB                \\ \hline
A8          & 75                & 2500                & 20             & 285 KB                \\ \hline
A9          & 75                & 2500                & 40             & 285 KB                \\ \hline
B1          & 25                & 4125                & 10             & 4.4 MB                \\ \hline
B2          & 25                & 4125                & 20             & 4.4 MB                \\ \hline
B3          & 25                & 4125                & 40             & 4.4 MB                \\ \hline
B4          & 50                & 3000                & 10             & 4.4 MB                \\ \hline
B5          & 50                & 3000                & 20             & 4.4 MB                \\ \hline
B6          & 50                & 3000                & 40             & 4.4 MB                \\ \hline
B7          & 75                & 2500                & 10             & 4.4 MB                \\ \hline
B8          & 75                & 2500                & 20             & 4.4 MB                \\ \hline
B9          & 75                & 2500                & 40             & 4.4 MB                \\ \hline
\end{tabular}
\end{table}

\autoref{tbl:Overview} shows the means of execution times of all cases, with IDs respective to the ones in \autoref{tbl:Cases}.

\begin{table}[htp]
\centering
\caption{Experiment results}
\label{tbl:Overview}
\begin{tabular}{|l|l|l|l|}
\hline
\textbf{ID} & \multicolumn{1}{c|}{\textbf{Original Mean (s)}} & \multicolumn{1}{c|}{\textbf{Improved Mean (s)}} & \multicolumn{1}{c|}{\textbf{Improvement}}  \\ \hline
A1          & 33.52523                                        & 33.56403                                         & -0.11\%                                    \\ \hline
A2          & 65.15423                                        & 64.7291                                          & 0.65\%                                     \\ \hline
A3          & 127.62171                                       & 127.15463                                        & 0.36\%                                     \\ \hline
A4          & 40.13374                                        & 39.08925                                         & 2.6\%                                      \\ \hline
A5          & 74.46792                                        & 72.24764                                         & 2.98\%                                     \\ \hline
A6          & 142.81972                                       & 139.09633                                        & 2.6\%                                      \\ \hline
A7          & 50.28715                                        & 48.71249                                         & 3.13\%                                     \\ \hline
A8          & 94.84399                                        & 92.35375                                         & 2.63\%                                     \\ \hline
A9          & 184.27691                                       & 179.04678                                        & 2.84\%                                     \\ \hline
B1          & 421.27128                                       & 420.70736                                        & 0.13\%                                     \\ \hline
B2          & 842.36554                                       & 839.1987                                         & 0.37\%                                     \\ \hline
B3          & 1685.27977                                      & 1673.40191                                       & 0.7\%                                      \\ \hline
B4          & 487.36775                                       & 472.67                                           & 3.02\%                                     \\ \hline
B5          & 969.60724                                       & 941.28422                                        & 2.92\%                                     \\ \hline
B6          & 1931.72836                                      & 1876.21198                                       & 2.87\%                                     \\ \hline
B7          & 497.82833                                       & 485.17491                                        & 2.54\%                                     \\ \hline
B8          & 991.18114                                       & 964.28065                                        & 2.71\%                                     \\ \hline
B9          & 1977.07522                                      & 1924.45449                                       & 2.66\%                                     \\ \hline
\end{tabular}
\end{table}

In all cases, means of both versions are statistically different according to t-test, except for cases A1, A2, A3, and B1. The similarity of those four cases is that they use a low timestep. The other two cases with low timestep (B2 and B3) also yield low improvements. Meanwhile, all cases with medium and high timesteps yield significant improvements. These reasons lead us to conclude that the timestep need to exceed a certain threshold to yield significant improvements. To make it clearer, even though case A1 yields a negative result, means of both versions in the case are not statistically different, and therefore not a concern to us.

The explanation for this is that since higher timestep leads to larger \texttt{stack} size, this let more swapped tensors to be asynchronously swapped in along with the computation. We explained this more deeply in Section~\ref{sec:designimplementation}.

Next we observe cases with different epochs, to see whether there is any relation between epoch and improvement. Looking at the cases, we get varied results, in some cases the improvement increases as the epoch increases (such as A8 to A9), and in some cases the reverse happens (such as A8 to B6). Therefore, no conclusion can be made about the relation between epoch and improvement.

Lastly, by observing cases with different datasets (cases prefixed with "A" and "B"), no pattern emerges which show that dataset affects the improvement. This means that the improvement should work on various datasets.

Overall, from the experiment results we can conclude timestep affects the improvement the most, while epoch and dataset don't bring the same effect. At lower timestep, the improved version doesn't bring any significant improvements, while at timesteps above certain threshold, the improved version consistently reduces training durations by 2.5\% to 3.2\%.

\section{Conclusion}

In this paper, we have discussed about TensorFlow's memory swapping and how it can be improved by prioritizing earlier tensors, which allows more asynchronous swapping, ultimately reducing training duration. Based on our experiments, the improved version reduces training durations by 2.5\% to 3.2\% on cases with high enough timestep.

\begin{thebibliography}{00}
    \bibitem{dean} Dean, Jeffrey, et al. "Large scale distributed deep networks." Advances in neural information processing systems. 2012.
    \bibitem{meng} Meng, Chen, et al. "Training deeper models by GPU memory improvement on TensorFlow." Proc. of ML Systems Workshop in NIPS. 2017.
    \bibitem{abadi} Abadi, Mart√≠n, et al. "Tensorflow: A system for large-scale machine learning." 12th {USENIX} Symposium on Operating Systems Design and Implementation ({OSDI} 16). 2016.
    \bibitem{nielsen} Nielsen, Michael A. Neural networks and deep learning. Vol. 25. San Francisco, CA, USA:: Determination press, 2015.
    \bibitem{lecun} LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. "Deep learning." nature 521.7553 (2015): 436.
    \bibitem{hinton} Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. "Learning representations by back-propagating errors." Cognitive modeling 5.3 (1988): 1.
    \bibitem{karpathy} Karpathy, Andrej. "The unreasonable effectiveness of recurrent neural networks." Andrej Karpathy blog 21 (2015).
\end{thebibliography}

\end{document}
