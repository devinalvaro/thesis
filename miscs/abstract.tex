\newpage
\clearpage

\begin{center}
    \large \bfseries \MakeUppercase{Abstrak}

    \vspace{0.6cm}

    \large \bfseries \MakeUppercase{Optimisasi \en{Memory Swapping} di TensorFlow}
\end{center}

\begin{center}
    Oleh

    \normalsize \MakeUppercase{\theauthor}
\end{center}

\vspace{0.2cm}

\begin{spacing}{1.0}
    Memori adalah salah satu kebutuhan terpenting pelatihan model \en{deep learning}, karena kurangnya memori dapat menggagalkan pelatihan. Terlebih, pelatihan seringkali menggunakan GPU, yang ukuran memorinya pada umumnya lebih kecil dari memori CPU. \en{Memory swapping} membantu menangani permasalahan ini, dengan memanfaatkan memori CPU sebagai \en{memory pool} tambahan. Akan tetapi, teknik ini menambah durasi pelatihan karena pemindahan isi memori antara GPU dan CPU membutuhkan waktu. Maka dari itu, \en{memory swapping} yang optimal perlu selektif dalam memilih isi memori yang di-\en{swap}.

    TensorFlow, sebuah \en{framework} pembelajaran mesin, memiliki fitur \en{memory swapping}, yang menurut analisis masih dapat dioptimisasi untuk mengurangi \en{overhead} terhadap durasi pelatihan. Optimisasi memanfaatkan sebuah properti pada pelatihan, di mana semakin awal \en{tensor} dihasilkan pada fase \en{feed forward}, semakin akhir \en{tensor} tersebut digunakan kembali pada fase \en{backpropagation}. Dari properti tersebut, \en{memory swapping} dapat mengutamakan \en{tensor} terawal, sehingga meningkatkan \en{asynchronicity} antara komputasi dan \en{swapping}.

    Optimisasi tersebut diimplementasikan dengan memodifikasi \en{kernel} TensorFlow pada bagian yang menangani \en{memory swapping}. Sebelumnya, \en{swapping} di TensorFlow secara naif dilakukan pada \en{tensor} terakhir. Setelah dioptimisasi, \en{swapping} diutamakan pada \en{tensor}-\en{tensor} terawal, sehingga lebih banyak \en{tensor} yang dapat di-\en{swap} balik secara \en{asynchronous} bersamaan dengan komputasi.

    Menurut pengujian menggunakan model char-rnn dengan berbagai parameter serta \en{dataset} berukuran 285 KB dan 4.4 MB, didapatkan bahwa optimisasi dapat mengurangi durasi pelatihan dari 2.5\% hingga 3.2\%. Selain itu, karena diimplementasikan di level \en{kernel}, optimisasi ini juga transparan dari sudut pandang \en{end-user} TensorFlow.

    Kata kunci: \en{memory swapping}, TensorFlow, \en{asynchronous}, \en{backpropagation}.
\end{spacing}
